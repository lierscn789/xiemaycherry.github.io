<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>学习の历程</title>
    <url>/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/</url>
    <content><![CDATA[<p>记录生活</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201104191815960.png" alt="image-20201104191815960" style="zoom:33%;"></p>
<a id="more"></a>
<h2><span id="202011-paper-paper-paper">202011 Paper Paper Paper</span><a href="#202011-paper-paper-paper" class="header-anchor">#</a></h2><p><strong>Week 1: HSR results</strong></p>
<ol>
<li><p>Figure &amp; Table</p>
<ol>
<li>Chinese report</li>
</ol>
</li>
</ol>
<p><strong>Week 2:  Talents flow pattern</strong></p>
<p><strong>Data</strong>(Netwrok &amp; indicator)</p>
<ol>
<li><p><strong>Data prepare(file)（half-day)</strong></p>
<p>resume data during 2014-2016 year（一定要先把数据准备好)</p>
</li>
</ol>
<p><strong>Method（four methodes)</strong></p>
<ol>
<li><p><strong>Distribution Feature</strong></p>
<ol>
<li><strong>Description of talents network(distribution feature )</strong></li>
<li><strong>page ranking algorithm(node importance)</strong></li>
</ol>
</li>
</ol>
<p>《Spatiotemporal Patterns of Population Mobility and Its Determinants in Chinese Cities Based on Travel Big Data  》</p>
<ol>
<li><p><strong>spatial correlation analysis Spatial autocorrelation model</strong>  （莫兰指数)</p>
<p>《Understanding regional talent attraction and its influencing factors in China: From the perspective of spatiotemporal pattern evolution  》</p>
</li>
<li><p><strong>GWR mode (determining factors)</strong></p>
<p>《The correlation between HSR construction and economic development – Empirical study of Chinese cities  》</p>
<p>《Spatially varying patterns of afforestation/reforestation and socioeconomic factors in China: a geographically weighted regression approach 》</p>
</li>
</ol>
<p>Week 3: Writting</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>11月4日</td>
<td>数据准备</td>
<td>地图准备</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11月5日</td>
<td>绘图</td>
<td>分布特征和流动特征</td>
<td>流动特征</td>
<td>流向图</td>
<td>地图</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>城市等级之间</td>
<td>城市群</td>
<td>桑基图</td>
<td>（可选）</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11月6日</td>
<td>节点重要性排序</td>
<td>PageRank</td>
<td>地图</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>表格</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11月7日</td>
<td>空间集聚性</td>
<td>全局相关性</td>
<td>图</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>具备相关性</td>
<td>表格</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11月8日 -  11月9日</td>
<td>空间自相关模型</td>
<td>权重矩阵</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>算法</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11月10日 -  11月15日</td>
<td>完成方法（英文为主，中文简略）</td>
<td>结果分析</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3><span id="planning-of-week-45-be-done"><strong>planning of week 45 be done!~</strong></span><a href="#planning-of-week-45-be-done" class="header-anchor">#</a></h3><ul>
<li>[ ] Research<ul>
<li>[ ] day1: figure &amp; Table</li>
<li>[ ] day2-3: result analysis</li>
<li>[ ] day4: effect 机制</li>
</ul>
</li>
<li>[ ] English</li>
<li>[ ] Reading</li>
</ul>
<h3><span id="1104">1104</span><a href="#1104" class="header-anchor">#</a></h3><p>今天我才认识到，</p>
<h3><span id="1103-ren-xing">1103 人性</span><a href="#1103-ren-xing" class="header-anchor">#</a></h3><p>人性就是给别人挖了一个洞，你还不得不心甘情愿的往里面跳。这种人性最高明了。人就是逃不掉对利益，金钱，权势的诱惑。</p>
<h3><span id="20201102-hao-hao-de-gong-zuo-hao-hao-de-gao-kpi-jiu-hui-zhuan-hen-duo-hen-duo-de-qian">20201102  好好的工作，好好的搞KPI, 就会赚很多很多的钱</span><a href="#20201102-hao-hao-de-gong-zuo-hao-hao-de-gao-kpi-jiu-hui-zhuan-hen-duo-hen-duo-de-qian" class="header-anchor">#</a></h3><p>Excepting map, I get all tables and figures.</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201102223605713.png" alt="image-20201102223605713" style="zoom: 67%;"></p>
<h2><span id="202010-gui-lu-kai-xin-zi-ru-mv">202010 规律，开心，自如.mv</span><a href="#202010-gui-lu-kai-xin-zi-ru-mv" class="header-anchor">#</a></h2><font color="red">**生活篇**</font>

<ul>
<li>[ ] 看血祭+书评</li>
</ul>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201001152013359.png" alt="image-20201001152013359" style="zoom: 50%;"></p>
<ul>
<li>[ ] 了解女孩子的心思</li>
</ul>
<font color="blue">科研篇</font>

<ul>
<li>[ ] the results of high-speed railway(one week)</li>
<li>[ ] reading paper</li>
</ul>
<font color="green">学习篇</font>

<ul>
<li>[ ] English</li>
</ul>
<hr>
<p>hr {</p>
<p>  border: 1px solid #c13535;</p>
<p>}</p>
<hr>
<p>hr {</p>
<p>  border: 1px solid #c13535;</p>
<p>}</p>
<h3><span id="planning-of-week-44-be-done"><strong>planning of week 44 be done!~</strong></span><a href="#planning-of-week-44-be-done" class="header-anchor">#</a></h3><p>把论文初稿出来</p>
<ul>
<li>[ ] PHD<ul>
<li>[ ] the first draft<ul>
<li>[ ] figure</li>
<li>[ ] table</li>
</ul>
</li>
</ul>
</li>
<li>[ ] English<ul>
<li>[ ] 2 hour/day</li>
</ul>
</li>
<li>[ ] Studying<ul>
<li>[ ] vidoe : how to use big data to solve economic and social problem.(pubished by a handsome professor in harvard university) <a href="https://www.youtube.com/watch?v=qgs-bfA6HRg&amp;list=PLalrHnPrv5uDe-vDW5dPxTByQoZu6P6Hq&amp;index=1">link</a></li>
<li>[ ] paper:  a list of 5 papers</li>
</ul>
</li>
</ul>
<h3><span id="20201101">20201101</span><a href="#20201101" class="header-anchor">#</a></h3><hr>
<h4><span id="202031">202031</span><a href="#202031" class="header-anchor">#</a></h4><p><strong>Planning:</strong> </p>
<p><strong>Done:</strong></p>
<pre><code>1. pychart (Map, Geo to draw a map at province level and at city level)(city name file , position, parameter)
 2. I finished the result
 3. I did not know why I 
</code></pre><hr>
<h4><span id="202030">202030</span><a href="#202030" class="header-anchor">#</a></h4><p><strong>Planning:</strong> </p>
<ul>
<li><p>[x] I plan to finish Chinese map(five picture)(morning)</p>
</li>
<li><p>[x] the difference of the feature of talents migration(afternoon)</p>
</li>
<li><p>[x] the correlation between talents migration metrics and economic development</p>
</li>
</ul>
<p>感觉好像做了很多无用功啊。但是后面用就高效率了</p>
<p>感觉有些太微小的细节更不不用考虑啊，所以啊不用花费那么多时间啊。</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201030222231596.png" alt="image-20201030222231596"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201030222402760.png" alt="image-20201030222402760" style="zoom: 50%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201030222540525.png" alt="image-20201030222540525" style="zoom: 80%;"></p>
<hr>
<h4><span id="202029-ta-ma-de-gao-wan-huan-shi-gong-zuo-suan-qiu-liao">202029  他妈的：搞完还是工作算求了</span><a href="#202029-ta-ma-de-gao-wan-huan-shi-gong-zuo-suan-qiu-liao" class="header-anchor">#</a></h4><p><strong>Planning:</strong></p>
<p>难道我就不能简简单单的做科研吗？</p>
<p>我还是对人性，人对利益的追求看的太轻了。权力，金钱，让我觉得无比的恶心。</p>
<p>要抬头挺胸的做人和做事</p>
<hr>
<h4><span id="202028-kai-xin">202028 开心</span><a href="#202028-kai-xin" class="header-anchor">#</a></h4><p><strong>Planning:</strong>  生活就是如此</p>
<p>hexo d 出现了问题，加载不了大的录屏，卧槽啊，导致一直提交不上</p>
<hr>
<h4><span id="202027">202027</span><a href="#202027" class="header-anchor">#</a></h4><p><strong>Planning:</strong> </p>
<ol>
<li><p>the results of talent imgration (map)</p>
<ol>
<li>English Listening</li>
</ol>
</li>
</ol>
<hr>
<h4><span id="202026">202026</span><a href="#202026" class="header-anchor">#</a></h4><p><strong>Planning:</strong> </p>
<ol>
<li><p>the migration status of city and city-pair </p>
<ol>
<li>watching video / hour</li>
<li>new concept(10/min) &amp; kaochong(Listening/30min) &amp; writting(/30min)</li>
</ol>
</li>
</ol>
<p><strong>Done:</strong></p>
<ol>
<li><p>how to analyse total information</p>
<ol>
<li>step by step </li>
</ol>
</li>
</ol>
<hr>
<h3><span id="planning-of-week-43-be-done">planning of week 43 be done!~</span><a href="#planning-of-week-43-be-done" class="header-anchor">#</a></h3><ul>
<li>[x] Results of reseach</li>
<li>[x] Studying English</li>
</ul>
<h3><span id="summary-of-week-43-the-importance-of-step-by-step">summary of week 43 the importance of step by step</span><a href="#summary-of-week-43-the-importance-of-step-by-step" class="header-anchor">#</a></h3><ol>
<li>I implement interface.</li>
<li>how to draw a high-quality and nice picture.</li>
<li>the first step is to prepare data,then the second step is that inputting data file into software.</li>
</ol>
<h4><span id="1025-oh-yeach-how-to-use-arcgis-map-to-draw-china-map-at-city-level">1025 oh yeach! how to use arcgis map to draw China map at city-level.</span><a href="#1025-oh-yeach-how-to-use-arcgis-map-to-draw-china-map-at-city-level" class="header-anchor">#</a></h4><p>I went to lab at  9:40 and arried at 10:00 o’clock</p>
<p>I made a mistake when a drink XY Line beacuse I the x,y location must not unable(blank)(costing 5 hours)</p>
<p>how to mark ?(fill in  by hand(manual operation))</p>
<p>Today ,I made a screen in AI.  I eat a lot,badly.</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/1025Z_1.gif" alt></p>
<h4><span id="1024-oh-yeach-a-china-map">1024  oh yeach! A China map</span><a href="#1024-oh-yeach-a-china-map" class="header-anchor">#</a></h4><p>I got up very late. I stayed in the bed until 11.am </p>
<p>I speet more than 5 hours on searching a China map.  Now , I could draw a China map thought a shp file. oh yeach! oh yeach!</p>
<h4><span id="1023-yuan-lai-wo-he-bie-ren-yi-yang-ye-shi-you-yu-wang-de">1023 原来我和别人一样也是有欲望的</span><a href="#1023-yuan-lai-wo-he-bie-ren-yi-yang-ye-shi-you-yu-wang-de" class="header-anchor">#</a></h4><p>喜、怒、哀、惧、爱、恶、欲，原来我也有。</p>
<h4><span id="1022-ren-xing-shi-yao-fu-chu-dai-jie-de">1022 任性是要付出代价的</span><a href="#1022-ren-xing-shi-yao-fu-chu-dai-jie-de" class="header-anchor">#</a></h4><ol>
<li><p>English(宾语从句，it)</p>
</li>
<li><p>模板+ 图层</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201021214624034.png" alt="image-20201021214624034" style="zoom:33%;"></p>
</li>
<li><p>要努力丫丫丫丫</p>
</li>
<li><p>今天把堆积了两个星期的衣服洗完了，感觉衣柜是空了不少啊~</p>
</li>
<li><p>昨天晚上买的百草园零食到了，不知道为什么最近晚上老是很饿</p>
</li>
</ol>
<h4><span id="1021-wu-fen-zhong-chu-yi-zhang-gao-zhi-liang-fu-za-de-tu-bu-shi-meng-origin">1021 五分钟出一张高质量，复杂的图不是梦~~~~origin</span><a href="#1021-wu-fen-zhong-chu-yi-zhang-gao-zhi-liang-fu-za-de-tu-bu-shi-meng-origin" class="header-anchor">#</a></h4><p>so easy~~</p>
<p>so easy~~</p>
<p>图层，模板，拼接的用法</p>
<h4><span id="1020">1020</span><a href="#1020" class="header-anchor">#</a></h4><p>English &amp; research</p>
<h4><span id="1019-hui-tu">1019 绘图</span><a href="#1019-hui-tu" class="header-anchor">#</a></h4><p>origin软件的更新速度比我的认知程度块多了。今天至于掌握到精髓部分了。</p>
<ol>
<li>主题模板<ol>
<li>坐标轴的范围</li>
<li>label的大小</li>
</ol>
</li>
<li>绘图模板（批量绘图)<ol>
<li>创建模板时注意数据结构的一致性。（不同单元格数据的位置或者名称要一样) 要对齐</li>
</ol>
</li>
<li>图层的合并和分解<ol>
<li>这个很好啊，拿来做组合，然后调节页面细节。</li>
</ol>
</li>
</ol>
<p>综上，要化相对复杂图，这些复杂图由各种相似的小图构成，避免重复性的调节，先把单个小图的模板做好（一个单元的内容),然后再把某个单元的内容做成模板，然后画出大的部分。然后在图层合并。先做好单个零件的图（如果要大量批量操作，做成模板),然后图层管理工具（合并)。</p>
<p>注意：</p>
<pre><code>1. 相同类型的绘图，保持相同的数据结构。
 2. 较大的单个图控制好物理尺寸
</code></pre><p><img src="https://pic1.zhimg.com/d7fc70f4a60ffc10c495167b494e72d4_r.jpg" alt="img" style="zoom: 50%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201019223051292.png" alt="image-20201019223051292" style="zoom:33%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201019223113139.png" alt="image-20201019223113139" style="zoom:33%;"></p>
<p>hr {</p>
<p>  border: 1px solid #c13535;</p>
<p>}</p>
<h3><span id="planning-of-week-42-be-done">planning of week 42 be done!~</span><a href="#planning-of-week-42-be-done" class="header-anchor">#</a></h3><ul>
<li>[ ] English(背)<ul>
<li>[ ] 新概念二（10)</li>
<li>[ ] 背诵一篇范文（1）</li>
<li>[ ] 精度(2)</li>
<li>[ ] 听力（30min/day)</li>
</ul>
</li>
<li>[ ] Research &amp; Data Analysis<ul>
<li>[ ] 数据分析思维篇（Blog1)</li>
<li>[ ] 数据分析报告(Report)</li>
<li>[ ] 封装time series(total sum of 5h) </li>
<li>[ ] 如何开展调研方法论</li>
</ul>
</li>
<li>[ ] 生活篇<ul>
<li>[ ] 每天至少半小时的阅读时间（中午吧）</li>
<li>[ ] 多喝水</li>
</ul>
</li>
</ul>
<h4><span id="1018-zhong-yu-zhang-wo-hui-tu-mi-jue-liao">1018 终于掌握绘图秘诀了</span><a href="#1018-zhong-yu-zhang-wo-hui-tu-mi-jue-liao" class="header-anchor">#</a></h4><p>一天的成果。为了以后使用方便。感觉傻瓜式的软件直接点快很多啊。如果是编程实现，感觉所以参数都要修改麻烦啊。</p>
<p>发现还是origin好用，直接点，点那里修改那里。</p>
<p>8pt Arial</p>
<p>感觉还是origin好，创建模板多好啊。能用origin的还是这个好</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201018205350508.png" alt="image-20201018205350508"></p>
<h4><span id="1017-plot-tu-pian-da-xiao-yu-zi-ti">1017 plot （图片大小与字体)</span><a href="#1017-plot-tu-pian-da-xiao-yu-zi-ti" class="header-anchor">#</a></h4><ol>
<li><p>终于知道图片尺寸的作用了。不管图片大小多少，只要把字体，和字号放在上面。然后不同大小的图片，最终效果一样。</p>
</li>
<li><p>绘图先定图片大小，尤其多张。然后用AI组成。</p>
<p>View : Whole Page：步骤2显示Page放大，虽然此时屏幕尺寸可能大于设置尺寸，但不影响输出效果</p>
</li>
<li><p>CMYK是适合印刷的四色模式。</p>
</li>
<li><p>C代表青色<code>Cyan</code>，M代表洋红色<code>Magenta</code>，Y代表黄色<code>Yellow</code>，K代表黑色<code>Black</code></p>
</li>
</ol>
<ul>
<li>有衬线字体：Times New Roman，宋体；</li>
<li>无衬线字体：Arial，黑体；</li>
</ul>
<ol>
<li>绘图一定改默认参数。尤其是科研绘图。就是改设置的参数，都要敲一下</li>
</ol>
<h4><span id="1016-yao-zai-zhe-ge-xie-yu-xing-feng-de-huan-jing-li-chong-sheng">1016  要在这个血雨腥风的环境里重生</span><a href="#1016-yao-zai-zhe-ge-xie-yu-xing-feng-de-huan-jing-li-chong-sheng" class="header-anchor">#</a></h4><p>data analysis</p>
<h4><span id="1015-cheng-wei-bie-de-ren-hao-ma-quantify">1015 成为别的人好吗？？？quantify</span><a href="#1015-cheng-wei-bie-de-ren-hao-ma-quantify" class="header-anchor">#</a></h4><p>experience:unit &amp;DID&amp;planning</p>
<p>efficency:quantify</p>
<p>要用一辈子来偿还曾经任性，无知犯过的错</p>
<hr>
<h4><span id="1014">1014</span><a href="#1014" class="header-anchor">#</a></h4><p>City-level &amp; Concept</p>
<hr>
<h4><span id="1013">1013</span><a href="#1013" class="header-anchor">#</a></h4><p>Coding &amp; English</p>
<p><a href="https://www.vitae.ac.uk/researchers-professional-development/about-the-vitae-researcher-development-framework/developing-the-vitae-researcher-development-framework">https://www.vitae.ac.uk/researchers-professional-development/about-the-vitae-researcher-development-framework/developing-the-vitae-researcher-development-framework</a></p>
<p><strong><a href="https://www.vitae.ac.uk/researchers-professional-development/knowledge-and-intellectual-abilities">Domain A: Knowledge and intellectual abilities:</a></strong> The knowledge, intellectual abilities and techniques to do research<strong>
</strong></p>
<p><strong><em>\</em><a href="https://www.vitae.ac.uk/researchers-professional-development/personal-effectiveness">Domain B: Personal effectiveness</a>\</strong>:** The personal qualities and approach to be an effective researcher</p>
<p><strong><em>\</em><a href="https://www.vitae.ac.uk/researchers-professional-development/research-governance-and-organisation">Domain C: Research governance and organisation</a>\</strong>:<br>**Knowledge of the professional standards and requirements to do research</p>
<p><strong><a href="https://www.vitae.ac.uk/researchers-professional-development/engagement-influence-and-impact">Domain D: Engagement, influence and impact</a></strong>: The knowledge and skills to work with others to ensure the wider impact of research</p>
<p><img src="https://pic2.zhimg.com/v2-2ec49490e8db93246265df3fe50e3b74_r.jpg?source=1940ef5c" alt="preview" style="zoom: 25%;"></p>
<h4><span id="1012-dan-ran-mv">1012 淡然.mv</span><a href="#1012-dan-ran-mv" class="header-anchor">#</a></h4><p>Pyechart绘制地图。1. 添加数据点。已经提供了省份、城市经纬度的坐标。自定义，添加即可。 2. 径向图 3. 属性自定义</p>
<p>ArcGIS: 准备地图。拼接，设置即可。也简单。</p>
<hr>
<h3><span id="review-amp-summary"><font color="blue" size="12">Review &amp; Summary</font></span><a href="#review-amp-summary" class="header-anchor">#</a></h3><ol>
<li>收获。<ol>
<li>重新搭建了Blog；总结和加深了seaborn和matplotlib的学习；系统性学习了计量经济学（玄学啊，还要在学)；</li>
<li>英语。每天基本上保持了半天的学习时间。在学新概念二的时态。</li>
</ol>
</li>
<li>感悟<ol>
<li>会看文献的人都相似，优秀的人都相似。calendar&amp;Study&amp;Reading</li>
</ol>
</li>
<li>改进<ol>
<li>如何高效记单词。学习英语效率不高的原因就是记忆力跟不上。我觉得大量堆积的输入，忘得很快啊。改进。</li>
<li>生活时间很少。最近终于有早睡的想法了。</li>
</ol>
</li>
</ol>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-202010112045149551.png" alt="image-20201011204514955" style="zoom: 15%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201011204514955.png" alt="image-20201011204514955" style="zoom:15%;"></p>
<hr>
<h3><span id="planning-of-week-41-coming-on">planning of week 41 coming on!~</span><a href="#planning-of-week-41-coming-on" class="header-anchor">#</a></h3><ul>
<li>[x] English<ul>
<li>[x] speeding two hours on studying every day.</li>
</ul>
</li>
<li>[ ] research<ul>
<li>[ ] reviewing resource</li>
<li>[ ] a new result</li>
</ul>
</li>
<li>[ ] specific tasks<ul>
<li>[ ] how to visualize Chinese map</li>
<li>[ ] a new analysis report</li>
<li>[ ] </li>
</ul>
</li>
</ul>
<hr>
<h4><span id="1011">1011</span><a href="#1011" class="header-anchor">#</a></h4><p>English &amp; Python</p>
<hr>
<h4><span id="1010">1010</span><a href="#1010" class="header-anchor">#</a></h4><p>学生版originPro 2021。 beauty</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201010144443156.png" alt="image-20201010144443156" style="zoom:50%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20201010145640459.png" alt="image-20201010145640459" style="zoom:50%;"></p>
<hr>
<h4><span id="1009">1009</span><a href="#1009" class="header-anchor">#</a></h4><p>English &amp; Blog</p>
<p>晚上和室友去逛</p>
<hr>
<h4><span id="1008">1008</span><a href="#1008" class="header-anchor">#</a></h4><p>Blog</p>
<hr>
<h4><span id="1007">1007</span><a href="#1007" class="header-anchor">#</a></h4><p>Today, English &amp; Research</p>
<hr>
<h4><span id="1006">1006</span><a href="#1006" class="header-anchor">#</a></h4><p>Today, I greatly has obtained much knowledgeable about econometrics.</p>
<p>English</p>
<hr>
<h4><span id="1005">1005</span><a href="#1005" class="header-anchor">#</a></h4><p>Experience &amp;Listening</p>
<hr>
<h4><span id="1004">1004</span><a href="#1004" class="header-anchor">#</a></h4><p>Experience &amp; English</p>
<p>I get a good measure to memory  words deeply.  It’s one that putting a   word into  a sentence which is link with life.  It can make the words keep in mind. </p>
<p>I has learnt how to use Geopy, a package in Python, to calculate distance for two points of surface of the earth.</p>
<p>The more you obtain,  the smarter you have.</p>
<p>I am going to be a researcher, so I need to collect and share what you have get.</p>
<p>It’s nice to cook. </p>
<h4><span id="1003">1003</span><a href="#1003" class="header-anchor">#</a></h4><p>English &amp; Experience</p>
<hr>
<h4><span id="1002">1002</span><a href="#1002" class="header-anchor">#</a></h4><p>English &amp; Experience</p>
<p>什么是犯贱，什么是</p>
<hr>
<h4><span id="1001">1001</span><a href="#1001" class="header-anchor">#</a></h4><p>I learned new concept English about one hours.</p>
<p>I collected data from other school website.</p>
<h2><span id="202009">202009</span><a href="#202009" class="header-anchor">#</a></h2><h3><span id="planning-of-week-40-just-do-it">planning of  week 40 just do it</span><a href="#planning-of-week-40-just-do-it" class="header-anchor">#</a></h3><ul>
<li>[x] Learning English by kaochong<ul>
<li>[x] speeding two hours every day on studying English</li>
</ul>
</li>
<li>[x] data analyse<ul>
<li>[x] prepare for data set for practice</li>
<li>[x] Excel(Blog) function &amp; plot</li>
</ul>
</li>
<li>[ ] experience<ul>
<li>[ ] selecting metrics &amp; model</li>
<li>[ ] reading significant paper(&gt;three PPT)</li>
<li>[ ] 李航《统计学习》chapter 3-4</li>
</ul>
</li>
<li>[ ] reading<ul>
<li>[ ] do read notes</li>
</ul>
</li>
<li>[ ] 彩铅</li>
</ul>
<h4><span id="0929">0929</span><a href="#0929" class="header-anchor">#</a></h4><p>In the morning, I stuied English .</p>
<p>In the afternoon, I listening to course kaochong.</p>
<p>In the everning, I studied KNN algirithm.</p>
<p>You decide what you will  meet. So I wan to meet more strong partner.</p>
<h4><span id="20928">20928</span><a href="#20928" class="header-anchor">#</a></h4><p>In the morning, I learned English and excel</p>
<p>in the afternoon, I summaried Excel &amp; coding</p>
<p>In the evening, I collected resource about how to show. and </p>
<h3><span id="planning-of-week-39">planning of  week 39</span><a href="#planning-of-week-39" class="header-anchor">#</a></h3><p>I think I could not get more knowledge than costing time. </p>
<p>I must more focused.  I must learn to apart time. </p>
<ul>
<li>[x] English Learning<ul>
<li>[ ] at least，I will speed two hours on studying it.(kaochong course and new concept )</li>
</ul>
</li>
<li>[ ] Paper<ul>
<li>[ ] Do a simple  report</li>
<li>[x] Do research</li>
<li>[x] Reading paper (&gt; five)</li>
</ul>
</li>
<li>[x] time series<ul>
<li>[x] giving results</li>
</ul>
</li>
</ul>
<p>Thing about clear, thus do it.</p>
<h3><span id="journey-of-life">journey of life</span><a href="#journey-of-life" class="header-anchor">#</a></h3><h4><span id="0924">0924</span><a href="#0924" class="header-anchor">#</a></h4><p>今天早上，睡觉+葛优瘫</p>
<p>今天下午，听力训练</p>
<p>今天晚上，句型</p>
<h4><span id="0923">0923</span><a href="#0923" class="header-anchor">#</a></h4><p>今天早上，英语</p>
<p>今天下午，继续测试</p>
<p>今天晚上，time series</p>
<h4><span id="0922">0922</span><a href="#0922" class="header-anchor">#</a></h4><p>今天早上，校稿</p>
<p>今天下午，校稿+美美</p>
<p>今天晚上，数据分析+time serie</p>
<h4><span id="0921">0921</span><a href="#0921" class="header-anchor">#</a></h4><p>今天早上，英语+数据处理</p>
<p>今天下午，城市层面的数据处理</p>
<p>今天晚上，时间序列+论文阅读</p>
<h3><span id="planning-of-week-38">planning of  week 38</span><a href="#planning-of-week-38" class="header-anchor">#</a></h3><ul>
<li><p>[x] 英语</p>
<ul>
<li>[x] 新概念(2) 三篇的精度+背诵</li>
<li>[x] 新概念(3)a puma at large 的背诵</li>
</ul>
</li>
<li><p>[ ] 论文</p>
<ul>
<li>[x] 跑论文数据</li>
<li>[ ] PSM的解读</li>
<li>[ ] 稳健性检测的方法</li>
<li>[ ] 问题汇总</li>
<li>[x] 阅读人口流动模式相关的introduction</li>
</ul>
</li>
<li>[x] time series<ul>
<li>[x] 测试结果+复制粘贴(这种应用好干，就是特征工程不同罢了)</li>
</ul>
</li>
</ul>
<p>In this week, I felt inefficient. This is because I repeat one thing. So I want to change it. I think I must improve my efficiency by handwriting. </p>
<h4><span id="0920">0920</span><a href="#0920" class="header-anchor">#</a></h4><p>In the morning ,I went to cainiao to get my package including shoes and a clothes. And run codes.</p>
<p>In the afternoon, I tested the experience. I felt too bad for the results is not significant.</p>
<p>In the evening, </p>
<h4><span id="0918-selecting-paper">0918 selecting paper(🙂）</span><a href="#0918-selecting-paper" class="header-anchor">#</a></h4><p>In the morning, I learned English. And ran program。</p>
<p>In the afternoon, I collected a few papers from other literature.</p>
<h4><span id="0917-efficiency-efficiency-efficiency-efficiency-how-to-write-introduction">0917 efficiency efficiency efficiency efficiency; how to write introduction</span><a href="#0917-efficiency-efficiency-efficiency-efficiency-how-to-write-introduction" class="header-anchor">#</a></h4><p>In the morning, I practice oral English. I read some paper.</p>
<p>In the afternoon, I think about how to show the framework.</p>
<p>In the evening,  running codes &amp; vacabulary</p>
<p><strong>It’s important to subdivide task and classify.</strong></p>
<p>You should make you time more valuable &amp; costly.</p>
<h4><span id="0916-efficiency">0916 efficiency</span><a href="#0916-efficiency" class="header-anchor">#</a></h4><p>In the morning, I learned the new concept English of series 2, read one paper , and rewrite my code. </p>
<p>In the afternoon, I learned the GWR model in Gedao and ArcGIS, at the same time, I ran my program.</p>
<p>In the evening,  I  corrected error of time series. And I thought about how to manage paper. How to classify for refer convenient.</p>
<h4><span id="0915-du-lun-wen">0915 读论文</span><a href="#0915-du-lun-wen" class="header-anchor">#</a></h4><p>In the morning, I study the new concept English of series 2 and process economic data set at city level.</p>
<p>In the afternoon,  I read more than three papers which refer to population migration pattern.  In information era, new timely, available, and detail data. In information era, new timely, available, and detail data offer chance to researchers for empirical research. On the one hand, we can research old problem based on new data, on the other hand, we can investigate socioeconomic development to reveal new phenomenon which may the census data and survey data can not do.</p>
<p>In the evening,   You can’t have your cake and eat it.  I classify the literature to the Excel and Blog.</p>
<h4><span id="0914-que-shi-zhi-chu-li-shi-jian-xu-lie">0914 缺失值处理 + 时间序列</span><a href="#0914-que-shi-zhi-chu-li-shi-jian-xu-lie" class="header-anchor">#</a></h4><p>今天早上，英语+数据整合（累，累，累) 流水线搭好了。</p>
<p>今天下午，处理缺失值</p>
<p>今天晚上，党会+time series </p>
<h4><span id="0913-pao-shi-yan">0913 跑实验</span><a href="#0913-pao-shi-yan" class="header-anchor">#</a></h4><p>今天早上，英语（新2）+ 检查数据流程</p>
<p>今天下午，测试实验数据，计量经济学就是玄学啊。换种解释变量和自变量，这结果就会发生天壤之别。问题是我的结果不显著啊。难怪要做那么多的稳健性测试。</p>
<p>今天晚上，跑time series。复制+粘贴，不用动脑子。</p>
<p>问题：如何养成白净的自然皮肤。戒腥，戒辣，戒糖；多喝水，多吃水果；</p>
<p>很多的人喜欢狂欢，可我就不同了，我偏偏喜欢独处，我觉得很轻松。一旦忙起来，我就效率非常高。</p>
<h4><span id="0912">0912</span><a href="#0912" class="header-anchor">#</a></h4><p>今天早上，睡到了8：00，关键是现在在和自己的生物钟发起挑战，我居然在学新概念二，当口语。我发现我高中英语就没有过关。</p>
<p>今天把新问题的数据HSR, City &amp; resume。 流水线。以后就是放文件，跑结果了。</p>
<h4><span id="0911">0911</span><a href="#0911" class="header-anchor">#</a></h4><p>今天早上，继续把暑假资料梳理。差不多好了。</p>
<p>今天下午，规划了下这学年的玩法。默默加油。下午继续梳理论文思路。文献分门别类的重要性。</p>
<p>今天晚上，文献阅读和接下来的项目规划。</p>
<p>最近五天就是清理实验数据阶段。</p>
<p>看完了实验室的关于社会经济学的论文。棒棒棒棒棒棒。我什么时候也能贡献一点点啊！把自己的研究方向和研究方法论调到和实验室同步了。</p>
<p>把研究人口流动模式的相关不错论文读了读，Introduction，好相似呢！不错不错呢。感觉渐渐建立起文献之间相关联的感觉了，探索新问题。</p>
<h4><span id="20200909-wu-liao-de-zi-ji">20200909 无聊的自己</span><a href="#20200909-wu-liao-de-zi-ji" class="header-anchor">#</a></h4><p>今天早上分类资料（堆积了很多个月了），总觉得不命名自己能找到</p>
<p>今天下午解决自己的疑问，似乎没有得到自己的答案</p>
<p>今天晚上看剧+coding</p>
<hr>
<h4><span id="20200909">20200909</span><a href="#20200909" class="header-anchor">#</a></h4><p>今天早上总结了最近所学</p>
<p>今天下午学习了绘图和数据思路的厘清。就是AI对图片做处理，</p>
<p>今天晚上读了一个体系的引言。难，难，难，难！</p>
<h4><span id="20200908">20200908</span><a href="#20200908" class="header-anchor">#</a></h4><p>今天早上在寝室看了科普资料。</p>
<p>今天下午整理数据资料和收集了资料，果然管理文件和数据是项技术活，不定期清理就爆炸了。当然了对于某些不会再修改的文件要单独保持好。对于对于二次性文件，最好每天都要清理和管理，命名和备份。积累的过程不是一朝一夕。我大概是这么回事，先大量产生垃圾文件，然后在里面掏金子。最后输出一份有模有样的汇总，报告，分析结果等等。我觉得产生大量垃圾文件的时候，要尽早清理。</p>
<p>今天晚上时间序列。提了一些好玩的idea，看了文献。</p>
<h4><span id="20200907-you-xie-wu-liao">20200907 有些无聊</span><a href="#20200907-you-xie-wu-liao" class="header-anchor">#</a></h4><p>今天晚上跑了时间序列。</p>
<hr>
<p>今天早上寝室睡觉，养精蓄锐。昨天走太久了。</p>
<p>今天晚上终于把时间序列跑出基本预测结果了。但是效率低，效率低，</p>
<hr>
<p>写程序，一定要讲究逻辑性啊，这样补充功能会方便很大啊。</p>
<p>pandas里面的赋值要小心啊，这个和索引有关系的啊</p>
<p>事事皆学问！</p>
<h4><span id="20200906-ju-can">20200906 聚餐</span><a href="#20200906-ju-can" class="header-anchor">#</a></h4><p>今天和本科的好朋友去犀浦和博物馆逛了很久，吃了很多好吃的</p>
<p>晚上想了想下周干嘛</p>
<h4><span id="20200905-jia-you-bi-guan">20200905 加油（闭关）</span><a href="#20200905-jia-you-bi-guan" class="header-anchor">#</a></h4><p>今天早上整理暑假任务</p>
<p>今天下午看了绘图echart</p>
<p>今天晚上继续time series(添堵啊)</p>
<h4><span id="20200904-duan-she-chi">20200904 断舍离</span><a href="#20200904-duan-she-chi" class="header-anchor">#</a></h4><p>今天早上扔掉了许多东西，翻出来特别多大学买的没有穿的东西</p>
<p>今天下午绘图看了1.3h，学习了猴子的数据分析的指标体系的建立方法！感觉还是套路.</p>
<p>今天晚上弄了时间序列</p>
<h4><span id="20200903-zhong-yu-kao-wan-liao-a">20200903 终于考完了啊</span><a href="#20200903-zhong-yu-kao-wan-liao-a" class="header-anchor">#</a></h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>必修环节</td>
<td>6400006003</td>
<td>学术活动</td>
<td>研究生院</td>
<td>0</td>
<td>1</td>
<td>春与秋</td>
<td>其他</td>
<td>考查</td>
<td>是</td>
<td></td>
<td></td>
</tr>
<tr>
<td>必修环节</td>
<td>6400006009</td>
<td>论文开题报告及文献阅读综述II</td>
<td>研究生院</td>
<td>0</td>
<td>1</td>
<td>春与秋</td>
<td>其他</td>
<td>考查</td>
<td>是</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>实践教学环节：还有五个学分。</p>
<p>实践教学环节6个学分中，基地实践必须完成2-4个学分，按照实践时间1-3个月、4-6个月、7-12个月及以上作为实践时间单位，分别认定为2学分、3学分和4学分。要求提交实践总结报告，实践基地（单位）就学生提交的报告给予相关支撑书面材料证明，根据实际实践时间，经导师审核通过，可获得2-4个学分。<br>实践教学课程主要指突出实践训练的实验课程，全校可通选，完成者取得相应学分。</p>
<p>4个学分(实习)+学位论文写作规范(1)个学分</p>
<p><strong>虽然在成为那个独特的自己的时候，会走很多的弯路，然后才知道怎么样的自己适合自己。什么的东西是应该丢掉的。</strong></p>
<p>修炼，修炼，修炼</p>
<p>不能再吃外卖了！</p>
<h4><span id="20200902-fu-xi">20200902 复习</span><a href="#20200902-fu-xi" class="header-anchor">#</a></h4><p>今天复习软件安全性分析</p>
<hr>
<h4><span id="20200901-fu-xi">20200901 复习</span><a href="#20200901-fu-xi" class="header-anchor">#</a></h4><p>今天早上复习背诵</p>
<p>今天下午敢报告</p>
<p>今天晚上Android漏洞，哎，虽然写过android apk,但是安全问题好难啊</p>
<h3><span id="202008">202008</span><a href="#202008" class="header-anchor">#</a></h3><h4><span id="2020-831-fu-xi-wan-shua">2020-831 复习+玩耍</span><a href="#2020-831-fu-xi-wan-shua" class="header-anchor">#</a></h4><p>今天早上复习软件安全性分析</p>
<p>今天下午和鹏哥出去看电影了</p>
<p>今天晚上复习软件安全性分析</p>
<hr>
<h4><span id="20200830-fu-xi-ing">20200830 复习ing</span><a href="#20200830-fu-xi-ing" class="header-anchor">#</a></h4><p>今天继续软件安全性分析（无聊，低效率)</p>
<p>决定晚上把报告完成了</p>
<hr>
<h4><span id="20200829-fu-xi-ing">20200829 复习ing</span><a href="#20200829-fu-xi-ing" class="header-anchor">#</a></h4><p>今天早上和同学聊天，居然出去实习了</p>
<p>今天晚上和下午复习软件安全性分析（难）</p>
<h4><span id="202008028-mi-ma-xue-kao-shi">202008028 密码学考试</span><a href="#202008028-mi-ma-xue-kao-shi" class="header-anchor">#</a></h4><p>今天早上靠密码学</p>
<p>今天下午收拾博客文章</p>
<p>今天晚上，由于下大雨，回寝室看剧了</p>
<h4><span id="20200827-gong-cheng-lun-li">20200827 工程伦理</span><a href="#20200827-gong-cheng-lun-li" class="header-anchor">#</a></h4><p>今天早上，</p>
<p>今天下午，考工程伦理。心塞塞。</p>
<p>今天晚上，看了我在颐和园。历史文物，园林风景太棒了。英语：《breakfast or lunch》。又进一步进行了职业规划； 复习密码学；学习了tableau，果然漂亮啊！</p>
<hr>
<h4><span id="202007826-deadline">202007826 Deadline</span><a href="#202007826-deadline" class="header-anchor">#</a></h4><p>今天早上，复习了全部的密码学。</p>
<p>今天下午，复习了工程伦理的分析题。</p>
<p>今天晚上，学习了数据分析中的逻辑思维，如何精准化产品设计，产业运营。</p>
<p>我已经受够了这样的生活了。</p>
<h4><span id="20200825-qi-xi-kuai-le">20200825 七夕快乐</span><a href="#20200825-qi-xi-kuai-le" class="header-anchor">#</a></h4><p>今天早上，起床发现臀部好疼啊</p>
<p>今天下午继续复习密码学 可证明的加密安全性；数字签名安全性相关概念。公钥体制的安全性</p>
<p>今天下午学习了A/B(what，why,  how)，用于评估某种产品和设计是否有效的提供了某项指标，进而有助于辅助决策。</p>
<h4><span id="20200824-fu-xi-ing">20200824 复习ing</span><a href="#20200824-fu-xi-ing" class="header-anchor">#</a></h4><p>今天早上在寝室复习控制流完整性</p>
<p>今天下午复习模糊测试，fuzzing，各种细节</p>
<p>今天晚上低效率复习密码学分组密码和基本概念，还可证明安全性，第七章的基本概念，区别公钥密码体制（三种），</p>
<p>《像鱼》好听</p>
<p>5W2H分析法在数据分析中的应用</p>
<p>期末考试咋回事背诵呢？还是数学好，一个定理做几十道题，这种学科几个点都不一定考</p>
<hr>
<p>下周，全新全意的看视频，看PPT，打印论文背诵</p>
<p>背诵，完成课程报告。</p>
<hr>
<h4><span id="20200823-fu-xi-ing">20200823 复习ing</span><a href="#20200823-fu-xi-ing" class="header-anchor">#</a></h4><p>今天早上寝室洗衣服（周末嘛，多睡会)</p>
<p>今天下午继续复习工程伦理，。。。文科。。。还看了我在颐和园等你。我还在想怎么用工程伦理分析生活中的小三。</p>
<p>今天晚上写了工程伦理的课件， 回寝室健身，哑铃到了</p>
<h4><span id="20200822-ping-dan-wu-qi-de-sheng-huo">20200822 平淡无奇的生活</span><a href="#20200822-ping-dan-wu-qi-de-sheng-huo" class="header-anchor">#</a></h4><p>今天早上低效率的复习了密码学和软件安全性分析，太难记忆了。看剧日常</p>
<p>今天下午复习继续</p>
<p>今天晚上继续复习，啊，为什么那么多要背的东西啊</p>
<h4><span id="20200821-wu-liao-de-sheng-huo">20200821 无聊的生活</span><a href="#20200821-wu-liao-de-sheng-huo" class="header-anchor">#</a></h4><p>昨天晚上睡不好，盖被子太热，不盖又太冷，折腾着睡不着，我的天啊。</p>
<p>今天早上备份资料。</p>
<p>今天下午和晚上继续复习。好心塞啊，太难背了，感觉在考文科啊。</p>
<h4><span id="20200820-fan-xiao-ji">20200820 返校记</span><a href="#20200820-fan-xiao-ji" class="header-anchor">#</a></h4><hr>
<h4><span id="20200819-lu-man-man-qi-xiu-yuan-xi">20200819 路漫漫其修远兮</span><a href="#20200819-lu-man-man-qi-xiu-yuan-xi" class="header-anchor">#</a></h4><h4><span id="20200822-wo-lei-liao">20200822 我累了</span><a href="#20200822-wo-lei-liao" class="header-anchor">#</a></h4><hr>
<p>八月要准备考试了</p>
<p>现代密码理论（李发根老师授课）考试时间：2020-08-28，09:30-11:30 </p>
<p>软件安全性分析（陈厅老师授课）考试时间：2020-09-03，09:30-11:30  死记硬背吧， 实在是恼火</p>
<h4><span id="20200814-zhao-dian-le-zi">20200814 找点乐子</span><a href="#20200814-zhao-dian-le-zi" class="header-anchor">#</a></h4><p>今天早上睡觉</p>
<p>今天下午改了会综述，思考了会自己找点什么东西玩玩</p>
<ol>
<li>找一些公开的项目（复现一下结果，补充一会)，玩玩数据分析（爬取数据-&gt;数据分析-&gt;数据建模）感觉这个过程满好玩的喔</li>
<li>不知道为什么我特别喜欢可视化，但现在遇到的瓶颈是怎么把图画好看, 字体，字号，颜色的配置)，准备好好玩玩这个，excel, echart, Python各种库，R， BI(tableau)都去水一把啦啦啦啦啦啦啦啦啦啦啦</li>
<li></li>
</ol>
<hr>
<h4><span id="20200812-xi-wang-ni-yan-jing-li-shi-wen-rou-gu-zi-li-shi-jian-chi-nu-li-cheng-wei-suo-ai-zhi-ren-xu-yao-de-xiao-tai-yang">20200812 希望你眼睛里是温柔，骨子里是坚持，努力成为所爱之人需要的小太阳！</span><a href="#20200812-xi-wang-ni-yan-jing-li-shi-wen-rou-gu-zi-li-shi-jian-chi-nu-li-cheng-wei-suo-ai-zhi-ren-xu-yao-de-xiao-tai-yang" class="header-anchor">#</a></h4><p>我没有在你最艰难的时候陪伴你，怎么能在你得意的时候出现呢？</p>
<p>今天下午时间序列</p>
<p>今天晚上</p>
<hr>
<h4><span id="20200811-hao-xiang-chi-bing-qi-lin-a-bing-qi-lin-bing-qi-lin-bing-qi-lin-bing-qi-lin">20200811 好想吃冰淇淋啊，冰淇淋，冰淇淋，冰淇淋，冰淇淋</span><a href="#20200811-hao-xiang-chi-bing-qi-lin-a-bing-qi-lin-bing-qi-lin-bing-qi-lin-bing-qi-lin" class="header-anchor">#</a></h4><hr>
<h4><span id="20200809-zhun-bei-diao-zheng-sheng-li-zhong">20200809 准备调整生理钟</span><a href="#20200809-zhun-bei-diao-zheng-sheng-li-zhong" class="header-anchor">#</a></h4><hr>
<h4><span id="20200808-ji-xu-yu-xi">20200808 继续预习</span><a href="#20200808-ji-xu-yu-xi" class="header-anchor">#</a></h4><p>今天下午复习了numpy，我觉得先用（初步实践后）再去写全面的基础教程，这样子的效果好很多（现在基本上掌握numpy了，包括细节，嗯嗯)。还复习了软件安全性分析。</p>
<ol>
<li>缺陷(设计和代码) 和漏洞的区别和联系</li>
<li>第三章（浏览器攻击)</li>
</ol>
<p>今天晚上看了bilibili (我的青春恋爱物语果然有问题)</p>
<ol>
<li>关于时间处理numpy, pandas 要单独学习一下，这个学好了，处理数据就好干了</li>
</ol>
<hr>
<h4><span id="20200807">20200807</span><a href="#20200807" class="header-anchor">#</a></h4><p>今天早上睡觉</p>
<p>今天下午，家里网太差了。复习密码学数字签名和整数溢出。</p>
<p>今天晚上，学习新知识——数据挖掘+读文章（社科类文章，越往内深入越有意思，突然觉得人生百态咯)</p>
<p>想法：</p>
<ol>
<li>实现一个python绘图模板，主要是想改python提供的默认参数，因为官方提供的绘图方式，不咋滴好看。</li>
</ol>
<hr>
<h4><span id="20200806">20200806</span><a href="#20200806" class="header-anchor">#</a></h4><p>今天复习考试了！</p>
<hr>
<hr>
<h4><span id="20200801-20200806">20200801-20200806</span><a href="#20200801-20200806" class="header-anchor">#</a></h4><hr>
<hr>
<p>八月份大事件：</p>
<pre><code> 1. 八月二十开学
 2. 密码学
 3. 软件安全性分析（课程设计）
 4. 六级考试
</code></pre><p>一切如期进行</p>
<p>8：00- 11：00 </p>
<p>14：00-17：00</p>
<p>19：00-22：00</p>
<p>好困啊，睡觉，睡觉，睡觉😴😴😴😴</p>
<h3><span id="202007">202007</span><a href="#202007" class="header-anchor">#</a></h3><hr>
<h4><span id="20200731">20200731</span><a href="#20200731" class="header-anchor">#</a></h4><p>今天早上睡觉，看视频。</p>
<p>今天下午去市里，吃了小龙虾，买了一大堆吃食</p>
<p>今天晚上看剧 </p>
<hr>
<h4><span id="20200730-zu-gou-re-ai-zu-gou-jian-chi">20200730 足够热爱，足够坚持</span><a href="#20200730-zu-gou-re-ai-zu-gou-jian-chi" class="header-anchor">#</a></h4><p>今天早上，继续睡觉，昨天晚上睡得太晚了</p>
<p>今天下午，继续实现时间序列项目！实战机器学习</p>
<p>今天晚上，看了推荐的nature paper，原创太厉害了。继续追纪晓岚和和珅。</p>
<hr>
<h4><span id="20200729-mian-zhao-da-hai-chun-nuan-hua-kai">20200729  面朝大海，春暖花开</span><a href="#20200729-mian-zhao-da-hai-chun-nuan-hua-kai" class="header-anchor">#</a></h4><p>今天下午，时间序列分析。</p>
<p>今天晚上，学习数据挖掘+python</p>
<hr>
<h4><span id="20200728-wa-jue-nei-xin-xu-yao-de-dong-xi">20200728  挖掘内心需要的东西</span><a href="#20200728-wa-jue-nei-xin-xu-yao-de-dong-xi" class="header-anchor">#</a></h4><p>今天早上，逛了知乎</p>
<p>今天下午，跑时间序列。</p>
<p>今天晚上，看python，规划自己的职业规划</p>
<hr>
<h4><span id="20200727">20200727</span><a href="#20200727" class="header-anchor">#</a></h4><p>今天早上睡觉，刷视频 aaaaaaaa</p>
<p>今天下午，继续码字。</p>
<p>今天晚上，sklearn太好玩了呀！</p>
<p>问题就在于你想太多，而读的书太少了！！！！！阅读的确给人很多启发，尤其是人物传记类。</p>
<p>帮别人填个志愿，还得到一个瓜瓜瓜！</p>
<p>在这么追剧下去，爱奇艺都要看完了啊！</p>
<hr>
<h4><span id="2020-7-26">2020-7-26</span><a href="#2020-7-26" class="header-anchor">#</a></h4><p>今天早上睡觉</p>
<p>今天下午数据分析</p>
<p>今天晚上时间序列。跑程序</p>
<hr>
<h4><span id="2020-7-25">2020-7-25</span><a href="#2020-7-25" class="header-anchor">#</a></h4><p>今天早上去外婆家吃饭。好吃呢</p>
<p>今天下午看了一下建模。建模果然很厉害啊！看了纪晓岚和和珅(太好看了)</p>
<p>今天晚上看回顾了数据分析</p>
<hr>
<h4><span id="2020-7-24">2020-7-24</span><a href="#2020-7-24" class="header-anchor">#</a></h4><p>今天早上学绘图。</p>
<p>今天下午跑时间序列</p>
<p>今天晚上总结绘图模板（seaborn ,matplotlib）看剧</p>
<p>弟弟太厉害了，文课考的太好了</p>
<hr>
<h4><span id="2020-7-23-zhen-zheng-de-ying-xiong-zhu-yi-zhe-shi-kan-qing-sheng-huo-de-zhen-xiang-yi-ran-re-ai-sheng-huo">2020-7-23 真正的英雄主义者是看清生活的真相，依然热爱生活。</span><a href="#2020-7-23-zhen-zheng-de-ying-xiong-zhu-yi-zhe-shi-kan-qing-sheng-huo-de-zhen-xiang-yi-ran-re-ai-sheng-huo" class="header-anchor">#</a></h4><p>今天早上睡觉，看剧。舒服</p>
<p>今天下午时间序列</p>
<p>今天晚上写分析报告。</p>
<hr>
<h4><span id="2020-7-22-dui">2020-7-22  对</span><a href="#2020-7-22-dui" class="header-anchor">#</a></h4><p>今天下午写分析报告，词穷。</p>
<p>今天晚上时间序列</p>
<hr>
<h4><span id="2020-7-21">2020-7-21</span><a href="#2020-7-21" class="header-anchor">#</a></h4><p>今天早上改代码</p>
<p>今天下午学习3绘图seaborn,不好用啊，字体和字号的设置，不对头。origin可能以后不能用了，matplotlib searborn 才是王道。绘图色彩搭配很重要</p>
<p>今天晚上学习了数据分析。我发现我太喜欢数据分析了，以后要从事这行了。</p>
<p>2020-7-20 </p>
<p>今天早上睡觉</p>
<p>今天下午测试数据（感觉应该没什么问题了）</p>
<p>今天晚上跑了xgboost</p>
<hr>
<h3><span id="2020-30-zhou">2020 30周</span><a href="#2020-30-zhou" class="header-anchor">#</a></h3><p>最近十天，把八月份要干的事情干完，八月份要准备六级和期末考试了，报告和测试，我的个天，心塞啊，我选了什么课啊！全心全意准备考试。</p>
<p>Input  ✔️❌ Output ❌❌✔️ 关键最近老是想睡觉。</p>
<font color="red">Input</font>

<ol>
<li>time series</li>
<li>数据分析技能</li>
</ol>
<font color="red">Output</font>

<ol>
<li>每两天交接一下</li>
<li>增量和差值学习一下</li>
<li>学习人际交往的本质</li>
</ol>
<hr>
<h4><span id="2020-7-19-ni-ru-jin-de-qi-zhi-li-cang-zhao-ni-zou-guo-de-lu-du-guo-de-shu-he-ai-guo-de-ren">2020-7-19 你如今的气质里，藏着你走过的路，读过的书，和爱过的人。</span><a href="#2020-7-19-ni-ru-jin-de-qi-zhi-li-cang-zhao-ni-zou-guo-de-lu-du-guo-de-shu-he-ai-guo-de-ren" class="header-anchor">#</a></h4><p>今天早上睡觉</p>
<p>下午跑时间序列</p>
<p>无争无忧！</p>
<hr>
<h4><span id="2020-7-18-ni-ru-jin-de-qi-zhi-li-cang-zhao-ni-zou-guo-de-lu-du-guo-de-shu-he-ai-guo-de-ren">2020-7-18  你如今的气质里，藏着你走过的路，读过的书，和爱过的人。</span><a href="#2020-7-18-ni-ru-jin-de-qi-zhi-li-cang-zhao-ni-zou-guo-de-lu-du-guo-de-shu-he-ai-guo-de-ren" class="header-anchor">#</a></h4><p>今天早上睡觉，煮饭。</p>
<p>今天下午时间序列&amp;处理数据</p>
<p>今天晚上看《谁说我结不了婚》</p>
<p>《面朝大海，春暖花开》做个幸福的人、种菜、劈柴，周游世界！</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/image-20200718223132260.png" alt="image-20200718223132260" style="zoom:33%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/MyBlog\hexo\source\_posts\学习Daily\image-20200718223122825.png" alt="image-20200718223122825" style="zoom:33%;"></p>
<hr>
<h4><span id="2020-7-17">2020-7-17</span><a href="#2020-7-17" class="header-anchor">#</a></h4><p>今天早上，睡觉</p>
<p>今天下午，调研了很多东西。跑了时间序列，交接了工作，下面都好干了</p>
<p>今天晚上，清洗数据。感觉应该快了</p>
<hr>
<h4><span id="2020-7-16">2020-7-16</span><a href="#2020-7-16" class="header-anchor">#</a></h4><p>你如今的气质里，藏着你走过的路，读过的书，和爱过的人。</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/QQ图片20200716222012.jpg" alt="QQ图片20200716222012" style="zoom:33%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/QQ图片20200716222004.jpg" alt="QQ图片20200716222004" style="zoom: 25%;"></p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/MyBlog\hexo\source\_posts\学习Daily\QQ图片20200716221955.jpg" alt="QQ图片20200716221955" style="zoom:33%;"></p>
<hr>
<h4><span id="2020-7-15">2020-7-15</span><a href="#2020-7-15" class="header-anchor">#</a></h4><p>今天早上处理数据。</p>
<p>今天下午搞时间序列分析。[]倒着索引</p>
<p>今天晚上看论文，对比分析了两篇同类的文章。</p>
<hr>
<h4><span id="2020-7-14">2020-7-14</span><a href="#2020-7-14" class="header-anchor">#</a></h4><p>今天下午（午睡了很久）学了空间分析工具，工具还挺好用的哇。继续sklearn</p>
<p>今天晚上继续处理数据，跑的时候读了别人的文献，突然觉悟。科研的过程就是浪费时间的过程。</p>
<hr>
<h4><span id="2020-7-13-sheng-cun-huan-shi-hui-mie-zhe-shi-ge-wen-ti">2020-7-13  生存还是毁灭？这是个问题。</span><a href="#2020-7-13-sheng-cun-huan-shi-hui-mie-zhe-shi-ge-wen-ti" class="header-anchor">#</a></h4><p>今天早上刷视频，学英语！<br>今天下午睡觉&amp;学软件&amp;跑程序</p>
<p>今天晚上读论文。</p>
<p>明天：1. 时间序列调参，交接工作。2. 调研一下数据归一化的方法。3. 读论文</p>
<hr>
<h3><span id="2020-29-zhou">2020 29周</span><a href="#2020-29-zhou" class="header-anchor">#</a></h3><font size="5" color="red">Input</font>

<ol>
<li>尝试调研小领域（感觉下载的文献质量非常高）</li>
<li>学习写作</li>
<li>时间序列（机器学习算法）</li>
</ol>
<font size="5" color="red">Output</font>

<ol>
<li>PPT</li>
<li>多记点专业词汇和句式（我就是照着别人写的，但是读起来不对劲，不通畅）</li>
</ol>
<hr>
<h4><span id="2020-7-12">2020-7-12</span><a href="#2020-7-12" class="header-anchor">#</a></h4><p>今天找了几篇非常好的文献，可以做这方面的细调研，大概就10篇左右。</p>
<p>今天处理了数据，实在是麻烦。</p>
<p>突然发现文章最难的是文献综述。</p>
<p>Input  ✔️❌ Output ❌❌✔️</p>
<hr>
<p>2020-7-11 看剧，跑程序</p>
<p>今天早上睡觉，看视频</p>
<p>今天下午和晚上，处理数据，啊麻烦</p>
<hr>
<h4><span id="2020-7-10-pao-cheng-xu">2020-7-10 跑程序</span><a href="#2020-7-10-pao-cheng-xu" class="header-anchor">#</a></h4><p>今天早上跑程序（封装接口）</p>
<p>今天下午跑程序，学习写作的思维</p>
<p>今天晚上跑程序，看纪晓岚和和珅。</p>
<hr>
<h4><span id="2020-7-9-ke-yan-xie-zuo-hao-tong-ku">2020-7-9  科研写作好痛苦</span><a href="#2020-7-9-ke-yan-xie-zuo-hao-tong-ku" class="header-anchor">#</a></h4><p>今天早上听了听力，刷B站</p>
<p>今天下午学习了如何进行科研写作。太不容易了。《Some Tips on Writing》。原因如下：词汇缺乏；直译；毫无讲故事的思路；交代不清晰，含糊。更别提逻辑了，我觉得中文论文的逻辑关系都把握不好。</p>
<hr>
<h4><span id="2020-7-8">2020-7-8</span><a href="#2020-7-8" class="header-anchor">#</a></h4><p>今天早上啃了一个🍎。 刷刷小红书，B站。</p>
<p>下午写了代码注释。封装成接口。规划了下一步方案。</p>
<p>晚上读了一篇Data Science的文章，数据好才能讲好故事啊。引力模型、线性回归模型。变量的构造是亮点。每天坚持写作（英文写作好难）</p>
<hr>
<h4><span id="2020-7-7">2020-7-7</span><a href="#2020-7-7" class="header-anchor">#</a></h4><p>下午实现了xgboost, 数据-&gt;模型-&gt;调超参数</p>
<p>晚上收拾了论文，感觉就像俄罗斯方块，底层没有做好，越往上堆，就要回归底层。写了1000多点的小论文，也不知道每一个优秀的博士要经历什么才能创造出这么多的文字。值得学习。</p>
<p>释放光芒！</p>
<hr>
<h4><span id="2020-7-6-ji-xu-nu-li-de-yi-tian">2020-7-6 继续努力的一天</span><a href="#2020-7-6-ji-xu-nu-li-de-yi-tian" class="header-anchor">#</a></h4><p>今天早听了英语访谈</p>
<p>今天下午复习了密码学第1-2章（1h);  </p>
<hr>
<h3><span id="2020-28-zhou">2020 28周</span><a href="#2020-28-zhou" class="header-anchor">#</a></h3><hr>
<p>2020-7-5 要做的事情还很多，我努力做我喜欢的一切，并不是为了赢别人，而是要自己满足。</p>
<p>今天又是睡得晚起的早的一天。a. 我把trfersh完全搞懂了。b. 谈整合资料的重要性，为什么我总是一个专题的资料，每次都要重复弄呢。不好不好不好。</p>
<p>这一周还是不错的，在精神上，每天睡足了10h，啊哈哈哈哈哈。时间序列基本上搞通了。读了几篇不错的论文，还把stata软件学会了，其实计量经济学无非就是要解决因果关系，遗漏变量，序列相关性，异方差性等等，几个问题，要解决是不容易的。sklearn用的不熟练</p>
<p>Input  ✔️ Output ❌✔️</p>
<p>这周主要把统计学里面的基础复习了，总感觉没找到我想要的那种深度。我也不知道我到底需要怎么样的深度。感觉自己又胖了！</p>
<font size="5" color="red">Input</font>

<ol>
<li>密码学复习两章（PPT+习题+百度）</li>
<li>网络安全复习两章（视频+笔记+概念）</li>
<li>写1000字左右的结课论文（关于大数据下孕育而生的计算社会经济学，抄袭抄袭，借鉴咯）</li>
<li>封装特征提取</li>
<li>论文数据分析(现在就差分析结果)（不知道我怎么会给造成一种，我很会写论文的样子，想太多了吧) 现在还是数据驱动研究，并非研究问题驱动数据的阶段</li>
<li>准备六级</li>
<li>reading record</li>
</ol>
<font size="5" color="red">Output</font>

<ol>
<li>基本的数据分析结果</li>
<li>论文阅读：一定要以PPT的形式给出（每周给自己做个1-3页的PPT)</li>
<li>看一道建模题</li>
<li>学习机器学习集成算法</li>
</ol>
<hr>
<h4><span id="2020-7-4-lan-de-de-wo">2020-7-4 懒得的我</span><a href="#2020-7-4-lan-de-de-wo" class="header-anchor">#</a></h4><p>今天早上又睡了很久！可以是昨天晚上看别人的vlog太久了，羡慕那种独居生活。我什么时候可以过上独居的日子啊！</p>
<p>今天下午学了stata这个工具，基本上学会了，我发现机器学习开源工具，有些不良心啊！</p>
<p>今天晚上看了会电视剧。跟踪公众号。学习统计学。</p>
<hr>
<h4><span id="2020-7-3-shui-bao-he-zu-du-hui-lun-wen-amp-qiao-dai-ma">2020-7-3 睡饱喝足，读会论文，&amp; 敲代码</span><a href="#2020-7-3-shui-bao-he-zu-du-hui-lun-wen-amp-qiao-dai-ma" class="header-anchor">#</a></h4><p>今天下午玩了xgboost,集成学习有点难，但也要手动推导，这可以加深对算法内涵的理解。</p>
<p>晚上统计学习，stata和计量经济学（学的很基础，但实际应用不是那么回事了）。做数据科学，肯定要学习R语言</p>
<p>沉思：😔😔😔。</p>
<hr>
<h4><span id="2020-7-2-xian-ru-shen-yuan-de-wo">2020-7-2 陷入深渊的我</span><a href="#2020-7-2-xian-ru-shen-yuan-de-wo" class="header-anchor">#</a></h4><p>今天又是上课，不知道老师扯了些什么，听着听着就关成静音了。(阴险.jpg)</p>
<p>实现了时间序列的特征工程，读了一篇好文章（关于女性政治地位的提高），还找到了一篇EPJ data science上的好文章（追踪文献的重要性）</p>
<p>明天：1）实现xgboost 2) 模型设置（中文）综述和变量 </p>
<hr>
<h4><span id="2020-7-1-wu-liao-de-wo">2020-7-1 无聊的我</span><a href="#2020-7-1-wu-liao-de-wo" class="header-anchor">#</a></h4><p>今天玩了tsfresh库，怎么那么难理解啊！实在是不知道别人怎么编程的！关键是返回的数据结构，和底层实现有关系！再次学习了参数估计（极大似然估计，最大后验估计）。我可能要手把手教那种。</p>
<hr>
<h3><span id="202006">202006</span><a href="#202006" class="header-anchor">#</a></h3><h4><span id="2020-6-30-xin-lei-de-yi-tian">2020-6-30 心累的一天</span><a href="#2020-6-30-xin-lei-de-yi-tian" class="header-anchor">#</a></h4><p>今天早上，下午上课，是张老师的课，课程难度很大。我也是半听半玩耍。可能是数学学多了，对这个社会问题感悟能力跟不上。就一句话存在即合理！顺道看了tsfresh的doc</p>
<p>晚上，读了一篇science，开创性工作就是厉害啊！从0到1是飞跃，1到2，3…是发展。还有跨界的精英。温习了机器学习里面的基本概念！</p>
<hr>
<h4><span id="2020-6-29">2020-6-29</span><a href="#2020-6-29" class="header-anchor">#</a></h4><p>今天早上，下午上课去了。还去看了回归分析sklearn的实现。 关键点相关性分析-&gt;因果分析， correlation － causality - prediction - control。解释型研究（描述，统计方法)——&gt;充分解释（因果关系）科学研究： 解释，预测，控制闭环（干预），找不到因果关系。 因果关系分析方法：Causality: Models, Reasoning, and Inference</p>
<p>晚上复习统计学day Ox 01（本科我怎么没有作电子稿笔记啊，哭死了，还去反翻看了过去的笔记和博客，菜死了）。加个学习小组，感觉别人做的资料，和我想要的深度差太远了。数据挖掘上次看那个Dr.yuan的课程，笔记不是特别好！还要把时间序列特征工程做了！</p>
<p>我发现申请实践学分，做助教，怎么那么多人申请啊！</p>
<p>统计学习方法那个，我发现我做的笔记，好差劲啊！</p>
<hr>
<h4><span id="2020-6-28-yuan-qi-shao-nu-de-du-bai-ge-ren-zhi-lu">2020-6-28 元气少女的独白— 个人之旅</span><a href="#2020-6-28-yuan-qi-shao-nu-de-du-bai-ge-ren-zhi-lu" class="header-anchor">#</a></h4><hr>
<h3><span id="2020-27-zhou-gong-xing-shi-jian">2020 27周：躬行实践</span><a href="#2020-27-zhou-gong-xing-shi-jian" class="header-anchor">#</a></h3><hr>
<font color="red" size="5"> INPUT </font>

<p>a.  Linear Models &amp; Python 学会 <a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning</a> </p>
<p>b. Time series &amp; Feature select tfresh</p>
<p>c.  English: youtube &amp; shanbei</p>
<p>e.  reading record</p>
<p>f.  概率论（极大似然估计，假设检验，显著性检测，参数估计）</p>
<font color="red" size="5">OUTPUT</font>

<p>a. kaggle project</p>
<p>b. 掌握一个中高难度的数学模型</p>
<p>c. Reading Record</p>
<hr>
<p>今天早上睡觉啦！</p>
<p>下午和电脑人一起玩麻将，收拾了本周的资料汇总</p>
<p>晚上看了视频学习关于xgboost,和关于一些公众号的文章</p>
<p>F:</p>
<p>​    输出（✔️）； 论文阅读（✔️）；读书（❌，主要是没有那种心境，不上瘾）；英语（✔️）</p>
<p>S:</p>
<ol>
<li><strong>论资料的收集的重要性</strong>。知识在脑海里会忘记，但硬盘不会。一定要把自己遇到的顶级资料收集好，厘成doc。一定是高质量和高密度的资料才记录。原来一直注重资料的学习，忽略了资料整合的重要性。并且最好每段时间，写一篇大汇总。</li>
<li><strong>切记从头开始</strong>。学习回路：资料收集（分等级：全局观—&gt;入门-进阶-高级，科普)——&gt;</li>
<li><strong>把自己的输出提升到别人也可以直接用的水平</strong>。不错，是这么回事！这也是对自己的一种要求。</li>
</ol>
<p>快乐夹杂着悲伤，至少结尾不能让自己太难堪！这条路只会与自己相关。</p>
<hr>
<h4><span id="2020-6-27-wai-gong-ba-shi-sui-sheng-ri">2020-6-27 外公八十岁生日</span><a href="#2020-6-27-wai-gong-ba-shi-sui-sheng-ri" class="header-anchor">#</a></h4><p>白天在外婆家，给教书先生外公过生日。</p>
<p><strong>你永远无法叫醒一个装睡的人</strong></p>
<p><strong>当你真心渴望某样东西时，整个宇宙都会联合起来帮助你完成！</strong></p>
<p>学到了两句话，我不知道选哪个！</p>
<p>关键词: 硬核</p>
<hr>
<h4><span id="2020-6-26">2020-6-26</span><a href="#2020-6-26" class="header-anchor">#</a></h4><p>今天早上，睡到了11：00</p>
<p>下午，查重，心累！时间序列！</p>
<p>晚上，读了一篇时间序列在预测能源消费里面的大综述。和本科的经历契合。重点看了最小二乘支持向量机的应用，研究结果拟合精度和预测精度还行。这些模型，我还挺熟悉的。</p>
<hr>
<h4><span id="2020-6-25-duan-wu-kuai-le-biubiubiu">2020-6-25 端午快乐,biubiubiu</span><a href="#2020-6-25-duan-wu-kuai-le-biubiubiu" class="header-anchor">#</a></h4><p>早上，睡到了中午，感觉太热了，不想早起。</p>
<p>中午，我和弟弟去街上吃饭了：豆腐、水饺、水煮肉片。感觉我要是愿意做一次饭，那简直是太阳从西方出来了。妈妈果然是世界上最辛苦的人了。</p>
<p>下午，交接一下时间序列项目，话说这个东西不是很火吗，为什么资料这么少啊！！！跑了一天一夜ARIMA了。帮我表弟写一篇中小企业发展现状的论文，找了几篇文章抄袭，看了别人的硕士学位论文，我才意识到我还是本科论文的水平。</p>
<p>晚上，研读论文。</p>
<hr>
<h4><span id="2020-6-24-7h-shui-mian-bu-zu-de-wo-jpg">2020-6-24 7h睡眠不足的我.jpg</span><a href="#2020-6-24-7h-shui-mian-bu-zu-de-wo-jpg" class="header-anchor">#</a></h4><p>今天早上迷迷糊糊起床，和同学唠嗑，昨天睡的好玩，1：30，听说墨西哥地震了，然后我发了消息，居然心安理得的睡着了！</p>
<p>晚上，了解时间序列的特征提取，什么鬼，怎么那么难啊！晚上看了大数据机器学习时代，科普。</p>
<p>突然想到还有好多复习的课程要考试啊，突然想到了。</p>
<p>Probit vs logit</p>
<p><a href="https://www.econometrics-with-r.org/11-2-palr.html">https://www.econometrics-with-r.org/11-2-palr.html</a></p>
<h4><span id="2020-6-23-wo-yao-fang-chang-jia-mp4">2020-6-23 我要放长假.mp4</span><a href="#2020-6-23-wo-yao-fang-chang-jia-mp4" class="header-anchor">#</a></h4><p>今天睡了10h多。晚上十一点睡到早上11.00，中途还迷迷糊糊的在研究生系统打卡！！！！已经是自然状态了啊！</p>
<p>下午：继续时间序列，理论完全不懂。这是怎么回事。</p>
<p>晚上：开开心心复习四川大学的概率论。好久没有接触机器学习了，差不多还给书本了。</p>
<p>机器学习算法+概率论统计学</p>
<hr>
<h4><span id="2020-6-22-ru-guo-ke-yi-yi-zhi-yi-zhi-shui-xia-qu-duo-hao-a-eps">2020-6-22 如果可以一直一直睡下去多好啊.eps</span><a href="#2020-6-22-ru-guo-ke-yi-yi-zhi-yi-zhi-shui-xia-qu-duo-hao-a-eps" class="header-anchor">#</a></h4><p>今天睡了11多个小时，太幸福了啊！</p>
<p>下午玩了会时间序列。和灯夜聊了很久的天，就是关于兴趣和人生追求。</p>
<hr>
<hr>
<h4><span id="2020-6-21">2020-6-21</span><a href="#2020-6-21" class="header-anchor">#</a></h4><p>今天上街去了，买了一个大大的冰淇淋。</p>
<font size="12," color="red">输出</font>

<ol>
<li>完成一篇博客</li>
<li>推进时间序列项目（读相关论文）</li>
<li>论文阅读</li>
</ol>
<p>今天早上和下午睡了好久啊！</p>
<p>下周规划：</p>
<ol>
<li><p>时间序列项目（主线1）：具体把相关的方法整理成笔记</p>
</li>
<li><p>统计学（主线2）：统计学课程每天学一学，做练习。</p>
</li>
</ol>
<p>mooc: <a href="https://www.icourse163.org/learn/NJUE-1001752031?tid=1206103246&amp;from=study#/learn/content">https://www.icourse163.org/learn/NJUE-1001752031?tid=1206103246&amp;from=study#/learn/content</a></p>
<ol>
<li><p>编程：SQL、Excel、Python的精髓用法，本着提高效率去的。</p>
</li>
<li><p>看剧 《白箱》</p>
</li>
<li><p>看两篇机器学习回归分析的论文</p>
<p>统计学习补上</p>
</li>
<li><p>书籍阅读：</p>
<p>​    《大秦帝国》</p>
<p>​    《人性的弱点》</p>
</li>
<li><p>每天2h的英语学习，过六级啊</p>
</li>
</ol>
<p>反思：</p>
<p>​    日出而作，日落而息。</p>
<hr>
<h4><span id="2020-6-20-xin-sai">2020-6-20 心塞</span><a href="#2020-6-20-xin-sai" class="header-anchor">#</a></h4><p>下午学了python技巧绘制各类bar,barh</p>
<hr>
<h4><span id="2020-06-19">2020-06-19</span><a href="#2020-06-19" class="header-anchor">#</a></h4><p>下周规划：</p>
<p>时间序列项目（主线1）：具体把相关的方法整理成笔记</p>
<p>统计学（主线2）：统计学课程每天学一学，做练习。</p>
<p>编程：SQL、Excel、Python的精髓用法，本着提高效率去的。。</p>
<p>晚上：恶补pythonic和统计学</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzI1MzAwODMyMQ==&amp;mid=2650338461&amp;idx=1&amp;sn=be67a2565cf5f0922e84e5076fe1c0d2&amp;chksm=f1d75433c6a0dd25fa6e25fae624cb21738b0fc4b28f833db0844f8bbb6e02c1dc9dfddac03a&amp;scene=0&amp;xtrack=1#rd">https://mp.weixin.qq.com/s?__biz=MzI1MzAwODMyMQ==&amp;mid=2650338461&amp;idx=1&amp;sn=be67a2565cf5f0922e84e5076fe1c0d2&amp;chksm=f1d75433c6a0dd25fa6e25fae624cb21738b0fc4b28f833db0844f8bbb6e02c1dc9dfddac03a&amp;scene=0&amp;xtrack=1#rd</a></p>
<hr>
<h4><span id="2020-06-18-xin-bu-zai-yan-quan-zhan">2020-06-18 😴 心不在焉 。全栈。</span><a href="#2020-06-18-xin-bu-zai-yan-quan-zhan" class="header-anchor">#</a></h4><p>今天发现自己离全栈在程度在降低。</p>
<ol>
<li><p>今天和萌萌组个学习队，加油，考研加油！！！！！！</p>
</li>
<li><p>就是参考文献的插入。</p>
<p><img src="/2019/02/22/%E5%AD%A6%E4%B9%A0Daily/微信图片_20200618225004.jpg" alt="微信图片_20200618225004" style="zoom: 25%;"></p>
</li>
</ol>
<hr>
<h4><span id="2020-06-17-wan-de-fun">2020-06-17 玩的fun</span><a href="#2020-06-17-wan-de-fun" class="header-anchor">#</a></h4><p>明天：</p>
<p>如果不出意外，尽量睡到九点！看会电视剧！</p>
<p>继续跑程序</p>
<p>学习统计学</p>
<p>下午搞了会数据</p>
<p>晚上：</p>
<ol>
<li>规划一会统计学学习</li>
<li>看了下自己的塑身计划，纠正体型，感觉还差好多啊</li>
<li>看下自己的技能树</li>
</ol>
<hr>
<h4><span id="2020-06-16">2020-06-16</span><a href="#2020-06-16" class="header-anchor">#</a></h4><ol>
<li><p>下午和爷爷聊天了，</p>
</li>
<li><p>晚上学习了新知识</p>
<p>继续学习统计学</p>
</li>
</ol>
<p>野外一游</p>
<hr>
<h4><span id="2020-6-15-er-ci-yuan-shao-nu-de-wei-xiao-jpg">2020-6-15 二次元少女的微笑.jpg</span><a href="#2020-6-15-er-ci-yuan-shao-nu-de-wei-xiao-jpg" class="header-anchor">#</a></h4><hr>
<h4><span id="2020-6-14-kai-xin-shui-jue-ing">2020-6-14 开心睡觉ing</span><a href="#2020-6-14-kai-xin-shui-jue-ing" class="header-anchor">#</a></h4><p>今天家里没有网，断网啊！</p>
<p>复盘这周工作，感觉要把某个知识、理论学到脑子里，不忘记，随机应变，太难了！</p>
<p>​        第一：跑程序。我觉得要跑大程序，才是把程序精髓学到家，时间效率。</p>
<p>​        第二：阅读。显而易见的现象问题啊。</p>
<p>​        第三：课程结课。 几门课程，感觉不是自己当初想选的，学起来怎么都不顺心。我就学不来了，配不上！</p>
<p>最主要的收获</p>
<p>​    pandas不断新增行的方法。</p>
<p>​    pandas if 的用法</p>
<h4><span id="2020-6-15-2020-6-21-ji-hua">2020-6-15—2020-6-21计划</span><a href="#2020-6-15-2020-6-21-ji-hua" class="header-anchor">#</a></h4><p>感觉自己是听蠢笨的啊。</p>
<hr>
<h4><span id="2020-6-13-bu-bei-bu-xi-png">2020-6-13 不悲不喜.png</span><a href="#2020-6-13-bu-bei-bu-xi-png" class="header-anchor">#</a></h4><p>今天a .出门拜访亲戚，身心疲惫</p>
<p>b. 好想回学校啊，想念食堂，想念杉杉宝贝</p>
<p>c. 什么能力都在下降。。。。不好的预感啊！过去学的东西，长时间搁置，已经发霉了，啊，啊，啊，啊，啊， 啊———————————</p>
<p>d. 还是要多元化发展，感觉自己已经是单一维度了，不知道怎么回事。</p>
<p>e. 我发现我记忆力衰退的太厉害了，脑子不够灵活，为什么呢。要及时记录啊。</p>
<p>好有效率问题啊，效率啊，效率啊，效率啊！</p>
<p>f. 看清自己，终能看破红尘。</p>
<p>今天：遇到的一些问题：</p>
<p>​    1. 对应DataFrame数据类型的行索引。在插入新行的问题，从其他DataFrame或者自定义插入。可以用append()在末尾新增行，但传参数切记是列表，最好用DataFrame</p>
<pre><code> 2. 还有列表和字符串的相互转换。如果要把[]传入DataFrame
</code></pre><hr>
<h4><span id="2020-6-12-bu-jiu-jie-liao-svg">2020-6-12 不纠结了.svg</span><a href="#2020-6-12-bu-jiu-jie-liao-svg" class="header-anchor">#</a></h4><p>明天周末了。今晚要追剧；车水马龙；在月光下奔跑，我什么都不想要，你爱我就好</p>
<p>今天是没有课的一天。还有杉杉在学校了。</p>
<hr>
<h4><span id="2020-6-11-ban-shui-ban-xing">2020-6-11 半睡半醒 :</span><a href="#2020-6-11-ban-shui-ban-xing" class="header-anchor">#</a></h4><p>明天</p>
<p>​        今天的任务没有做完，明天继续</p>
<p>今天</p>
<p>跑程序的过程中，把文件夹清理了</p>
<p>那么有趣，那么有价值-——评判公平性标准</p>
<p>品读了《喜欢你是寂静的》</p>
<p>听了陈奕迅《我要稳稳的幸福》</p>
<hr>
<h4><span id="2020-6-10-kai-xin-eps">2020-6-10 开心.eps</span><a href="#2020-6-10-kai-xin-eps" class="header-anchor">#</a></h4><p>明天：</p>
<ol>
<li>调用时间序列指标 2. 线性回归模型 3. 跑程序，计算人才流动指标</li>
</ol>
<hr>
<h4><span id="2020-6-9-yu-yue-jpg">2020-6-9 愉悦.jpg</span><a href="#2020-6-9-yu-yue-jpg" class="header-anchor">#</a></h4><p>今天：</p>
<ol>
<li>精读了一篇论文，收获满满，因为都是自己学过，看过的方法，看别人论文里面的应用，有所启发！以后要看顶级的资料</li>
<li>喝着安慕希，看别人的数据分析报告。</li>
<li>最近知识输入太过了，虽然都是本科了解过的（数学方面）</li>
<li>重要要清洗出数据，不知道为什么，我发现我的数据，居然只读了十几万条，然后重新跑。</li>
<li>等忙过了这段时间，一定要好好厘清最近的思路<ol>
<li>可视化</li>
<li>数据清洗：python</li>
<li>线性预测模型</li>
<li>计量经济学</li>
</ol>
</li>
</ol>
<hr>
<h4><span id="2020-6-8-yuan-qi-kai-xin-xi-chu-wang-wai">2020-6-8 元气 开心 喜出望外</span><a href="#2020-6-8-yuan-qi-kai-xin-xi-chu-wang-wai" class="header-anchor">#</a></h4><p>明天1. 基本数据统计结果 2. 项目交接！3. good night</p>
<ol>
<li><p>今天早</p>
</li>
<li><p>今天做了的数据序列项目上周的报告！</p>
</li>
<li><p>今天终于预处理完论文数据了</p>
</li>
</ol>
<h4><span id="2020-6-6">2020-6-6</span><a href="#2020-6-6" class="header-anchor">#</a></h4><ol>
<li>今天又是元气满满的一天</li>
<li>温习了numpy的用法，numpy,pandas,list直接的相互转换关系，以及sklearn metric里面的各类指标，什么叫指标的鲁棒性，怎么按需求选或者构造适合自己的评估指标。</li>
<li>读了高见师兄的综述。</li>
<li>做了指标体系图和高铁站点（Edraw中文坑，怎么不是矢量图啊，Visio破解不安全，AI还不知道怎么操作）</li>
<li>明天录视频，</li>
</ol>
<hr>
<h4><span id="2020-6-5">2020-6-5</span><a href="#2020-6-5" class="header-anchor">#</a></h4><ol>
<li>今天熟悉了时间序列流程；1. 平稳性。序列平稳性是做分析的基础。平稳性检测—单位根；非平稳性处理：差分~log~分解~平滑处理；2. ARIMA模型 基本思路；相关性和偏相关性定阶数 3. 网格+信息准则 调参：惩罚性 nbeats工具封装好了 4. predict样本内，动态和静态预测 forecase外推，timedelt 绘图，指标</li>
<li>假设检验：统计量和显著性水平，总感觉理解起来太难了，等忙了一定要好好复习统计学，参数估计，显著性检测，好好补。</li>
<li>记不住的函数<ol>
<li>pd.date_range(start = sub.index[-1],end = sub.index[-1]+timedelta(days = 2),freq = ‘1h’)</li>
<li>stat_rawdata = rawdata[rawdata[‘站点名称’]==stat] 布尔类型的切片，不上很明白原理</li>
<li>plt.xlim(sub.index[0],sub.index[-1]+timedelta(days = 2))</li>
<li>绘制热力图<ol>
<li>seaborn里面的heamap()<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line">x = np.random.rand(10, 10)</span><br><span class="line">sns.heatmap(x, vmin=0, vmax=1, center=0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4><span id="2020-6-4">2020-6-4</span><a href="#2020-6-4" class="header-anchor">#</a></h4><p>今天最大的头疼之处和领悟就是，跑大型程序，一定要准备一份下程序，调好结果才放在服务器上面跑，不然结果难以想象。。。。。。。。。。。。。</p>
<ol>
<li>感觉调包侠也不是想象中那么好当的，一是现实中的数据一般般不规则，我发誓以后一定要做个合格的客户，二是参数默认是最讨厌的</li>
<li>今天又改程序，数据预处理果然头疼啊！希望周末可以出个大概结果，再重新跑数据</li>
</ol>
<p>绝知此事要躬行，原来觉得自己学各种库还不错，实践的时候还是用不上的时候，😔</p>
<ol>
<li>还有就是命名的重要性，</li>
<li>准备出一篇实践的心得，月末，主要是现在写程序太安逸了，</li>
<li>知识<ol>
<li>pandas新增列</li>
<li>更改索引</li>
<li>range()</li>
<li>行政单位，变更情况</li>
<li>函数，写函数，不知道为什么，像python这种解释型的语言，自己老是重复，不好，不好，</li>
</ol>
</li>
</ol>
<hr>
<h4><span id="2020-6-3">2020-6-3</span><a href="#2020-6-3" class="header-anchor">#</a></h4><p>今天还是学了不少东西，效率不高，主要是记不住函数传参数！</p>
<ol>
<li>掌握了一些pythonic的用法，想in for if ; if else 的一行代码；lambda单行函数功能的简化等等</li>
<li>一些常用的数据分析函数，sort_values();concat; append; time_range();绘图功能；就是就不知道，肯定是别人已经封装好了！</li>
</ol>
<hr>
<h4><span id="2020-6-2">2020-6-2</span><a href="#2020-6-2" class="header-anchor">#</a></h4><p>今天总体还是满不错的！</p>
<hr>
<h4><span id="2020-6-1">2020-6-1</span><a href="#2020-6-1" class="header-anchor">#</a></h4><p>今天刚好是2020年的一半，忙完第一学期（半梦半醒）,最终如梦初醒！<br>今天接触许多新鲜的玩意：</p>
<ol>
<li>ArcMap绘制地图、流向图（关键是起点和终点坐标，如何根据名称获取坐标，要做连接。Arc ToolBox中提供了许多工具，方便了用户完成一些简单的操作，如Join、Excel to Tabel常用的工具箱。今天学习绘制地图和流向图涉及的操作包括：文件夹链接到工作目录；ArcMap导入Excel坐标数据并显示；XY to Line工具流向图；提取面要素的质心点；多表链接操作；属性设置（bar)。渲染结果的确漂亮</li>
<li>linux系统后端运行 nohup &amp;命令的使用，如何记录日志文件，定向输出；学了ps命令 ps 查看进程；还有|通道，grep查找； ps -ef| grep pyton</li>
<li>matplotlib绘图的原理。<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">创建figure后，还需要轴</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(221)</span><br><span class="line">ax2 = fig.add_subplot(222)</span><br><span class="line">ax3 = fig.add_subplot(224)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(nrows=2, ncols=2)</span><br><span class="line">axes[0,0].set(title='Upper Left')</span><br><span class="line">axes[0,1].set(title='Upper Right')</span><br><span class="line">axes[1,0].set(title='Lower Left')</span><br><span class="line">axes[1,1].set(title='Lower Right')</span><br><span class="line"></span><br><span class="line">axes[0,0].plot()</span><br><span class="line">axes[0,0].set_xlim([-1,6])</span><br><span class="line">axes[0,0].legend()</span><br></pre></td></tr></tbody></table></figure>
matplotlib.plot的基础绘图流程：</li>
</ol>
<p>创建画布（选择是否绘制子图，指定画布大小，像素）<br>添加标题—添加x轴的名称，刻度与范围—添加y轴的名称，刻度与范围<br>绘制图形，设置图形的样式，颜色，背景，并添加图例<br>保存图形，显示图形</p>
<p>感谢爷爷提供的CSDN账号 下载不少资料，一次性解决完了！</p>
<h3><span id="202005">202005</span><a href="#202005" class="header-anchor">#</a></h3><h4><span id="2020-5-31">2020-5-31</span><a href="#2020-5-31" class="header-anchor">#</a></h4><p>这是上完研究生第一学期上，第二学期下中旬！也是隔了很久才更新个人记录。<br>研一上，换了一个新环境，终于有个特别舒适的环境了！大屏幕，柔软的凳子,太舒服了！怎可辜负呢。<br>电科的课程实在是太多了，都不能安心玩耍了！<br>这段时间算个积淀吧！</p>
<h3><span id="201906">201906</span><a href="#201906" class="header-anchor">#</a></h3><hr>
<p>2019.6.2</p>
<p>今天才来写，罪过啊！老师不在，室友不在，无聊至极，只能以视频唯友，解闷也！睡了一个星期，但是我瘦了2斤了。开始不在懒惰了，在这样下去我就完蛋了。。。。。。。</p>
<p>不知道怎么回事，VPN老是 time out ,明明还有$啊！</p>
<p>造成了只能一段一段翻译，无语凝噎ing</p>
<p>还有百度上说是两篇，可是要求是一篇，于是我多花了2h,天杀的！！！</p>
<p>下午跑了个步，</p>
<p>越来越发现，事先计划一下，再去做，效率更好了！</p>
<h3><span id="201905">201905</span><a href="#201905" class="header-anchor">#</a></h3><hr>
<p>2019.5.12</p>
<p>今天才来更新这个日常，真是罪过，上段时间一直忙其他的事情，毕业设计，回家，处理家务啊。直到今天学完了Course 3,如何改善神经网络的性能</p>
<h3><span id="201904">201904</span><a href="#201904" class="header-anchor">#</a></h3><hr>
<p>2019.4.18</p>
<p>今天学了一部分course two week two的课程，终于学到精彩的部分了，由于今天想练彩铅和帮忙收拾教室，所以就没有学完了，学了几种变种的梯度下降法，提高速度，太棒了，太棒了，太棒了！！！！！！</p>
<hr>
<p>2019.4.17 </p>
<p>今天学完国course two的第一周课程，大概记时4h，还是颇多收获，学到了以前完全没有接触的东西，正则化方法，梯度消失的和爆炸的问题，感觉很棒，</p>
<p>2019.4.16</p>
<p>今天学了吴恩达老师的第一周关于神经网络的基础的课程，好厉害的</p>
<p>2019.4.15</p>
<p>今天学习了deep-layer neural network的正向传播和反向传播的过程，矩阵化计算的方式。</p>
<hr>
<p>2019.4.14</p>
<p>今天学习了第三周课程，shallow (2层)神经网络的正向传播和反向传播，以及矩阵化的计算方式，以及和logistics regression的表示上的不同地方</p>
<hr>
<p>2019.4.13</p>
<p>今天继续学习第二周课程，logistics regression 的模型，策略，算法的相关，如何把学习的到数学知识应用上去。</p>
<hr>
<p>2019.4.12</p>
<p>今天没有去跑步，处理一些情感问题去了，学了Androw Ng的 第二周的部分课程，主要是链式求导法则，通过计算图，一步一步的复合求导，复合函数用计算图表示，并链式求导</p>
<p>2019.4.11</p>
<p>今天是很愉快的一天，起的很早，精神很足，学了学习，写完了吴恩达老师的第一周深度学习。</p>
<hr>
<p>2019.4.10</p>
<font color="red" size="6">今天刷完了第一遍，完了西瓜书第一章到第十一章，不过呢，还是要继续在刷基础还看论文</font><font>

这段时间，跑步锻炼第一，睡觉，贪睡第二，第三，开始散漫了，自律的我，在哪里去了啊！ 争取在上研究生这段时间，把机器学习、Python编程能力提升上去

今天晚上弄了一个，吴恩达的deep Learning Ai课程笔记，<font color="blue" size="5">开始学习这个系列的课程了</font>

<p>笔记模板</p>
<p>英文：<a href="http://dl-notes.imshuai.com/#/">http://dl-notes.imshuai.com/#/</a>](<a href="http://dl-notes.imshuai.com/#/c1w1">http://dl-notes.imshuai.com/#/c1w1</a>)</p>
<p>什么都有的 <a href="https://redstonewill.com/category/deeplearning/">https://redstonewill.com/category/deeplearning/</a></p>
<p>中文笔记：</p>
<p>详细，相当于翻译： <a href="http://www.ai-start.com/dl2017/html/lesson1-week1.html">http://www.ai-start.com/dl2017/html/lesson1-week1.html</a></p>
<p>思考和总结： <a href="https://link.zhihu.com/?target=http%3A//kyonhuang.top/Andrew-Ng-Deep-Learning-notes/">https://link.zhihu.com/?target=http%3A//kyonhuang.top/Andrew-Ng-Deep-Learning-notes/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/35333489">https://zhuanlan.zhihu.com/p/35333489</a></p>
<p><a href="http://imshuai.com/tag/deeplearning-ai-notes/">http://imshuai.com/tag/deeplearning-ai-notes/</a></p>
<p><a href="https://zhuanlan.zhihu.com/koalatree">https://zhuanlan.zhihu.com/koalatree</a></p>
<p>上班族的学习时长：<br>  整理笔记要明显消耗更多的时间，基本上10分钟的视频，做笔记至少要50分钟才能看完。每周的视频大概在2-3个小时，这就意味着看学习视频就要10-15个小时。编程作业3个小时左右，每周平均在15个小时，16周的课，净时间就花了240个小时！</p>
<p>video  <a href="https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists">https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists</a></p>
<p>西瓜书的公式推导：</p>
<p> <a href="https://datawhalechina.github.io/pumpkin-book/#/chapter1/chapter1">https://datawhalechina.github.io/pumpkin-book/#/chapter1/chapter1</a></p>
<p>统计机器学习：</p>
<p><a href="https://github.com/SmirkCao/Lihang">https://github.com/SmirkCao/Lihang</a></p>
<h3><span id="201903">201903</span><a href="#201903" class="header-anchor">#</a></h3><hr>
<p>2019.3.23-2019.3.24 </p>
<p>今天（今天）是周末，两个早上都睡觉去了，处理一下，我弟弟的问题，和孩子交流的重要性，晚上什么都没有干，周日下午跑了步，学习了python sci-learn里面的交叉验证、超参数选择，集成学习库的用法，终于不再迷茫了，慢慢来</p>
<ol>
<li><p>及时的记录学习过程</p>
</li>
<li><p>多去看别人的进展</p>
</li>
<li><p>去做别人没有做过的事情</p>
<p>只有你努力，努力的方向是正确的，就可以做出伟大的成就</p>
<p><a href="https://zhuanlan.zhihu.com/p/29704017">https://zhuanlan.zhihu.com/p/29704017</a></p>
<hr>
</li>
</ol>
<p>2019.3.21</p>
<p>今天了解一下操盘，就是那个股票，感觉做数据科学家好吃香啊，加油，接下来有得忙了</p>
<hr>
<p>2019.3.20 </p>
<hr>
<p>2019.3.19</p>
<hr>
<p>2019.3.18</p>
<p>今天看病去了，睡了好久，药物作用果然出乎意料，</p>
<p>2019.3.17</p>
<p>今天主要学习支持向量机的前世今生，怎么由来的</p>
<hr>
<p>2019.3.16</p>
<hr>
<p>2019.3.14</p>
<p>今天改了论文，感觉快完了；学习了BP矩阵推导，人真的是越来越聪明和灵活</p>
<p>2019.3.13</p>
<p>今天晚上学完了回归树，好棒，虽然原理给人的感觉很直接，但是也是一种体现</p>
<hr>
<p>2019.3.12</p>
<p>今天跑了步，早上改了毕业设计，下午配置了新手机，晚上写了日记</p>
<p>2019.3.11</p>
<p>今天早上听了听力，修改了毕业设计，核函数还是不是很懂；下午看了决策回归树，感觉很棒，过拟合才是该解决的关键问题！跑了步，感觉自己身体状况很不好啊！ 晚上听力电台，写了程序！</p>
<p>慢慢的学习，慢慢的努力，慢慢的加油，慢慢的遇见自己的憧憬天空</p>
<hr>
<p>2019.3.10</p>
<p>今天写了日记，说不上自己哪里郁闷，哪里开心！希望早点跑完程序，早点完事</p>
<hr>
<p>2019.3.9</p>
<p>今天才发现，最近状态非常的不好，可能是无所事事吧，可是我有很多事情要做，加油，少女，加油，少女，以后一定要记得写了!总觉得自己逃不出自己的羁绊，被束缚</p>
<h3><span id="201902">201902</span><a href="#201902" class="header-anchor">#</a></h3><hr>
<p>2019.2.28<br>今天调整自己的心态，回顾自己的生活，自己太急于求成了，心急吃不了热豆腐啊！！！！！<br>中国有句老话说得对，积少成多，不积跬步无以至千里，不积小流无以成江河，慢慢来，弄透弄清楚</p>
<hr>
<p>2019.2.27<br>今天早上，我读了同校同学写的简书文章，实在是感到好想笑，搜索了别人的解决方法，还是自己的知识量不足啊！！！！！！！！<br>下午看了下假设检验的视频，再去跑了两个小时的步</p>
<p>晚上逛了一下午各位网站</p>
<hr>
<p>2019.2.26<br>今天看了下某个计算机大佬的历程，深深地感到佩服。<br>《梦想小镇》又多了一块地盘了，</p>
<p>又重新学习了matplotlib,才发现,然后了玩了一下sklearn里面的带cross-validation的lasso的regression 再kaggle housr-prices 里面，rmse=$0.15386$,rank = 2903,不过做得也相当</p>
<hr>
<p>2019.02.25</p>
<p>今天好像不在状态，可能是焦虑+迷茫，有动力，动力的方向在哪里啊！，还是好好的做好当下吧！ 阿西吧</p>
<hr>
<p>2019.02.24<br>今天特别不想起床，有点小感冒，整理周志华的第一章笔记<br>下午总结日前学习的python库<br>顺便去kaggle做了小练习，数据的预处理工作。</p>
</font>]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
  <entry>
    <title>规划—-技能Post_Get</title>
    <url>/2020/10/09/%E8%A7%84%E5%88%92%E2%80%94-%E6%8A%80%E8%83%BDPost-Get/</url>
    <content><![CDATA[<h1><span id="202010">202010</span><a href="#202010" class="header-anchor">#</a></h1><h2><span id="list">List</span><a href="#list" class="header-anchor">#</a></h2><ol>
<li>origin绘图模板</li>
<li>Python绘图模型显示数值</li>
<li>ArcGIS绘图地图</li>
<li>GeoDa 分析空间数据</li>
</ol>
<p>不同数据间的汇合：键值的同一化</p>
<h2><span id="pyecharts-map">Pyecharts—Map</span><a href="#pyecharts-map" class="header-anchor">#</a></h2><p>行政边界</p>
<p><a href="https://blog.csdn.net/Jarry_cm/article/details/101464556?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight">https://blog.csdn.net/Jarry_cm/article/details/101464556?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight</a></p>
<p><a href="https://blog.csdn.net/sinat_34117508/article/details/93186175">https://blog.csdn.net/sinat_34117508/article/details/93186175</a></p>
<p><a href="https://gallery.pyecharts.org/#/Calendar/calendar_heatmap">https://gallery.pyecharts.org/#/Calendar/calendar_heatmap</a></p>
<h2><span id="zi-liao-qing-dan">资料清单</span><a href="#zi-liao-qing-dan" class="header-anchor">#</a></h2><ol>
<li>地级市的地图<ol>
<li>标注省份城市</li>
<li>地市级图像</li>
<li>省份边界</li>
</ol>
</li>
<li>城市层面的数据</li>
</ol>
<h2><span id="ji-qi-xue-xi-suan-fa">机器学习算法</span><a href="#ji-qi-xue-xi-suan-fa" class="header-anchor">#</a></h2><p>要系统整理一个（原理推导，调包上sklean)</p>
<p>用latex做一个学术报告PPT</p>
<h2><span id="origin-zhi-shi-dian">origin 知识点</span><a href="#origin-zhi-shi-dian" class="header-anchor">#</a></h2><ol>
<li><p>新版本</p>
</li>
<li><p><a href="https://www.originlab.com/doc/python/originpro/New-originpro-package-for-easy-access-to-Python">https://www.originlab.com/doc/python/originpro/New-originpro-package-for-easy-access-to-Python</a></p>
</li>
<li><a href="https://www.originlab.com/doc/python/originpro/Sample-Projects-with-attached-Python-Code">https://www.originlab.com/doc/python/originpro/Sample-Projects-with-attached-Python-Code</a></li>
</ol>
<p><a href="https://icalculate.website/2020/02/24/Origin-Plugins-Graph-Maker/">https://icalculate.website/2020/02/24/Origin-Plugins-Graph-Maker/</a></p>
<p><img src="/2020/10/09/%E8%A7%84%E5%88%92%E2%80%94-%E6%8A%80%E8%83%BDPost-Get/录制_2020_10_10_19_14_51_262.gif" alt="录制_2020_10_10_19_14_51_262"></p>
]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>规划</tag>
      </tags>
  </entry>
  <entry>
    <title>科研之材料清单</title>
    <url>/2020/10/30/%E7%A7%91%E7%A0%94%E4%B9%8B%E6%9D%90%E6%96%99%E6%B8%85%E5%8D%95/</url>
    <content><![CDATA[<h1><span id="zi-yuan">资源</span><a href="#zi-yuan" class="header-anchor">#</a></h1><h2><span id="cheng-shi-she-hui-jing-ji-she-hui">城市社会经济社会</span><a href="#cheng-shi-she-hui-jing-ji-she-hui" class="header-anchor">#</a></h2><ol>
<li>知网免费下载</li>
</ol>
<h2><span id="di-tu-shp">地图shp</span><a href="#di-tu-shp" class="header-anchor">#</a></h2>]]></content>
      <tags>
        <tag>清单</tag>
      </tags>
  </entry>
  <entry>
    <title>科研绘图之拼接和组合</title>
    <url>/2020/10/18/%E7%A7%91%E7%A0%94%E7%BB%98%E5%9B%BE%E4%B9%8B%E6%8B%BC%E6%8E%A5%E5%92%8C%E7%BB%84%E5%90%88/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="dfb64e7902ea49fdb3d66d75bed602a2afab24e3d443e1a4fcf4f522c4583a41">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化之pyecharts</title>
    <url>/2020/10/12/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Bpyecharts/</url>
    <content><![CDATA[<h1><span id="pyecharts">PyEcharts</span><a href="#pyecharts" class="header-anchor">#</a></h1><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">1. 确定图类型</span><br><span class="line">	调用类，或者函数</span><br><span class="line">	确定参数</span><br><span class="line">2. 导入数据</span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://www.bilibili.com/video/BV1E7411w7Zq?p=17">https://www.bilibili.com/video/BV1E7411w7Zq?p=17</a></p>
<p>流向图不能显示数值大小，是个大问题</p>
<h2><span id="sheng-fen">省份</span><a href="#sheng-fen" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Map  <span class="comment"># 注意这里与老版本pyecharts调用的区别</span></span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">province_distribution = {<span class="string">'河南'</span>: <span class="number">45.23</span>, <span class="string">'北京'</span>: <span class="number">37.56</span>, <span class="string">'河北'</span>: <span class="number">21</span>, <span class="string">'辽宁'</span>: <span class="number">12</span>, <span class="string">'江西'</span>: <span class="number">6</span>, <span class="string">'上海'</span>: <span class="number">20</span>, <span class="string">'安徽'</span>: <span class="number">10</span>, <span class="string">'江苏'</span>: <span class="number">16</span>, <span class="string">'湖南'</span>: <span class="number">9</span>,</span><br><span class="line">                         <span class="string">'浙江'</span>: <span class="number">13</span>, <span class="string">'海南'</span>: <span class="number">2</span>, <span class="string">'广东'</span>: <span class="number">22</span>, <span class="string">'湖北'</span>: <span class="number">8</span>, <span class="string">'黑龙江'</span>: <span class="number">11</span>, <span class="string">'澳门'</span>: <span class="number">1</span>, <span class="string">'陕西'</span>: <span class="number">11</span>, <span class="string">'四川'</span>: <span class="number">7</span>, </span><br><span class="line">                         <span class="string">'内蒙古'</span>: <span class="number">3</span>, <span class="string">'重庆'</span>: <span class="number">3</span>,<span class="string">'云南'</span>: <span class="number">6</span>, <span class="string">'贵州'</span>: <span class="number">2</span>, <span class="string">'吉林'</span>: <span class="number">3</span>, <span class="string">'山西'</span>: <span class="number">12</span>, <span class="string">'山东'</span>: <span class="number">11</span>, <span class="string">'福建'</span>: <span class="number">4</span>, <span class="string">'青海'</span>: <span class="number">1</span>, </span><br><span class="line">                         <span class="string">'天津'</span>: <span class="number">1</span>,<span class="string">'其他'</span>: <span class="number">1</span>}</span><br><span class="line">provice = list(province_distribution.keys())</span><br><span class="line">values = list(province_distribution.values())</span><br><span class="line">province = [<span class="string">'广东'</span>, <span class="string">'湖北'</span>, <span class="string">'湖南'</span>, <span class="string">'四川'</span>, <span class="string">'重庆'</span>, <span class="string">'黑龙江'</span>, <span class="string">'浙江'</span>, <span class="string">'山西'</span>]</span><br><span class="line">data_province = [(i, random.randint(<span class="number">100</span>, <span class="number">200</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> province]</span><br><span class="line"></span><br><span class="line">china_province = (</span><br><span class="line">    Map()</span><br><span class="line">    .add(<span class="string">''</span>, data_province, <span class="string">'china'</span>)</span><br><span class="line">    .set_global_opts(</span><br><span class="line">        title_opts=opts.TitleOpts(title=<span class="string">'Provinces of China'</span>),</span><br><span class="line">        visualmap_opts=opts.VisualMapOpts(</span><br><span class="line">            min_=<span class="number">100</span>,</span><br><span class="line">            max_=<span class="number">200</span>,</span><br><span class="line">            is_piecewise=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    .render(path=<span class="string">'中国省级地图.html'</span>)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="cheng-shi">城市</span><a href="#cheng-shi" class="header-anchor">#</a></h2><h2><span id="map">Map</span><a href="#map" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Map</span><br><span class="line"><span class="keyword">from</span> pyecharts.faker <span class="keyword">import</span> Faker</span><br><span class="line"></span><br><span class="line"><span class="comment"># map 函数 地图主要用于地理区域数据的可视化。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># map.add(name, attr, value, maptype='china', is_roam=True, is_map_symbol_show=True, **kwargs)</span></span><br><span class="line"></span><br><span class="line">c = (</span><br><span class="line">    Map()</span><br><span class="line">    .add(</span><br><span class="line">        <span class="string">"商家A"</span>,</span><br><span class="line">        [list(z) <span class="keyword">for</span> z <span class="keyword">in</span> zip(Faker.guangdong_city, Faker.values())],</span><br><span class="line">        value,maptype = <span class="string">"china-cities"</span>,</span><br><span class="line">        label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>),</span><br><span class="line">        is_label_show=<span class="literal">True</span> , //显示文本吗</span><br><span class="line">    )</span><br><span class="line">    .set_global_opts(</span><br><span class="line">        title_opts=opts.TitleOpts(title=<span class="string">"Map-中国地图（带城市）"</span>),</span><br><span class="line">        visualmap_opts=opts.VisualMapOpts(),</span><br><span class="line">        is_piecewise=<span class="literal">True</span>,//是否分段</span><br><span class="line">    )</span><br><span class="line">    .render(<span class="string">"map_china_cities.html"</span>)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="geo">Geo</span><a href="#geo" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Geo</span><br><span class="line"><span class="keyword">from</span> pyecharts .globals <span class="keyword">import</span> ChartType, SymbolType, GeoType</span><br><span class="line"><span class="keyword">from</span> snapshot_selenium <span class="keyword">import</span> snapshot <span class="keyword">as</span> driver</span><br><span class="line"><span class="keyword">from</span> pyecharts.render <span class="keyword">import</span> make_snapshot</span><br><span class="line">geo = Geo()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 新增坐标点，添加名称跟经纬度</span></span><br><span class="line">geo.add_coordinate(name=<span class="string">"China"</span>,longitude=<span class="number">104.195</span>,latitude=<span class="number">35.675</span>)</span><br><span class="line">geo.add_coordinate(name=<span class="string">"Australia"</span>,longitude=<span class="number">100.195</span>,latitude=<span class="number">35.675</span>)</span><br><span class="line"><span class="comment"># 添加数据项</span></span><br><span class="line">geo.add_schema(maptype=<span class="string">"china"</span>)</span><br><span class="line">geo.add(<span class="string">""</span>,[(<span class="string">"Australia"</span>,<span class="number">128326</span>),</span><br><span class="line">(<span class="string">"China"</span>,<span class="number">109</span>),</span><br><span class="line">],type_=ChartType.EFFECT_SCATTER)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制流向</span></span><br><span class="line">geo.add(<span class="string">"流向图"</span>,[</span><br><span class="line">(<span class="string">"Australia"</span>,<span class="string">"China"</span>),</span><br><span class="line">],</span><br><span class="line">type_= GeoType.LINES,</span><br><span class="line">effect_opts=opts.EffectOpts(symbol=SymbolType.ARROW,symbol_size=<span class="number">5</span>,color=<span class="string">"yellow"</span>),</span><br><span class="line">linestyle_opts=opts.LineStyleOpts(curve=<span class="number">0.2</span>),</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">geo.set_series_opts(label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>))</span><br><span class="line">geo.set_global_opts(visualmap_opts=opts.VisualMapOpts(max_=<span class="number">130000</span>),title_opts=opts.TitleOpts(title=<span class="string">"mygeo"</span>))</span><br><span class="line">geo.render(<span class="string">'test.html'</span>)</span><br><span class="line">geo.render(<span class="string">'test.png'</span>)</span><br><span class="line"><span class="comment">#make_snapshot(driver,geo.render(), "bar.png")</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Geo</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># geo.add(name, attr, value, type="scatter", maptype='china', coordinate_region='中国', symbol_size=12, border_color="#111", geo_normal_color="#323c48", geo_emphasis_color="#2a333d", geo_cities_coords=None, is_roam=True, **kwargs)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用参数说明：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数	接收值	说明</span></span><br><span class="line"><span class="comment"># name	str	图例名称</span></span><br><span class="line"><span class="comment"># attr	list	属性名称</span></span><br><span class="line"><span class="comment"># value	list	属性所对应的值</span></span><br><span class="line"><span class="comment"># type	str	图例类型，有'scatter','effectScatter','heatmap'可选。默认为'scatter'</span></span><br><span class="line"><span class="comment"># maptype	str	地图类型</span></span><br><span class="line"><span class="comment"># coordinate_region	str	城市坐标所属国家</span></span><br><span class="line"><span class="comment"># symbol_size	int	标记图形大小。默认为12</span></span><br><span class="line"><span class="comment"># border_color	str	地图边界颜色。默认为'#111'</span></span><br><span class="line"><span class="comment"># geo_normal_color	str	正常状态下地图区域的颜色。默认为'#323c48'</span></span><br><span class="line"><span class="comment"># geo_emphasis_color	str	高亮状态下地图区域的颜色。默认为'#2a333d'</span></span><br><span class="line"><span class="comment"># geo_cities_coords	dict	用户自定义地区经纬度，类似如{'阿城':[126.58,45.32],}这样的字典。</span></span><br><span class="line"><span class="comment"># is_roam	bool	是否开启鼠标缩放和平移漫游。'scale'缩放、'move'平移、'True'都开启；默认为True。</span></span><br><span class="line"><span class="comment"># Geo 函数的使用：</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">datafile = <span class="string">u'D:\\pythondata\\travel\\travel_data.xlsx'</span></span><br><span class="line">data = pd.read_excel(datafile)</span><br><span class="line">attr = data[<span class="string">'City'</span>]</span><br><span class="line">value = data[<span class="string">'Score'</span>]</span><br><span class="line"> </span><br><span class="line">geo = Geo(<span class="string">"♡♡♡  往后余生，踏遍山河  ♡♡♡"</span>, title_color=<span class="string">"#2E2E2E"</span>,</span><br><span class="line">          title_text_size=<span class="number">24</span>,title_top=<span class="number">20</span>,title_pos=<span class="string">"center"</span>, width=<span class="number">1300</span>,height=<span class="number">600</span>, </span><br><span class="line">          background_color=<span class="string">'#F6CEF5'</span>)</span><br><span class="line">geo.add(<span class="string">""</span>, attr, value, type=<span class="string">"effectScatter"</span>, is_random=<span class="literal">True</span>, visual_range=[<span class="number">0</span>, <span class="number">100</span>], </span><br><span class="line">        maptype=<span class="string">'china'</span>,visual_text_color=<span class="string">"#FF0000"</span>, geo_normal_color=<span class="string">"#6E6E6E"</span>,geo_emphasis_color=<span class="string">'#F5D0A9'</span>,</span><br><span class="line">        symbol_size=<span class="number">8</span>, effect_scale=<span class="number">5</span>, is_visualmap=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Geo 图类型，有 scatter, effectScatter, heatmap, lines 4 种，建议使用 </span></span><br><span class="line">        <span class="comment"># GeoType.GeoType.EFFECT_SCATTER，GeoType.HEATMAP，GeoType.LINES</span></span><br><span class="line">geo.render(path=<span class="string">u'D:\\pythondata\\travel\\往后余生.html'</span>)<span class="comment">#生成html文件</span></span><br></pre></td></tr></tbody></table></figure>
]]></content>
  </entry>
  <entry>
    <title>科研之材料收集</title>
    <url>/2020/10/12/%E7%A7%91%E7%A0%94%E4%B9%8B%E6%9D%90%E6%96%99%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="6d68026e797ef6d64da02da4be82e8e478e2865154ff86de9f8c276404d3b5d1">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>科研材料</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化之Geopandas</title>
    <url>/2020/10/12/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BGeopandas/</url>
    <content><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/141729108">https://zhuanlan.zhihu.com/p/141729108</a></p>
]]></content>
  </entry>
  <entry>
    <title>可视化之Matplotlib</title>
    <url>/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BMatplotlib/</url>
    <content><![CDATA[<p>此篇博客针对matplotlib库进行全面的讲解，以原代码展示为主，辅以动态图片。 matplotlib库绘制图的大致可分为五大步骤：</p>
<ol>
<li><p>导入模块（from matplotlib.pyplot as plt)</p>
</li>
<li><p>创建画布 (fig = plt.figure()) </p>
</li>
<li>创建坐标系(ax = fig.add_axes())</li>
<li>绘制图片(ax.类型)</li>
<li>美化和规范性(ax.set_(参数))</li>
</ol>
<a id="more"></a>
<h2><span id="matplotlib">Matplotlib</span><a href="#matplotlib" class="header-anchor">#</a></h2><p><img src="/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BMatplotlib/Users\ADMIN\Documents\Apowersoft\ApowerREC\20201011_195152.gif" alt="20201011_195152"></p>
<h2><span id="chuang-jian-hua-bu">创建画布</span><a href="#chuang-jian-hua-bu" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="comment"># 导入matplotlib的pyplot模块</span></span><br><span class="line"><span class="comment"># 创建类的实例: plt.pie plt.bar etc</span></span><br><span class="line"><span class="comment"># figure(num=None, figsize=None(inches,width,height), dpi=None（分辨率:单位像素点), facecolor=None, edgecolor=None, frameon=True)</span></span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html#module-matplotlib.pyplot">pyplot模块里面的方法和类</a></p>
<p>可以调用函数获取figur(gcf())和Axes(gca())；</p>
<p>创建类实例，绘制图</p>
<p>厘米和英寸的换算关系：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">1 inchs = 2.54 cm</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="chuang-jian-zuo-biao-xi">创建坐标系</span><a href="#chuang-jian-zuo-biao-xi" class="header-anchor">#</a></h2><h3><span id="fig-add-axes-wei-zhi-zi-ding-yi">fig.add_axes() 位置自定义</span><a href="#fig-add-axes-wei-zhi-zi-ding-yi" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fig = plt.figure()  <span class="comment"># 新建图形对象</span></span><br><span class="line">axes = fig.add_axes([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">0.8</span>])  <span class="comment"># 控制画布的左，下，宽度，高度（是长宽的百分比)</span></span><br><span class="line">axes.plot(x, y, <span class="string">'r'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">fig = plt.figure()  # 新建画板</span><br><span class="line">axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])  # 大画布</span><br><span class="line">axes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3])  # 小画布</span><br><span class="line"></span><br><span class="line">axes1.plot(x, y, 'r')  # 大画布</span><br><span class="line">axes2.plot(y, x, 'g')  # 小画布</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="fig-add-suplots-zi-tu">fig.add_suplots()子图</span><a href="#fig-add-suplots-zi-tu" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fig, axes = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>)  <span class="comment"># 子图为 1 行，2 列</span></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axes:</span><br><span class="line">    ax.plot(x, y, <span class="string">'r'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>fig.add_suplot()每一个子图</p>
<p>顺序很重要</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 向画布添加子图 1</span></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line"><span class="comment"># 向画布添加子图 2</span></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, projection=<span class="string">'3d'</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="xi-jie-zhi-ding">细节制定</span><a href="#xi-jie-zhi-ding" class="header-anchor">#</a></h3><p>通过创建坐标系，返回坐标系对象(Axes)Axes是类。然后利用axes类，对图片细节就像美化。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">matplotlib</span>.<span class="title">axes</span>.<span class="title">Axes</span>(<span class="params">fig, rect, facecolor=None, frameon=True, sharex=None, sharey=None, label=<span class="string">''</span>, xscale=None, yscale=None, box_aspect=None, **kwargs</span>)</span></span><br></pre></td></tr></tbody></table></figure>
<h2><span id="plottingp"></span><a href="#plottingp" class="header-anchor">#</a></h2><h3><span id="basic"></span><a href="#basic" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html#matplotlib.axes.Axes.plot"><code>Axes.plot</code></a></th>
<th>Plot y versus x as lines and/or markers.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.errorbar.html#matplotlib.axes.Axes.errorbar"><code>Axes.errorbar</code></a></td>
<td>Plot y versus x as lines and/or markers with attached errorbars.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.scatter.html#matplotlib.axes.Axes.scatter"><code>Axes.scatter</code></a></td>
<td>A scatter plot of <em>y</em> vs.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot_date.html#matplotlib.axes.Axes.plot_date"><code>Axes.plot_date</code></a></td>
<td>Plot data that contains dates.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.step.html#matplotlib.axes.Axes.step"><code>Axes.step</code></a></td>
<td>Make a step plot.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.loglog.html#matplotlib.axes.Axes.loglog"><code>Axes.loglog</code></a></td>
<td>Make a plot with log scaling on both the x and y axis.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.semilogx.html#matplotlib.axes.Axes.semilogx"><code>Axes.semilogx</code></a></td>
<td>Make a plot with log scaling on the x axis.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.semilogy.html#matplotlib.axes.Axes.semilogy"><code>Axes.semilogy</code></a></td>
<td>Make a plot with log scaling on the y axis.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.fill_between.html#matplotlib.axes.Axes.fill_between"><code>Axes.fill_between</code></a></td>
<td>Fill the area between two horizontal curves.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.fill_betweenx.html#matplotlib.axes.Axes.fill_betweenx"><code>Axes.fill_betweenx</code></a></td>
<td>Fill the area between two vertical curves.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.bar.html#matplotlib.axes.Axes.bar"><code>Axes.bar</code></a></td>
<td>Make a bar plot.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.barh.html#matplotlib.axes.Axes.barh"><code>Axes.barh</code></a></td>
<td>Make a horizontal bar plot.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.stem.html#matplotlib.axes.Axes.stem"><code>Axes.stem</code></a></td>
<td>Create a stem plot.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.eventplot.html#matplotlib.axes.Axes.eventplot"><code>Axes.eventplot</code></a></td>
<td>Plot identical parallel lines at the given positions.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.pie.html#matplotlib.axes.Axes.pie"><code>Axes.pie</code></a></td>
<td>Plot a pie chart.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.stackplot.html#matplotlib.axes.Axes.stackplot"><code>Axes.stackplot</code></a></td>
<td>Draw a stacked area plot.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.broken_barh.html#matplotlib.axes.Axes.broken_barh"><code>Axes.broken_barh</code></a></td>
<td>Plot a horizontal sequence of rectangles.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.vlines.html#matplotlib.axes.Axes.vlines"><code>Axes.vlines</code></a></td>
<td>Plot vertical lines.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.hlines.html#matplotlib.axes.Axes.hlines"><code>Axes.hlines</code></a></td>
<td>Plot horizontal lines at each <em>y</em> from <em>xmin</em> to <em>xmax</em>.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.fill.html#matplotlib.axes.Axes.fill"><code>Axes.fill</code></a></td>
<td>Plot filled polygons.</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="appearance"></span><a href="#appearance" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.axis.html#matplotlib.axes.Axes.axis"><code>Axes.axis</code></a></th>
<th>Convenience method to get or set some axis properties.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_axis_off.html#matplotlib.axes.Axes.set_axis_off"><code>Axes.set_axis_off</code></a></td>
<td>Turn the x- and y-axis off.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_axis_on.html#matplotlib.axes.Axes.set_axis_on"><code>Axes.set_axis_on</code></a></td>
<td>Turn the x- and y-axis on.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_frame_on.html#matplotlib.axes.Axes.set_frame_on"><code>Axes.set_frame_on</code></a></td>
<td>Set whether the axes rectangle patch is drawn.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.get_frame_on.html#matplotlib.axes.Axes.get_frame_on"><code>Axes.get_frame_on</code></a></td>
<td>Get whether the axes rectangle patch is drawn.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_axisbelow.html#matplotlib.axes.Axes.set_axisbelow"><code>Axes.set_axisbelow</code></a></td>
<td>Set whether axis ticks and gridlines are above or below most artists.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.get_axisbelow.html#matplotlib.axes.Axes.get_axisbelow"><code>Axes.get_axisbelow</code></a></td>
<td>Get whether axis ticks and gridlines are above or below most artists.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.grid.html#matplotlib.axes.Axes.grid"><code>Axes.grid</code></a></td>
<td>Configure the grid lines.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.get_facecolor.html#matplotlib.axes.Axes.get_facecolor"><code>Axes.get_facecolor</code></a></td>
<td>Get the facecolor of the Axes.</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_facecolor.html#matplotlib.axes.Axes.set_facecolor"><code>Axes.set_facecolor</code></a></td>
<td>Set the facecolor of the Axes.</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes">参考这里</a></p>
<h2><span id="ke-du">刻度</span><a href="#ke-du" class="header-anchor">#</a></h2><p>from pylab import *<br>from matplotlib.ticker import MultipleLocator, FormatStrFormatter</p>
<p>xmajorLocator   = MultipleLocator(20) #将x主刻度标签设置为20的倍数<br>xmajorFormatter = FormatStrFormatter(‘%1.1f’) #设置x轴标签文本的格式<br>xminorLocator   = MultipleLocator(5) #将x轴次刻度标签设置为5的倍数</p>
<p>ymajorLocator   = MultipleLocator(0.5) #将y轴主刻度标签设置为0.5的倍数<br>ymajorFormatter = FormatStrFormatter(‘%1.1f’) #设置y轴标签文本的格式<br>yminorLocator   = MultipleLocator(0.1) #将此y轴次刻度标签设置为0.1的倍数</p>
<p>t = arange(0.0, 100.0, 1)<br>s = sin(0.1<em>pi</em>t)<em>exp(-t</em>0.01)</p>
<p>ax = subplot(111) #注意:一般都在ax中设置,不再plot中设置<br>plot(t,s,’—b*’)</p>
<h1><span id="she-zhi-zhu-ke-du-biao-qian-de-wei-zhi-biao-qian-wen-ben-de-ge-shi">设置主刻度标签的位置,标签文本的格式</span><a href="#she-zhi-zhu-ke-du-biao-qian-de-wei-zhi-biao-qian-wen-ben-de-ge-shi" class="header-anchor">#</a></h1><p>ax.xaxis.set_major_locator(xmajorLocator)<br>ax.xaxis.set_major_formatter(xmajorFormatter)</p>
<p>ax.yaxis.set_major_locator(ymajorLocator)<br>ax.yaxis.set_major_formatter(ymajorFormatter)</p>
<h1><span id="xian-shi-ci-ke-du-biao-qian-de-wei-zhi-mei-you-biao-qian-wen-ben">显示次刻度标签的位置,没有标签文本</span><a href="#xian-shi-ci-ke-du-biao-qian-de-wei-zhi-mei-you-biao-qian-wen-ben" class="header-anchor">#</a></h1><p>ax.xaxis.set_minor_locator(xminorLocator)<br>ax.yaxis.set_minor_locator(yminorLocator)</p>
<p>ax.xaxis.grid(True, which=’major’) #x坐标轴的网格使用主刻度<br>ax.yaxis.grid(True, which=’minor’) #y坐标轴的网格使用次刻度</p>
<p>show()</p>
<h2><span id="hui-tu">绘图</span><a href="#hui-tu" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>  窗口：figure: 一个窗口，plt.figure(num=,figsize=(h,w))下面数据都属于当前的figure,有一定的顺序喔</span><br><span class="line"><span class="number">2.</span>  画图：plt.plot(x,y,color=,linewidth=,linestyle,label=)</span><br><span class="line"><span class="number">3.</span>  标注信息： plt.xlim((,)), plt.yxlim((,)),plt.xlabel(),plt.ylabel(),ticks:图像的小标，plt.xticks(),plt.yticks([值<span class="number">1</span>，值<span class="number">2</span>],[<span class="string">r'$值1\ 对应的文字$'</span>,<span class="string">r'值2的文字 \alpha])</span></span><br><span class="line"><span class="string">4.  坐标轴：axis gac='</span>get current axis<span class="string">'</span></span><br><span class="line"><span class="string">    ax = plt.gca() # 轴</span></span><br><span class="line"><span class="string">    \# 获取四个轴</span></span><br><span class="line"><span class="string">    ax.spines['</span>right|left|top|<span class="string">'].set_color('</span>none<span class="string">') </span></span><br><span class="line"><span class="string">    ax.xaxis.set_ticks_position('</span>bottom<span class="string">')</span></span><br><span class="line"><span class="string">    ax.spines['</span>bottom<span class="string">'].set_position(('</span>data<span class="string">',-1))</span></span><br><span class="line"><span class="string">5.  图例：legend: </span></span><br><span class="line"><span class="string">    a. plt.plot(,label=), plt.legend()</span></span><br><span class="line"><span class="string">    b. l1, = plt.plot() plt.legend(handles=[l1,],labels=[,],loc='</span>best|upper right|<span class="string">')</span></span><br><span class="line"><span class="string">6.  注解 annotation </span></span><br><span class="line"><span class="string">    a. 点的位置(x0，y0) plt.scatter(). plt.plot([x0,y0],[y0,0],'</span>k--<span class="string">',lw=)</span></span><br><span class="line"><span class="string">    b . method 1:</span></span><br><span class="line">    plt.annotate(r'name',xy=(,)起始点，xycoords='data'//基于xy,xytext=(+30,30),textcoords='offseet points'//文本基于xy,arrowprops=dict(arrowstyle='-&gt;'箭头,connectionstyle='arc3,rad=.2')弧度)</span><br><span class="line"><span class="number">7.</span>  Bar 柱状图</span><br><span class="line">    plt.bar(x,+|-y,facecolor=<span class="string">""</span>,edgecolor,)</span><br><span class="line">    |<span class="comment"># ha horizontal alignment 对齐方式</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,y):</span><br><span class="line">     plt.text(x+<span class="number">0</span>,<span class="number">4</span>,y+<span class="number">0.05</span>,<span class="string">'%.2f'</span>%y,ha=<span class="string">'center'</span>,va=<span class="string">'bottom'</span>)</span><br><span class="line"><span class="number">8.</span>  很多自动 subplot(总行，当前行的列，总的按最小分的第几个)</span><br><span class="line">    subplot(,,)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>绘图</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化之Python高级进阶篇</title>
    <url>/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BPython%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E7%AF%87/</url>
    <content><![CDATA[<h1><span id="figure-vs-subplot-vs-axes">figure vs.  subplot vs. axes</span><a href="#figure-vs-subplot-vs-axes" class="header-anchor">#</a></h1><h2><span id="figure-tu-xiang">figure 图像</span><a href="#figure-tu-xiang" class="header-anchor">#</a></h2><p>figure是图像的载体,使用pyplot.figure()创建, 一个程序可以创建多个画布, 画图操作作用于最近创建的画布上, 多个画布顺序显示.</p>
<h2><span id="axes-subplot-zuo-biao-xi">axes(subplot) 坐标系</span><a href="#axes-subplot-zuo-biao-xi" class="header-anchor">#</a></h2><p>坐标系,可理解为画布上的一个区域, 一个figure上可以有多个axes，被分成多个区域， 可用pyplot.axes和pyplot.subplots/pyplot.subplot来创建. pyplot画图操作本质就是操作axes对象。</p>
<p><a href="https://matplotlib.org/api/axes_api.html">相关属性(调用函数)</a></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">fig, axess = plt.subplots(2,2)</span><br><span class="line">ax1 = axess[0,0]</span><br><span class="line">ax2 = axess[0,1]</span><br><span class="line">ax3 = axess[1,0]</span><br><span class="line">ax4 = axess[1,1]</span><br><span class="line">ax2.plot(np.arange(4),np.arange(4))</span><br><span class="line">plt.scatter(np.arange(4), np.arange(4))</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">plt.subplot(2,2,1) </span><br><span class="line">plt.subplot(2,2,2) </span><br><span class="line">plt.plot(np.arange(4),np.arange(4))</span><br><span class="line">plt.subplot(2,2,3)</span><br><span class="line">plt.subplot(2,2,4)</span><br><span class="line">plt.scatter(np.arange(4), np.arange(4))</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(211)</span><br><span class="line"> </span><br><span class="line">ax2 = fig.add_subplot(212)</span><br><span class="line"> </span><br><span class="line">print(type(ax1))</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax3 = fig.add_axes([0.1, 0.1, 0.8, 0.8])</span><br><span class="line">#0.1,0.1为距离画布左下角figure画布的距离</span><br><span class="line">ax4 = fig.add_axes([0.72, 0.72, 0.16, 0.13])</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="she-zhi-zu-jian-de-shu-xing">设置组件的属性</span><a href="#she-zhi-zu-jian-de-shu-xing" class="header-anchor">#</a></h3><p>规律可行</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">axes.set_xlim() .set_ylim() .set.xticks() .set_ylabel() set_xlabel()</span><br><span class="line">xx_kws = {} color, lw = '' </span><br><span class="line">line_kws</span><br><span class="line">scatter_kws</span><br><span class="line">hist_kws</span><br><span class="line">kde_kws</span><br><span class="line">命名很规律</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="axis-zuo-biao-zhou">axis 坐标轴</span><a href="#axis-zuo-biao-zhou" class="header-anchor">#</a></h2><p><img src="/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BPython%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E7%AF%87/20200311202147484.png" alt="在这里插入图片描述"></p>
<p><img src="/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BPython%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E7%AF%87/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxMzQ3ODY5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;"></p>
<h1><span id="han-shu-diao-yong-guan-xi">函数调用关系</span><a href="#han-shu-diao-yong-guan-xi" class="header-anchor">#</a></h1><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.gcf().subplots(2,2)\</span><br><span class="line">其中 .gcf() 的作用是获取当前 figure，即 get current figure。另外对应的 .gca() 就是获取当前 axes，即 get current axes</span><br></pre></td></tr></tbody></table></figure>
<h1><span id="hui-tu-yuan-li">绘图原理</span><a href="#hui-tu-yuan-li" class="header-anchor">#</a></h1><p>创建画布(figure)—&gt;添加axes-&gt;修改属性(调用函数绘图)-&gt;爆操</p>
<h2><span id="shu-xing-she-zhi">属性设置</span><a href="#shu-xing-she-zhi" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">g.map_lower(sns.regplot, lowess=False, ci=None, line_kws={'color': 'black','lw':0.3},scatter_kws={ 'alpha': 0.3,'color':"b",'sizes':(15,),</span><br><span class="line">   'linewidths':0.3,'edgecolor':'r','facecolor':'red'})</span><br><span class="line">   g.map_diag(sns.distplot, kde_kws={'color': 'black','lw':0.3},hist_kws = {"color": "g",'lw':0.3})</span><br></pre></td></tr></tbody></table></figure>
]]></content>
  </entry>
  <entry>
    <title>可视化之Seaborn</title>
    <url>/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BSeaborn/</url>
    <content><![CDATA[<p>[导读]此篇Blog将简单呈现从入门到精通再到升仙的学习路径。本着通俗易懂的原则，面向新手的打怪升级套路，重量级的推出不仅仅适用于工程、竞赛的绘图标准，更是满足科研绘图的高水准的教程。(先定制出目标，能不能到达效果就看下文了)</p>
<ol>
<li>导入模块</li>
<li>设置风格</li>
<li>设置环境</li>
<li>选择想要绘制的图形</li>
<li>规范化(调色板等等)</li>
</ol>
<a id="more"></a>
<h1><span id="wu-jia-you-nu-chu-chang-cheng-seaborn">吾家有女初长成— seaborn</span><a href="#wu-jia-you-nu-chu-chang-cheng-seaborn" class="header-anchor">#</a></h1><p>seaborn是基于matplotlib库进行封装和集成的一个Python的可视化库。</p>
<p><a href="http://seaborn.pydata.org/api.html#">官方文档</a></p>
<h3><span id="step1-feng-ge-she-zhi">Step1 风格设置</span><a href="#step1-feng-ge-she-zhi" class="header-anchor">#</a></h3><h3><span id="method-1-set-tong-yong-she-zhi">Method 1: set() 通用设置</span><a href="#method-1-set-tong-yong-she-zhi" class="header-anchor">#</a></h3><p>通过set()函数可进行风格(style)，和环境（context）设置</p>
<p><a href="http://seaborn.pydata.org/generated/seaborn.set_theme.html#seaborn.set_theme">详情</a></p>
<p><a href="https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py">详情</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">sns.set()  <span class="comment">#默认风格为darkgrid</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_theme</span>(<span class="params">context=<span class="string">"notebook"</span>, style=<span class="string">"darkgrid"</span>, palette=<span class="string">"deep"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              font=<span class="string">"sans-serif"</span>, font_scale=<span class="number">1</span>, color_codes=True, rc=None</span>):</span></span><br><span class="line">    <span class="string">"""Set multiple theme parameters in one step.</span></span><br><span class="line"><span class="string">    Each set of parameters can be set directly or temporarily, see the</span></span><br><span class="line"><span class="string">    referenced functions below for more information.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    context : string or dict</span></span><br><span class="line"><span class="string">        Plotting context parameters, see :func:`plotting_context`.</span></span><br><span class="line"><span class="string">    style : string or dict</span></span><br><span class="line"><span class="string">        Axes style parameters, see :func:`axes_style`.</span></span><br><span class="line"><span class="string">    palette : string or sequence</span></span><br><span class="line"><span class="string">        Color palette, see :func:`color_palette`.</span></span><br><span class="line"><span class="string">    font : string</span></span><br><span class="line"><span class="string">        Font family, see matplotlib font manager.</span></span><br><span class="line"><span class="string">    font_scale : float, optional</span></span><br><span class="line"><span class="string">        Separate scaling factor to independently scale the size of the</span></span><br><span class="line"><span class="string">        font elements.</span></span><br><span class="line"><span class="string">    color_codes : bool</span></span><br><span class="line"><span class="string">        If ``True`` and ``palette`` is a seaborn palette, remap the shorthand</span></span><br><span class="line"><span class="string">        color codes (e.g. "b", "g", "r", etc.) to the colors from this palette.</span></span><br><span class="line"><span class="string">    rc : dict or None</span></span><br><span class="line"><span class="string">        Dictionary of rc parameter mappings to override the above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    set_context(context, font_scale)</span><br><span class="line">    set_style(style, rc={<span class="string">"font.family"</span>: font})</span><br><span class="line">    set_palette(palette, color_codes=color_codes)</span><br><span class="line">    <span class="keyword">if</span> rc <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mpl.rcParams.update(rc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="string">"""Alias for :func:`set_theme`, which is the preferred interface."""</span></span><br><span class="line">    set_theme(*args, **kwargs)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="set-style-feng-ge-she-zhi">set_style()风格设置</span><a href="#set-style-feng-ge-she-zhi" class="header-anchor">#</a></h3><h4><span id="quan-ju-feng-ge">全局风格</span><a href="#quan-ju-feng-ge" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">2.set_style(style=None, rc=None)，自定义整体风格</span><br><span class="line">style: 参数为"white"、"dark"、 "whitegrid"、 "darkgrid"、 "ticks"或者None，默认为darkgrid</span><br><span class="line">rc: rcdict, optional</span><br><span class="line">Parameter mappings to override the values in the preset seaborn style dictionaries. This only updates parameters that are considered part of the style definition.</span><br><span class="line"></span><br><span class="line">set_style("whitegrid")</span><br><span class="line">set_style("ticks", {"xtick.major.size": 8, "ytick.major.size": 8})</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><a href="http://seaborn.pydata.org/generated/seaborn.set_style.html?highlight=set_style#seaborn.set_style">详情</a></p>
<h4><span id="dang-qian-tu-axes-de-feng-ge">当前图(axes)的风格</span><a href="#dang-qian-tu-axes-de-feng-ge" class="header-anchor">#</a></h4><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">3.</span>axes_style()，设置子图风格</span><br><span class="line">可与<span class="keyword">with</span>搭配使用，设置<span class="keyword">with</span>代码块内的图表风格，不影响整体图表风格。</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">15</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">"dark"</span>,rc = ): <span class="comment">#只对with代码块内的图表风格生效，即只对第一个子图生效</span></span><br><span class="line">    plt.subplot(<span class="number">121</span>)       </span><br><span class="line">    sinplot()</span><br><span class="line"></span><br><span class="line">sns.set_style(<span class="string">"white"</span>)       <span class="comment">#整体风格为white</span></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="rc">rc</span><a href="#rc" class="header-anchor">#</a></h4><p>_style_keys = [</p>
<pre><code>"axes.facecolor",
"axes.edgecolor",
"axes.grid",
"axes.axisbelow",
"axes.labelcolor",

"figure.facecolor",

"grid.color",
"grid.linestyle",

"text.color",

"xtick.color",
"ytick.color",
"xtick.direction",
"ytick.direction",
"lines.solid_capstyle",

"patch.edgecolor",
"patch.force_edgecolor",

"image.cmap",
"font.family",
"font.sans-serif",

"xtick.bottom",
"xtick.top",
"ytick.left",
"ytick.right",

"axes.spines.left",
"axes.spines.bottom",
"axes.spines.right",
"axes.spines.top",
</code></pre><h3><span id="step2-huan-jing-she-zhi-context">Step2 环境设置context</span><a href="#step2-huan-jing-she-zhi-context" class="header-anchor">#</a></h3><h5><span id="5-set-context-xian-shi-bi-li">5.set_context()显示比例</span><a href="#5-set-context-xian-shi-bi-li" class="header-anchor">#</a></h5><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">sns.set() //重设默认参数</span><br><span class="line">sns.set_context(<span class="string">"paper"</span>,rc)</span><br><span class="line">sinplot()</span><br><span class="line">可选参数为<span class="string">'paper'</span>、 <span class="string">'notebook'</span>、<span class="string">'talk'</span>、<span class="string">'poster'</span>，默认为notebook，设置标签、线等的大小。</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="ju-bu-axes">局部axes</span><a href="#ju-bu-axes" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"> // 自定义图形规模</span><br><span class="line">sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})</span><br><span class="line">sinplot()</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="rc">rc</span><a href="#rc" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// 可设置的选择项 rc</span><br><span class="line">_context_keys = [</span><br><span class="line"></span><br><span class="line">    "font.size",</span><br><span class="line">    "axes.labelsize",</span><br><span class="line">    "axes.titlesize",</span><br><span class="line">    "xtick.labelsize",</span><br><span class="line">    "ytick.labelsize",</span><br><span class="line">    "legend.fontsize",</span><br><span class="line"></span><br><span class="line">    "axes.linewidth",</span><br><span class="line">    "grid.linewidth",</span><br><span class="line">    "lines.linewidth",</span><br><span class="line">    "lines.markersize",</span><br><span class="line">    "patch.linewidth",</span><br><span class="line"></span><br><span class="line">    "xtick.major.width",</span><br><span class="line">    "ytick.major.width",</span><br><span class="line">    "xtick.minor.width",</span><br><span class="line">    "ytick.minor.width",</span><br><span class="line"></span><br><span class="line">    "xtick.major.size",</span><br><span class="line">    "ytick.major.size",</span><br><span class="line">    "xtick.minor.size",</span><br><span class="line">    "ytick.minor.size",</span><br><span class="line"></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="step-3-diao-se-ban">Step 3 调色板</span><a href="#step-3-diao-se-ban" class="header-anchor">#</a></h3><p>就是对图片的颜色进行配置。选择指定的颜色风格。通过color_palette()对颜色板进行处理，返回颜色，并对图片进行上色。</p>
<h4><span id="diao-se-ban">调色板</span><a href="#diao-se-ban" class="header-anchor">#</a></h4><p>主要使用以下几个函数设置颜色：<br>color_palette() 能传入任何Matplotlib所有支持的颜色<br>color_palette() 不写参数则默认颜色<br>set_palette() 设置所有图的颜色</p>
<p>主要使用以下几个函数设置颜色：</p>
<h5><span id="color-palette-ji-yu-rgb-she-zhi-yan-se-de-jie-kou">color_palette() 基于RGB设置颜色的接口</span><a href="#color-palette-ji-yu-rgb-she-zhi-yan-se-de-jie-kou" class="header-anchor">#</a></h5><p> 能传入任何Matplotlib所有支持的颜色。基于RGB原理设置颜色的接口，可接收一个调色板对象作为参数，同时可以设置颜色数量</p>
<p>Returns:<br>palette : list of RGB tuples. 返回一个颜色三元组列表（默认是6个）</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">seaborn.color_palette(palette=<span class="literal">None</span>, n_colors=<span class="literal">None</span>, desat=<span class="literal">None</span>, as_cmap=<span class="literal">False</span>)</span><br><span class="line">Return a list of colors <span class="keyword">or</span> continuous colormap defining a palette.</span><br><span class="line"></span><br><span class="line">Possible palette values include:</span><br><span class="line">Name of a seaborn palette (deep, muted, bright, pastel, dark, colorblind)</span><br><span class="line"></span><br><span class="line">Name of matplotlib colormap</span><br><span class="line"></span><br><span class="line">‘husl’ <span class="keyword">or</span> ‘hsl’</span><br><span class="line"></span><br><span class="line">‘ch:&lt;cubehelix arguments&gt;’</span><br><span class="line"></span><br><span class="line">‘light:&lt;color&gt;’, ‘dark:&lt;color&gt;’, ‘blend:&lt;color&gt;,&lt;color&gt;’,</span><br><span class="line"></span><br><span class="line">A sequence of colors <span class="keyword">in</span> any format matplotlib accepts</span><br></pre></td></tr></tbody></table></figure>
<h5><span id="chang-jian-de-yan-se-ban">常见的颜色板</span><a href="#chang-jian-de-yan-se-ban" class="header-anchor">#</a></h5><div class="table-container">
<table>
<thead>
<tr>
<th>颜色组合名称(首字母大写)</th>
<th>调色显示</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.Accent</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705221558317.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>2.Accent_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705221700433.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>3.Blues</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705221906969.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>4.Blues_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705221930868.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>5.BrBG</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705221956955.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>6.BrBG_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222053250.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><em>相信聪明的你们发现了颜色名称的某个规律 XXX_r 与 XXX 大多只是颜色顺序相反，故以下只显示 XXX</em></td>
<td></td>
</tr>
<tr>
<td><strong>7.BuGn</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222450188.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>8.BuPu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222523230.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>9.CMRmap</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222545523.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>10.Dark2</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222706888.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>11.Dark2_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222739608.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>12.GnBu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705222816501.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>13.Greens</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223538747.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>14.Greys</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223715276.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>15.OrRd</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223759570.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>16.Oranges</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223840601.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>17.PRGn</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223911555.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>18.Paired</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705223939745.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>19.Paired_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224011917.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>20.Pastel1</strong></td>
<td><img src="https://img-blog.csdnimg.cn/2020070522410197.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>21.Pastel1_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224133734.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>22.Pastel2</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224206318.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>23.Pastel2_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224252881.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>24.PiYG</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224320624.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>25.PuBu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224348864.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>26.PuBuGn</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224418436.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>27.PuOr</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224447620.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>28.PuRd</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224518127.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>29.Purples</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224642465.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>30.RdBu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224707150.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>31.RdYlBu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224740421.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>32.RdYlGn</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224902910.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>33.Reds</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705224923484.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>34.Set1</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225047406.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>35.Set1_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225105357.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>36.Set2</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225142778.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>37.Set2_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225116145.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>38.Set3</strong></td>
<td><img src="https://img-blog.csdnimg.cn/2020070522513594.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>39.Set3_r</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225125876.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>40.Spectral</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225230345.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>41.Wistia</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225253431.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>42.YlGn</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225317456.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>43.YlGnBu</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225342903.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>44.YlOrBr</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225414861.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td><strong>45.YlOrRd</strong></td>
<td><img src="https://img-blog.csdnimg.cn/20200705225438445.png" alt="在这里插入图片描述"></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色组合名称(首字母小写)</th>
<th>调色板显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.afmhot</td>
<td><img src="https://img-blog.csdnimg.cn/20200705225833490.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>2.autumn</td>
<td><img src="https://img-blog.csdnimg.cn/20200705225923448.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>3.binary</td>
<td><img src="https://img-blog.csdnimg.cn/20200705225946512.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>4.bone</td>
<td><img src="https://img-blog.csdnimg.cn/20200711222858478.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>5.brg</td>
<td><img src="https://img-blog.csdnimg.cn/20200711222917515.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>6.bwr</td>
<td><img src="https://img-blog.csdnimg.cn/20200711222937493.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>7.cividis</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223007874.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>8.cool</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223026393.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>9.coolwarm</td>
<td><img src="https://img-blog.csdnimg.cn/2020071122305035.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>10.copper</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223107272.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>11.cubehelix</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223136492.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>12.flag</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223207310.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>13.gist_earth</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223248722.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>14.gist_gray</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223337835.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>15.gist_heat</td>
<td><img src="https://img-blog.csdnimg.cn/2020071122360377.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>16.gist_ncar</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223633135.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>17.gist_rainbow</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223650940.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>18.gist_stern</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223708961.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>19.gist_yarg</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223726672.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>20.gnuplot</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223742799.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>21.gnuplot2</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223751528.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>22.gray</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223807222.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>23.hot</td>
<td><img src="https://img-blog.csdnimg.cn/20200711223830254.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>24.hsv</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224238846.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>25.icefire</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224251307.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>26.inferno</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224310304.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>27.jet</td>
<td><code>ValueError: No.</code></td>
</tr>
<tr>
<td>28.magma</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224519925.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>29.mako</td>
<td><img src="https://img-blog.csdnimg.cn/2020071122453289.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>30.nipy_spectral</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224547445.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>31.ocean</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224600931.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>32.pink</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224611215.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>33.plasma</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224621593.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>34.prism</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224634866.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>35.rainbow</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224649463.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>36.rocket</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224700998.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>37.seismic</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224712787.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>38.spring</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224722695.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>39.summer</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224734854.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>40.1.tab10</td>
<td><img src="https://img-blog.csdnimg.cn/20200711224957282.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>40.2.tab10_r</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225029402.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>41.1.tab20</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225054379.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>41.2.tab20_r</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225125799.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>42.1.tab20c</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225136691.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>42.2.tab20c_r</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225201378.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>43.terrain</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225233217.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>44.twilight</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225246534.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>45.twilight_shifted</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225257688.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>46.viridis</td>
<td><img src="https://img-blog.csdnimg.cn/2020071122531496.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>47.vlag</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225326549.png" alt="在这里插入图片描述"></td>
</tr>
<tr>
<td>48.winter</td>
<td><img src="https://img-blog.csdnimg.cn/20200711225346625.png" alt="在这里插入图片描述"></td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">color_palette() 不写参数则默认颜色</span><br><span class="line"></span><br><span class="line">current_palette = sns.color_palette() </span><br><span class="line"></span><br><span class="line">sns.palplot(current_palette)</span><br><span class="line"></span><br><span class="line"> plt.show()</span><br><span class="line"></span><br><span class="line">set_palette() 设置所有图的颜色</span><br><span class="line"></span><br><span class="line">sns.palplot(sns.color_palette("hls",8))</span><br><span class="line"></span><br><span class="line"> plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h5><span id="hls-palette-ji-yu-hls">hls_palette 基于hls</span><a href="#hls-palette-ji-yu-hls" class="header-anchor">#</a></h5><h5><span id="yan-se-de-hue-se-xiang-luminance-liang-du-ji-saturation-bao-he-du">颜色的hue(色相),Luminance(亮度)及Saturation(饱和度)</span><a href="#yan-se-de-hue-se-xiang-luminance-liang-du-ji-saturation-bao-he-du" class="header-anchor">#</a></h5><p>l-光度 lightness<br>s-饱和 saturation</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sns.palplot(sns.hls_palette(8,l=.7,s=.9)) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="xkcd-xuan-qu-yan-se">xkcd选取颜色</span><a href="#xkcd-xuan-qu-yan-se" class="header-anchor">#</a></h4><p>xkcd包含了一套众包努力的针对随机RGB色的命名。产生了954个可以随时通过xkcd_rgb字典中调用的命名颜色</p>
<p>plt.plot([0,1],[0,1],sns.xkcd_rgb[‘pale red’],lw = 3) #lw = 线宽度<br>plt.plot([0,1],[0,2],sns.xkcd_rgb[‘medium green’],lw = 3)<br>plt.plot([0,1],[0,3],sns.xkcd_rgb[‘denim blue’],lw = 3)<br>plt.show()</p>
<p><img src="https://img-blog.csdnimg.cn/20190216223007343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Ryb2tlX1pob3U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5><span id="se-xi-dui-bi">色系对比</span><a href="#se-xi-dui-bi" class="header-anchor">#</a></h5><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line"></span><br><span class="line">#默认</span><br><span class="line">plt.subplot(231)</span><br><span class="line">palette=sns.color_palette()</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br><span class="line">#设置饱和度；l-亮度 lightness；s-饱和 saturation</span><br><span class="line">plt.subplot(232)</span><br><span class="line">palette=sns.hls_palette(6, l=.7, s=.9)</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br><span class="line">#配对对比</span><br><span class="line">plt.subplot(233)</span><br><span class="line">palette=sns.color_palette("Paired",6)</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br><span class="line">#使用xkcd颜色来命名颜色</span><br><span class="line">plt.subplot(234)</span><br><span class="line">colors = ["pale red","windows blue", "amber", "greyish", "faded green", "dusty purple"]</span><br><span class="line">palette=sns.xkcd_palette(colors)</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br><span class="line">#渐变色</span><br><span class="line">plt.subplot(235)</span><br><span class="line">palette=sns.color_palette("Blues")#想要翻转渐变，可以在面板名称中添加一个_r后缀</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br><span class="line">#色调线性变换</span><br><span class="line">plt.subplot(236)</span><br><span class="line">palette=sns.cubehelix_palette(6, start=.75, rot=-.75)</span><br><span class="line">sns.boxplot(data=data,palette=palette)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200701142050146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHlfc2hlbnps,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Uwr86onk-1593584425956)(evernotecid://DD492144-9AFF-43C1-9BC0-5A625709FC62/appyinxiangcom/28357599/ENResource/p129)]"></p>
<h3><span id="step4-zi-ti">Step4 字体</span><a href="#step4-zi-ti" class="header-anchor">#</a></h3><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from matplotlib import rcParams</span><br><span class="line">font1 = {</span><br><span class="line">        'font.family':'serif',</span><br><span class="line">        'font.serif': 'Times New Roman',</span><br><span class="line">         'font.style': 'normal',</span><br><span class="line">         'font.size': 12,</span><br><span class="line">         }</span><br><span class="line">rcParams.update(font1)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from matplotlib import rcParams</span><br><span class="line">font1 = {</span><br><span class="line">        'font.family':'serif',</span><br><span class="line">        'font.serif': 'Times New Roman',</span><br><span class="line">         'font.style': 'normal',</span><br><span class="line">         'font.size': 12,</span><br><span class="line">         }</span><br><span class="line">rcParams.update(font1)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="step5-an-ding-zhi-xu-qiu-tu">Step5 按定制需求图</span><a href="#step5-an-ding-zhi-xu-qiu-tu" class="header-anchor">#</a></h3><p>Seaborn 一共拥有 50 多个 API 类，相比于 Matplotlib 数千个的规模，可以算作是短小精悍了。其中，根据图形的适应场景，Seaborn 的绘图方法大致分类 6 类，分别是：关联图、类别图、分布图、回归图、矩阵图和组合图。而这 6 大类下面又包含不同数量的绘图函数。</p>
<h5><span id="guan-lian-tu-api-lei">关联图：API类</span><a href="#guan-lian-tu-api-lei" class="header-anchor">#</a></h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">关联性分析</th>
<th style="text-align:center">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">relplot</td>
<td style="text-align:center">绘制关系图</td>
</tr>
<tr>
<td style="text-align:center">scatterplot</td>
<td style="text-align:center">多维度分析散点图</td>
</tr>
<tr>
<td style="text-align:center">lineplot</td>
<td style="text-align:center">多维度分析线形图</td>
</tr>
</tbody>
</table>
</div>
<h5><span id="lei-bie-tu">类别图</span><a href="#lei-bie-tu" class="header-anchor">#</a></h5><ul>
<li>分类散点图：<a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html"><code>stripplot()</code></a> (<code>kind="strip"</code>)<a href="https://seaborn.pydata.org/generated/seaborn.swarmplot.html"><code>swarmplot()</code></a> (<code>kind="swarm"</code>)</li>
<li>分类分布图：<a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html"><code>boxplot()</code></a> (<code>kind="box"</code>)<a href="https://seaborn.pydata.org/generated/seaborn.violinplot.html"><code>violinplot()</code></a> (<code>kind="violin"</code>)<a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html"><code>boxenplot()</code></a> (<code>kind="boxen"</code>)</li>
<li>分类估计图：<a href="https://seaborn.pydata.org/generated/seaborn.pointplot.html"><code>pointplot()</code></a> (<code>kind="point"</code>)<a href="https://seaborn.pydata.org/generated/seaborn.barplot.html"><code>barplot()</code></a> (<code>kind="bar"</code>)<a href="https://seaborn.pydata.org/generated/seaborn.countplot.html"><code>countplot()</code></a> (<code>kind="count"</code>)</li>
</ul>
<p>seaborn作为一个库，里面包含了很多类，如PairGrid类。创造相应类的实例（传参)，就可以调用类里面的函数，参数，进行个性化设置。</p>
<ol>
<li><p>导入库 2. 创造类实例 3. 调用实例的参数和方法 4. show()</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">g = sns.FacetGrid(tips, col=<span class="string">"time"</span>)</span><br><span class="line"><span class="comment"># 创造了FaceGrid的实例。下面给出了FaceGrid的初始化方法。通过g可以调用类里面的函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FacetGrid</span>(<span class="params">Grid</span>):</span></span><br><span class="line">    <span class="string">"""Multi-plot grid for plotting conditional relationships."""</span></span><br><span class="line"><span class="meta">    @_deprecate_positional_args</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self, data, *,</span></span></span><br><span class="line"><span class="function"><span class="params">        row=None, col=None, hue=None, col_wrap=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        sharex=True, sharey=True, height=<span class="number">3</span>, aspect=<span class="number">1</span>, palette=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        row_order=None, col_order=None, hue_order=None, hue_kws=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        dropna=False, legend_out=True, despine=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        margin_titles=False, xlim=None, ylim=None, subplot_kws=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        gridspec_kws=None, size=None</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br></pre></td></tr></tbody></table></figure>
</li>
</ol>
<h4><span id="fen-bu-tu">分布图</span><a href="#fen-bu-tu" class="header-anchor">#</a></h4><p>   分布图主要是用于可视化变量的分布情况，一般分为单变量分布和多变量分布。当然这里的多变量多指二元变量，更多的变量无法绘制出直观的可视化图形。</p>
<p>   单变量分布</p>
<p>   <a href="https://seaborn.pydata.org/generated/seaborn.distplot.html"><code>distplot</code></a>，<a href="https://seaborn.pydata.org/generated/seaborn.kdeplot.html"><code>kdeplot</code></a></p>
<p>   多变量分布图</p>
<p>   <a href="https://seaborn.pydata.org/generated/seaborn.jointplot.html"><code>jointplot</code></a>，<a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html"><code>pairplot</code></a></p>
<h5><span id="hui-gui-tu">回归图</span><a href="#hui-gui-tu" class="header-anchor">#</a></h5><p>   回归图的绘制函数主要有：<a href="https://seaborn.pydata.org/generated/seaborn.lmplot.html"><code>lmplot</code></a> 和 <a href="https://seaborn.pydata.org/generated/seaborn.regplot.html"><code>regplot</code></a>。</p>
<h5><span id="ju-zhen-tu">矩阵图</span><a href="#ju-zhen-tu" class="header-anchor">#</a></h5><p>   <a href="https://seaborn.pydata.org/generated/seaborn.heatmap.html"><code>heatmap</code></a> 和 <a href="https://seaborn.pydata.org/generated/seaborn.clustermap.html"><code>clustermap</code></a></p>
<p>   <a href="https://github.com/mwaskom/seaborn/tree/master/seaborn">官方文档</a></p>
<h4><span id="duo-hui-tu-wang-ge">多绘图网格</span><a href="#duo-hui-tu-wang-ge" class="header-anchor">#</a></h4><h2><span id="1-xiao-ping-mian-wang-ge">1.小平面网格</span><a href="#1-xiao-ping-mian-wang-ge" class="header-anchor">#</a></h2><p>1.1 FaceGrid</p>
<p>1.2 FacetGrid.map</p>
<p>1.3 FacetGrid.map_dataframe</p>
<h2><span id="2-pei-dui-wang-ge">2.配对网格</span><a href="#2-pei-dui-wang-ge" class="header-anchor">#</a></h2><p>2.1 PairGrid</p>
<p>2.2 PairGrid.map</p>
<p>2.3 PairGrid.map_diag</p>
<p>2.4 PairGrid.map_offdiag</p>
<p>2.5 PairGrid.map_lower</p>
<p>2.6 PairGrid.map_upper</p>
<h2><span id="3-lian-he-wang-ge">3.联合网格</span><a href="#3-lian-he-wang-ge" class="header-anchor">#</a></h2><p>3.1 JointGrid</p>
<p>3.2 JointGrid.plot</p>
<p>3.3 JointGrid.plot_joint</p>
<p>3.4 JointGrid.plot_marginals</p>
<h5><span id="pairgrid">PairGrid</span><a href="#pairgrid" class="header-anchor">#</a></h5><p>可绘制两种类型的图，一种是指定<strong>{x, y}_vars</strong>；另一类：x和y轴相同。可以调用方法进行绘图。</p>
<p><img src="/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BSeaborn/Users\ADMIN\Documents\Apowersoft\Apowersoft Online Screen Recorder\20201010_211035.gif" alt="20201010_211035"></p>
<p>二类：x,y轴相同</p>
<p><img src="/2020/10/11/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BSeaborn/Users\ADMIN\Documents\Apowersoft\Apowersoft Online Screen Recorder\20201010_211219.gif" alt="20201010_211219"></p>
<h2><span id="zu-he-tu">组合图</span><a href="#zu-he-tu" class="header-anchor">#</a></h2><h2><span id="cuo-wu">错误</span><a href="#cuo-wu" class="header-anchor">#</a></h2><p>如果利用pairGrig，pairplot画回归图，似乎有的回归曲线加不上。其他的图没有问题</p>
<h2><span id="ke-yi-jiang-dan-ge-tu-zu-he">可以将单个图组合</span><a href="#ke-yi-jiang-dan-ge-tu-zu-he" class="header-anchor">#</a></h2><p>主要网格不行</p>
<p>支出ax参数的可以</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))</span><br><span class="line">#我们在这里生成一个fig和三个axes，在下面绘图的时候只需要用ax参数来指定特定的axes就可以了</span><br><span class="line">sns.barplot(x='class', y='age', data=titanic, ax=ax1)</span><br><span class="line">sns.countplot(x='sex', data=titanic, ax=ax2)</span><br><span class="line">sns.distplot(titanic['age'], ax=ax3)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="hui-zong">汇总</span><a href="#hui-zong" class="header-anchor">#</a></h3><p><a href="http://seaborn.pydata.org/api.html#">http://seaborn.pydata.org/api.html#</a></p>
<p><a href="https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py">https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py</a></p>
<p><a href="https://xkcd.com/color/rgb/">https://xkcd.com/color/rgb/</a></p>
]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>绘图</tag>
        <tag>Seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title>Social-and-Economic-Data-Set</title>
    <url>/2020/10/04/Social-and-Economic-Data-Set/</url>
    <content><![CDATA[<h1><span id="tong-ji-nian-jian">统计年鉴</span><a href="#tong-ji-nian-jian" class="header-anchor">#</a></h1><p>知网：<a href="https://data.cnki.net/Yearbook/Navi?type=type&amp;code=A">中国经济社会大数据研究平台</a></p>
<p>统计局：<a href="http://www.stats.gov.cn/tjsj/">国家统计局</a></p>
<h1><span id="di-tu">地图</span><a href="#di-tu" class="header-anchor">#</a></h1><ul>
<li>全国基础地理数据库：<a href="http://www.webmap.cn/">http://www.webmap.cn</a></li>
</ul>
<h1><span id="xing-zheng-qu">行政区</span><a href="#xing-zheng-qu" class="header-anchor">#</a></h1><h2><span id="xing-zheng-qu-hua-fen">行政区划分</span><a href="#xing-zheng-qu-hua-fen" class="header-anchor">#</a></h2><p>民政部发布的行政区划代码： <a href="http://www.mca.gov.cn/article/sj/xzqh/">http://www.mca.gov.cn/article/sj/xzqh/</a></p>
<p>统计用区划代码和城乡划分代码编制规则 <a href="http://www.stats.gov.cn/tjsj/tjbz/200911/t20091125_8667.html">http://www.stats.gov.cn/tjsj/tjbz/200911/t20091125_8667.html</a></p>
<p><a href="http://lbsyun.baidu.com/index.php?title=open/dev-res">http://lbsyun.baidu.com/index.php?title=open/dev-res</a></p>
<p><strong>国民经济和社会发展统计公报</strong></p>
<p><a href="https://www.cnstats.org/tjgb/201605/gddws-2015-qdc.html">中国统计信息网</a></p>
]]></content>
      <categories>
        <category>Economy</category>
      </categories>
      <tags>
        <tag>economy</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-k近邻法</title>
    <url>/2020/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-k%E8%BF%91%E9%82%BB%E6%B3%95/</url>
    <content><![CDATA[<p>K近邻(k-nearest neighbor, KNN)是一种基于相似的算法，既适用于分类也适用于回归，是属于监督学习算法。</p>
<p>KDD is a simple but most used algorithm in classification which belongs to supervised learning</p>
<a id="more"></a>
<h1><span id="k-jin-lin-fa-de-jiang-jie">k近邻发的讲解</span><a href="#k-jin-lin-fa-de-jiang-jie" class="header-anchor">#</a></h1><p>一句话就是：物以类聚，人以群分。</p>
<p>假定有一个带标签的测试集，基于某种距离度量，然后找到每个训练集与其最靠近的k个训练样本，并在基于此k个‘邻居’的信息来预测。</p>
<h2><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor">#</a></h2><ol>
<li>思想简单，易于实现，无需估计参数，无需训练</li>
<li>适合对稀有事物进行分类</li>
<li>适合多分类</li>
</ol>
<h2><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor">#</a></h2><ol>
<li>内存开销大，因为要计算每个样本的距离</li>
<li>如果样本不平衡，如果每个样本异常，容易造成分类效果差</li>
<li>解释性差</li>
</ol>
<h1><span id="k-jin-lin-mo-xing">k近邻模型</span><a href="#k-jin-lin-mo-xing" class="header-anchor">#</a></h1><h2><span id="mo-xing">模型</span><a href="#mo-xing" class="header-anchor">#</a></h2><p>确定训练集</p>
<p>距离度量</p>
<p>k值</p>
<p>分类决策规则</p>
<h2><span id="ju-chi-du-liang">距离度量</span><a href="#ju-chi-du-liang" class="header-anchor">#</a></h2><p>研究的是特征空间($R^n$)，$x_i,x_j$的$L_p$距离定义为</p>
<script type="math/tex; mode=display">
L_p(x_i,x_j) = (\sum_{l = 1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}</script><p>欧式距离</p>
<script type="math/tex; mode=display">
L_2(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}</script><p>曼哈顿距离</p>
<script type="math/tex; mode=display">
L_1(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|)</script><p>切比雪夫距离</p>
<script type="math/tex; mode=display">
L_1(x_i,x_j) = max(|x_i^{(l)}-x_j^{(l)}|)</script><h2><span id="k-zhi-de-xuan-ze">k值的选择</span><a href="#k-zhi-de-xuan-ze" class="header-anchor">#</a></h2><p>k太小，容易过拟合</p>
<p>k太大，容易欠拟合</p>
<h1><span id="kd-shu">KD树</span><a href="#kd-shu" class="header-anchor">#</a></h1><p>KD树是一种二叉树，其中每个节点都是一个k维数值点，每个节点都表示一个与当前划分维度的坐标轴垂直的超平面。</p>
<p><img src="/2020/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-k%E8%BF%91%E9%82%BB%E6%B3%95/MyBlog\MyBlog\hexo\source\_posts\机器学习-k近邻法\image-20200929203239695.png" alt="image-20200929203239695"></p>
<ol>
<li>根节点。取中位数。</li>
<li>然后在划分</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20190421162333605.png#pic_center" alt="在这里插入图片描述"></p>
<p><strong>kd树的最近邻搜索算法：</strong></p>
<ol>
<li>从根节点出发，递归的向下访问kd树，若目标点x x<em>x</em>当前维的坐标小于切分点的坐标，则向左子树移动，否则向右子树移动。直到子结点为叶结点为止。</li>
<li>以此叶结点为“当前最近点”，此时以目标点为圆心（球心），目标点到此叶结点距离为半径，能够得到一个圆（球体）。</li>
<li>从叶结点向上回退到该叶节点的父节点：<br>（a）计算目标点和父节点的距离，如果距离目标点更近，则以该父节点为“当前最近点”，距离为“当前最近距离”。<br>（b）检查该父结点是否在（2）中的球体内（检查的方法见下面例子中红色字体），如若不在球内，则说明该父节点的另外一边子树区域不可能存在比当前点距离目标点更近的点，继续向上回退，继续判断。<br>如若在球内，则说明该父结点另外子树对应的区域中可能存在距离目标点更近的点，则执行以该结点的兄弟结点（比如该结点为左节点，则兄弟为右结点）为根节点再次执行最近邻搜索算法 <strong>敲黑板：</strong> 这个红字部分是很多自称熟悉了kd树算法的人都会搞错的一个细节，李航的书中只说了 “递归的执行最近邻搜索” 这么一句话，但是可能很难深入理解到我上面说的。</li>
<li>当回退到根节点时，搜索结束。</li>
</ol>
<p>划分了区域，检查父结点的分支的区域是否有更近的结点。</p>
<p>先到叶子结点。计算当前的最小距离。然后反向回溯。判断是否跳到另外一个空间去搜索。（当前最小距离）</p>
<p>圆心：节点。然后计算距离，是否搜索节点的另外一个空间。如果要，需要加入新的搜索路径。</p>
<h2><span id="sou-suo">搜索</span><a href="#sou-suo" class="header-anchor">#</a></h2><p>找到当前最近邻</p>
<p>然后回溯</p>
<p>检测：更新‘当前最近’</p>
<p>回溯</p>
<p>检查：</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>k近邻法</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析之Excel</title>
    <url>/2020/09/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel/</url>
    <content><![CDATA[<p>此篇Blog本着详细总结关于利用Excel这一工具处理，分析小规模数据集的相关内容。下面将从五个篇章展开，分别是基础篇、数据清洗篇、统计函数篇、关联匹配类篇、数据分析篇、数据可视化篇</p>
<p><img src="/2020/09/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel/Excel工具进行数据分析-提纲.svg" alt></p>
<a id="more"></a>
<h1><span id="ji-chu-pian">基础篇</span><a href="#ji-chu-pian" class="header-anchor">#</a></h1><h2><span id="zi-ti-he-zi-hao">字体和字号</span><a href="#zi-ti-he-zi-hao" class="header-anchor">#</a></h2><h3><span id="zi-ti">字体</span><a href="#zi-ti" class="header-anchor">#</a></h3><p>如果是word,中文：宋体；英文：新罗马字体</p>
<p>如果是PPT，就要按场景更换了</p>
<h3><span id="zi-hao">字号</span><a href="#zi-hao" class="header-anchor">#</a></h3><p><strong>汉字：号数制。</strong>汉字大小定为七个号数等级——按，1号、2号、3号、4号、5号、6号、7号由大至小排列。在字号等级之间又增加一些字号，并取名为“小几号字”，如“小4号””、“小5号”等等。</p>
<p><strong>国际：点数制。</strong>国际上最通行的印刷字体的计量方法，“点”是国际上计量字体大小的基本单位，从英文“Point’，译音而来，一般用小写‘p’来表示，俗称“磅”。磅值(pt)</p>
<p>两者的对于关系：</p>
<ol>
<li><p>字号‘八号’对应磅值5</p>
</li>
<li><p>字号‘七号’对应磅值5.5</p>
</li>
<li><p>字号‘小六’对应磅值6.5</p>
</li>
<li><p>字号‘六号’对应磅值7.5</p>
</li>
<li><p>字号‘小五’对应磅值9</p>
</li>
<li><p>字号‘五号’对应磅值10.5</p>
</li>
<li><p>字号‘小四’对应磅值12</p>
</li>
<li><p>字号‘四号’对应磅值14</p>
</li>
<li><p>字号‘小三’对应磅值15</p>
</li>
<li><p>字号‘三号’对应磅值16</p>
</li>
<li><p>字号‘小二’对应磅值18</p>
</li>
<li><p>字号‘二号’对应磅值22</p>
</li>
<li><p>字号‘小一’对应磅值24</p>
</li>
<li><p>字号‘一号’对应磅值26</p>
</li>
<li><p>字号‘小初’对应磅值36</p>
</li>
<li><p>字号‘初号’对应磅值42</p>
</li>
</ol>
<h2><span id="jian-ju">间距</span><a href="#jian-ju" class="header-anchor">#</a></h2><p>行间距</p>
<p>标题，段前段后</p>
<h2><span id="yin-yong">引用</span><a href="#yin-yong" class="header-anchor">#</a></h2><p>绝对引用和相对引用。绝对引用$</p>
<h1><span id="shu-ju-qing-xi-pian">数据清洗篇</span><a href="#shu-ju-qing-xi-pian" class="header-anchor">#</a></h1><p>在数据清洗的过程中，涉及的操作可能有：字符串含空格；提取字符；时间格式设置；标准化；</p>
<p><strong>数据去重</strong></p>
<p><strong>数据缺失值</strong></p>
<p><strong>异常值</strong></p>
<p><strong>噪音处理</strong></p>
<p>噪音，是被测量变量的随机误差或方差。我们在上文中提到过异常点（离群点），那么离群点和噪音是不是一回事呢？我们知道，观测量(Measurement) = 真实数据(True Data) + 噪声 (Noise)。离群点(Outlier)属于观测量，既有可能是真实数据产生的，也有可能是噪声带来的，但是总的来说是和大部分观测量之间有明显不同的观测值。</p>
<p><img src="/2020/09/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel/清洗处理类.svg" alt="清洗处理类"></p>
<h1><span id="tong-ji-han-shu-pian">统计函数篇</span><a href="#tong-ji-han-shu-pian" class="header-anchor">#</a></h1><p>统计类函数</p>
<h1><span id="guan-lian-pi-pei-lei-pian">关联匹配类篇</span><a href="#guan-lian-pi-pei-lei-pian" class="header-anchor">#</a></h1><p><img src="/2020/09/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel/关联匹配类.png" alt="关联匹配类"></p>
<h1><span id="shu-ju-fen-xi-pian">数据分析篇</span><a href="#shu-ju-fen-xi-pian" class="header-anchor">#</a></h1><p>数据透析表</p>
<p>分类汇总</p>
<h1><span id="shu-ju-ke-shi-hua-pian">数据可视化篇</span><a href="#shu-ju-ke-shi-hua-pian" class="header-anchor">#</a></h1><p><a href="https://zhuanlan.zhihu.com/p/110389312">https://zhuanlan.zhihu.com/p/110389312</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/97316717">https://zhuanlan.zhihu.com/p/97316717</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/74625617">https://zhuanlan.zhihu.com/p/74625617</a></p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title>生活终是慢慢开出灿烂的花朵</title>
    <url>/2020/09/26/%E7%94%9F%E6%B4%BB%E7%BB%88%E6%98%AF%E6%85%A2%E6%85%A2%E5%BC%80%E5%87%BA%E7%81%BF%E7%83%82%E7%9A%84%E8%8A%B1%E6%9C%B5/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="f484cf056efb4c9e995588eea26cb931df0631c2ecff4efb1bb27e74a2b9c2f8">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>美好</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析-七周</title>
    <url>/2020/09/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%B8%83%E5%91%A8/</url>
    <content><![CDATA[<p>以业务为核心目的的数据分析。</p>
<a id="more"></a>
<p>数据分析： 数据+统计分析+洞察/洞见（商业价值)</p>
<p>路径：数据清洗+数据探索+数据分析+结果检验+数据可视化</p>
<h1><span id="shu-ju-fen-xi-di-yi-zhou">数据分析-第一周</span><a href="#shu-ju-fen-xi-di-yi-zhou" class="header-anchor">#</a></h1><h2><span id="step1-ming-que-mu-de">Step1: 明确目的</span><a href="#step1-ming-que-mu-de" class="header-anchor">#</a></h2><h2><span id="step2-guan-cha-shu-ju">Step2: 观察数据</span><a href="#step2-guan-cha-shu-ju" class="header-anchor">#</a></h2><h3><span id="shu-ju-you-wu-que-shi-zhi"><strong>数据有无缺失值</strong></span><a href="#shu-ju-you-wu-que-shi-zhi" class="header-anchor">#</a></h3><h3><span id="shu-ju-shi-fou-yi-zhi-hua"><strong>数据是否一致化</strong></span><a href="#shu-ju-shi-fou-yi-zhi-hua" class="header-anchor">#</a></h3><h3><span id="shu-ju-shi-fou-you-zang-shu-ju"><strong>数据是否有脏数据</strong></span><a href="#shu-ju-shi-fou-you-zang-shu-ju" class="header-anchor">#</a></h3><p>乱码，错位，重复值，未匹配数据，加密数据等。能影响到分析的都算脏数据，没有一致化也可以算。</p>
<h3><span id="shu-ju-biao-zhun-jie-gou">数据标准结构</span><a href="#shu-ju-biao-zhun-jie-gou" class="header-anchor">#</a></h3><h2><span id="step-3-shu-ju-qing-xi">Step 3: 数据清洗</span><a href="#step-3-shu-ju-qing-xi" class="header-anchor">#</a></h2><p><img src="/2020/09/24/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%B8%83%E5%91%A8/MyBlog\MyBlog\hexo\source\_posts\数据分析-七周\Excwel.png" alt></p>
<p>百度一些你就知道</p>
<h3><span id="qing-xi-chu-li-lei">清洗处理类</span><a href="#qing-xi-chu-li-lei" class="header-anchor">#</a></h3>]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析-路线</title>
    <url>/2020/09/23/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E8%B7%AF%E7%BA%BF/</url>
    <content><![CDATA[<p><a href="https://ask.hellobi.com/blog/qinlu/8930">https://ask.hellobi.com/blog/qinlu/8930</a></p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>路线</tag>
      </tags>
  </entry>
  <entry>
    <title>科研之分析方法论</title>
    <url>/2020/09/16/%E7%A7%91%E7%A0%94%E4%B9%8B%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%E8%AE%BA/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="972ba472c07784402783f060d7284491f7ec6c6cb41628893ee61b3c68c8dcea">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>计量经济学</title>
    <url>/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/</url>
    <content><![CDATA[<p>就是呈现一些关于计量经济学的相关实践方法</p>
<a id="more"></a>
<h1><span id="ji-liang-jing-ji-xue">计量经济学</span><a href="#ji-liang-jing-ji-xue" class="header-anchor">#</a></h1><p>计量经济学的模型估计的含义，不是确定的值，而是随机变量，反映了平均影响（期望)。还有随机误差项和普通方程的区别</p>
<p>面板模型bai引入固定时间效应stata操作方法：<br>xi: xtreg y x1 x2 x3 i.year,fe 双向固定du效应，既可以控制年度效zhi应，又可以用固dao定效应消除部分内生性<br>xi: xtreg y x1 x2 x3 i.year LSDV法 就是虚拟变量最小二乘回归<br>另外，建议用聚类稳健标准差,这是解决异方差的良药<br>xi: xtreg y x1 x2 x3 i.year,fe vce(cluster.个体变量)<br>xi: xtreg y x1 x2 x3 i.year ，vce(cluster.个体变量)</p>
<p><a href="https://www.jianshu.com/p/baef9e9bc75d">https://www.jianshu.com/p/baef9e9bc75d</a></p>
<p><a href="https://wenku.baidu.com/view/6cd81d7be518964bcf847c75.html">https://wenku.baidu.com/view/6cd81d7be518964bcf847c75.html</a></p>
<p><a href="https://www.stata.com/manuals13/xtxtreg.pdf">https://www.stata.com/manuals13/xtxtreg.pdf</a></p>
<p><a href="https://stata-club.github.io/stata_article/2017-06-02.html">https://stata-club.github.io/stata_article/2017-06-02.html</a></p>
<h1><span id="san-da-mo-xing">三大模型</span><a href="#san-da-mo-xing" class="header-anchor">#</a></h1><p>固定效应模型，随机效应模型和混合效应模型</p>
<h2><span id="hun-he-gu-ji-mo-xing">混合估计模型</span><a href="#hun-he-gu-ji-mo-xing" class="header-anchor">#</a></h2><p>reg cp ip</p>
<h2><span id="gu-ding-xiao-ying-mo-xing">固定效应模型</span><a href="#gu-ding-xiao-ying-mo-xing" class="header-anchor">#</a></h2><p>个体，时间，时间和个体</p>
<h2><span id="sui-ji-xiao-ying-mo-xing">随机效应模型</span><a href="#sui-ji-xiao-ying-mo-xing" class="header-anchor">#</a></h2><p>tsset id year<br>xtreg cp ip,re</p>
<p>五、针对固定效应和随机效应模型选择主要根据Hausman检验结果判定：　　　<br>xtreg cp ip, fe<br>est store FE<br>xtreg cp ip, re<br>est store RE<br>hausman FE RE　由于原假设是随机效应和固定效应无差异，如果拒绝原假设，则采用固定效应模型，否则随机效应模型。</p>
<p><a href="http://www.jeepxie.net/article/928076.html">http://www.jeepxie.net/article/928076.html</a></p>
<p>Three econometric models—a pooled model, a fixed-effect<br>8 model, and a random-effect model  </p>
<p>方差分析主要有三种模型：即固定效应模型（fixed effects model），随机效应模型（random effects model），混合效应模型（mixed effects model）。</p>
<p>所谓的固定、随机、混合，主要是针对分组变量而言的。</p>
<p>固定效应模型，表示你打算比较的就是你现在选中的这几组。例如，我想比较3种药物的疗效，我的目的就是为了比较这三种药的差别，不想往外推广。这三种药不是从很多种药中抽样出来的，不想推广到其他的药物，结论仅限于这三种药。“固定”的含义正在于此，这三种药是固定的，不是随机选择的。</p>
<p>随机效应模型，表示你打算比较的不仅是你的设计中的这几组，而是想通过对这几组的比较，推广到他们所能代表的总体中去。例如，你想知道是否名牌大学的就业率高于普通大学，你选择了北大、清华、北京工商大学、北京科技大学4所学校进行比较，你的目的不是为了比较这4所学校之间的就业率差异，而是为了说明他们所代表的名牌和普通大学之间的差异。你的结论不会仅限于这4所大学，而是要推广到名牌和普通这样的一个更广泛的范围。“随机”的含义就在于此，这4所学校是从名牌和普通大学中随机挑选出来的。混合效应模型就比较好理解了，就是既有固定的因素，也有随机的因素。</p>
<p>一般来说，只有固定效应模型，才有必要进行两两比较，随机效应模型没有必要进行两两比较，因为研究的目的不是为了比较随机选中的这些组别。</p>
<p>固定效应和随机效应的选择是大家做面板数据常常要遇到的问题，一个常见的方法是做huasman检验，即先估计一个随机效应，然后做检验，如果拒绝零假设，则可以使用固定效应，反之如果接受零假设，则使用随机效应。但这种方法往往得到事与愿违的结果。另一个想法是在建立模型前根据数据性质确定使用那种模型，比如数据是从总体中抽样得到的，则可以使用随机效应，比如从N个家庭中抽出了M个样本，则由于存在随机抽样，则建议使用随机效应，反之如果数据是总体数据，比如31个省市的Gdp，则不存在随机抽样问题，可以使用固定效应。同时，从估计自由度角度看，由于固定效应模型要估计每个截面的参数，因此随机效应比固定效应有较大的自由度.</p>
<p>固定效应模型<br>　　固定效应模型（fixed effects model）的应用前提是假定全部研究结果的方向与效应大小基本相同，即各独立研究的结果趋于一致，一致性检验差异无显著性。因此固定效应模型适用于各独立研究间无差异，或差异较小的研究。<br>　　固定效应模型是指实验结果只想比较每一自变项之特定类目或类别间的差异及其与其他自变项之特定类目或类别间交互作用效果，而不想依此推论到同一自变项未包含在内的其他类目或类别的实验设计。例如：研究者想知道教师的认知类型在不同教学方法情境中，对儿童学习数学的效果有何不同，其中教师和学生的认知类型，均指场地依赖型和场地独立型，而不同的教学方法，则指启发式、讲演式、编序式。当实验结束时，研究者仅就两种类型间的交互作用效果及类型间的差异进行说明，而未推论到其他认知类型，或第四种教学方法。象此种实验研究模式，即称为固定效果模式。与本词相对者是随机效应模型（random effect model）、混合效应模型（mixed effect model）。</p>
<p>随机效应模型 random effects models<br>　　随机效应模型(random effects models)是经典的线性模型的一种推广，就是把原来（固定）的回归系数看作是随机变量，一般都是假设是来自正态分布。如果模型里一部分系数是随机的，另外一些是固定的，一般就叫做混合模型（mixed models）。<br>　　虽然定义很简单，对线性混合模型的研究与应用也已经比较成熟了，但是如果从不同的侧面来看，可以把很多的统计思想方法综合联系起来。概括地来说，这个模型是频率派和贝叶斯模型的结合，是经典的参数统计到高维数据分析的先驱，是拟合具有一定相关结构的观测的典型工具。<br>　　随机效应最直观的用处就是把固定效应推广到随机效应。注意，这时随机效应是一个群体概念，代表了一个分布的信息 or 特征，而对固定效应而言，我们所做的推断仅限于那几个固定的（未知的）参数。例如，如果要研究一些水稻的品种是否与产量有影响，如果用于分析的品种是从一个很大的品种集合里随机选取的，那么这时用随机效应模型分析就可以推断所有品种构成的整体的一些信息。这里，就体现了经典的频率派的思想-任何样本都来源于一个无限的群体(population)。<br>同时，引入随机效应就可以使个体观测之间就有一定的相关性，所以就可以用来拟合非独立观测的数据。经典的就有重复观测的数据，多时间点的记录等等，很多时候就叫做纵向数据(longitudinal data)，已经成为很大的一个统计分支。</p>
<p>随机效应模型(random effects models)，简称REM，是经典的线性模型的一种推广，就是把原来（<a href="https://baike.baidu.com/item/固定效应模型/628635">固定效应模型</a>）的回归系数看作是随机变量，一般都是假设是来自正态分布。如果模型里一部分系数是随机的，另外一些是固定的，一般就叫做<a href="https://baike.baidu.com/item/混合模型/6048939">混合模型</a>（mixed models）</p>
<h2><span id="xtref">xtref</span><a href="#xtref" class="header-anchor">#</a></h2><h2><span id="reg">reg</span><a href="#reg" class="header-anchor">#</a></h2><h2><span id="gu-ding-xiao-ying-hui-gui">固定效应回归</span><a href="#gu-ding-xiao-ying-hui-gui" class="header-anchor">#</a></h2><p>实际上是个体效应</p>
<p>如果要时间效应，效应自己添加时间固定效应</p>
<p><img src="file:///C:\Users\ADMIN\AppData\Roaming\Tencent\Users\2323020965\QQ\WinTemp\RichOle\%RD8D0A3HLYAMXPV831`S}X.png" alt="img"></p>
<p> 最后一个可以添加很多个虚拟变量</p>
<h1><span id="stata-mian-ban-shu-ju-mo-xing-jin-xing-hausman-jian-yan"><strong>STATA面板数据模型进行Hausman检验</strong></span><a href="#stata-mian-ban-shu-ju-mo-xing-jin-xing-hausman-jian-yan" class="header-anchor">#</a></h1><h2><span id="shi-jian-gu-ding-xiao-ying-he-ge-ti-xiao-ying-mo-xing-he-sui-ji-xiao-ying-mo-xing">时间固定效应和个体效应模型和随机效应模型</span><a href="#shi-jian-gu-ding-xiao-ying-he-ge-ti-xiao-ying-mo-xing-he-sui-ji-xiao-ying-mo-xing" class="header-anchor">#</a></h2><p>二、固定效应模型<br>1.个体固定效应模型：<br> tsset id year　　　　<br> xtreg Y X, fe 或者xtreg Y X , fe i(id)　　　　<br> 针对个体固定效应（H0：不存在个体固定效应）的F检验自动生成，如果p&lt;=10%则应该选择个体固定效应。　　<br>2.时刻固定效应模型：<br>（1）麻烦的间接方法<br> tsset id year　　　　<br> xi:reg Y X i.year<br>  对于时间固定效应模型的检验不是很直接，要用wald检验，相应的命令为：<br>  建设是四年数据，时间虚变量为_Iyear_2、 _Iyear_3、 _Iyear_4，那么wald检验<br>  test _Iyear_2=_Iyear_3= _Iyear_4<br>  test _Iyear_2=_Iyear_3= _Iyear_4=0<br>（2）巧妙的方法<br>   这个方法有点麻烦，后来论坛中有人聪明的提出一种方法，让人眼前一亮，就是将时间和截面变量交换位置，之前得到的是个体固定效应，之后就是时间固定效应，具体如下：<br> tsset year id<br> xtreg Y X,fe<br>  针对时期固定效应（H0：不存在时期固定效应）的F检验自动生成。<br>我刚开始对此方法不是很有信心，最后自己将其与第一种方法做了对比发觉，估计的参数值和其他统计量均为一致性，因此推荐后面这种方法。　<br>（3）直接的方法<br>  参照个体固定效应的方法，我们再推荐一种简便直接方法：<br>  tsset id year　　　　<br>  xtreg Y X ,fe i(year)</p>
<p>  针对时期固定效应（H0：不存在时期固定效应）的F检验自动生成。<br>  比较三种方法，第二、三种方法更为直接和有效，第一种与他们的区别还有一点就是常数项估计值不同，而第二种方法缺乏理论依据和现实做的人比较少，因此综合来看，第三种方法最为有效和直接。　　　<br>3.时刻个体双固定效应模型　<br>  实际上连玉君讲义中的时间效应（人大经济论坛出的stata论文专题讲义的p230）是时间个体双固定效应，可以这样理解fe只是固定个体效应，比如在个体固定效应模型中，输入fe和输入fe i(id)，得到的F值和p值均一致，另外从stata命令的中看sigma_u：panel-level standard deviation，F_f：F for u_i=0，均在说个体效应问题而时间效应已经通过设置时间虚拟变量进行了控制。具体方法如下：<br>　<br>  xtset id year<br>  xi:xtreg y x1 x2 i.year,fe<br>  这种方法有个问题是，估计的时候可能会出现：<br>“independent variables are collinear with the panel variable year”<br>解决的办法是从新生成一个panel varible比如code，此code是id和year的综合,前提是提前设置了<br>tsset id year。然后按照如下命令进行：<br>  gen code =year+id<br>  tsset code year<br>xi:xtreg y x1 x2 i.year,fe<br>针对时期固定效应（H0：不存在时期固定效应）的F检验自动生成。<br>最后在三种模型中到底选择哪个，主要根据F检验值是否显著进行判断，第一个显著后面不显著就选个体固定效应模型，第 二个显著其他不显著选择时间固定效应模型，第 三个显著意味着前两个均显著，那么选择个体时间双固定模型。<br>三、随机效应模型　　　　<br>tsset id year　<br> xtreg cp ip,re　　　　<br>　　　<br>四、回归系数不同的面板数据模型<br>by id: reg cp ip　　　　<br>然后把斜率＆截距整理合成一下就ok。</p>
<p>五、针对固定效应和随机效应模型选择主要根据Hausman检验结果判定：　　　<br>  xtreg cp ip, fe　　<br>  est store FE 　<br>  xtreg cp ip, re<br>　　est store RE　<br>  hausman FE  RE　由于原假设是随机效应和固定效应无差异，如果拒绝原假设，则采用固定效应模型，否则随机效应模型。　</p>
<h2><span id="sui-ji-xiao-ying-he-gu-ding-xiao-ying-de-xuan-ze">随机效应和固定效应的选择</span><a href="#sui-ji-xiao-ying-he-gu-ding-xiao-ying-de-xuan-ze" class="header-anchor">#</a></h2><p>tsset id year　 </p>
<p>  xtreg cp ip, fe　　<br>  est store FE 　<br>  xtreg cp ip, re<br>　　est store RE　<br>  hausman FE  RE</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1045132-24aa6e61e9ea3868.png?imageMogr2/auto-orient/strip|imageView2/2/w/873/format/webp" alt="img"></p>
<p>这里最重要的还是检验P值，P&lt;0.05说明可以拒绝原假设（随机效应回归模型），因此我们的模型采用固定效应回归模型更为合适。</p>
<h1><span id="did">DID</span><a href="#did" class="header-anchor">#</a></h1><p>政策前后，时间前，时间后，至少要知道两期</p>
<p>处理组和对照组：受不受影响。</p>
<p>截面数据：设置的方法</p>
<p>面板数据</p>
<p>**PSM-DID方法———————————————-</p>
<p>** PSM的部分<br>set seed 0001 //定义种子<br>gen tmp = runiform() //生成随机数<br>sort tmp //把数据库随机整理<br>psmatch2 treated $xlist, out(ln_w) logit ate neighbor(1) common caliper(.05) ties //通过近邻匹配，这里可以要outcome，也可以不要它</p>
<p>pstest $xlist, both graph //检验协变量在处理组与控制组之间是否平衡<br>gen common=_support<br>drop if common == 0 //去掉不满足共同区域假定的观测值</p>
<p>** DID的部分，根据上面匹配好的数据<br>reg ln_w did time treated $xlist </p>
<p>xtreg ln_w did time treated $xlist i.year, fe</p>
<p>**PSM-DID部分结束———————————————————</p>
<p><a href="https://mp.weixin.qq.com/s/yNZ95g8poUzyDGin7JMalA">https://mp.weixin.qq.com/s/yNZ95g8poUzyDGin7JMalA</a></p>
<p>控制组和对照组受政策影响的时间是否一致</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006161241325.png" alt="image-20201006161241325"></p>
<h1><span id="yi-fang-chai">异方差</span><a href="#yi-fang-chai" class="header-anchor">#</a></h1><p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006110349569.png" alt="image-20201006110349569"></p>
<p>对于每一个X,对应一个相同的分布。但实际上，可能不是。</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006110508755.png" alt="image-20201006110508755"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006110654746.png" alt="image-20201006110654746"></p>
<p>WLS: 加权最小二乘法(WLS)</p>
<p>white调整（最大)</p>
<p>截面数据</p>
<p>异方差检验：方法</p>
<p>如果存在：怀特调整； 变换方程的形式；重新定义变量；</p>
<p>纯异方差：</p>
<p>非纯异方差:</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006111253388.png" alt="image-20201006111253388"></p>
<p><a href="https://mlln.cn/2018/12/11/stata%E6%95%99%E7%A8%8B03-%E5%BC%82%E6%96%B9%E5%B7%AE%E7%9A%84%E6%A3%80%E9%AA%8C%E5%92%8C%E5%A4%84%E7%90%86/">https://mlln.cn/2018/12/11/stata%E6%95%99%E7%A8%8B03-%E5%BC%82%E6%96%B9%E5%B7%AE%E7%9A%84%E6%A3%80%E9%AA%8C%E5%92%8C%E5%A4%84%E7%90%86/</a></p>
<p>为了减少异方差和使结果更加稳健，我们使用“OLS+稳健标准误”的办法，通过加robust选项来求稳健标准误。使用“OLS+稳健标准误”的好处是，它对回归系数及标准误的估计都是一致的，并不需要知道条件方差函数的形式，更适用于一般的情形。</p>
<h1><span id="xu-lie-xiang-guan-xing">序列相关性</span><a href="#xu-lie-xiang-guan-xing" class="header-anchor">#</a></h1><p>什么是？</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006124927754.png" alt="image-20201006124927754"></p>
<p>检测</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006125117457.png" alt="image-20201006125117457"></p>
<p>补救措施</p>
<p>GLS: </p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006125335017.png" alt="image-20201006125335017"></p>
<h1><span id="yi-lou-bian-liang">遗漏变量</span><a href="#yi-lou-bian-liang" class="header-anchor">#</a></h1><h1><span id="xu-ni-bian-liang">虚拟变量</span><a href="#xu-ni-bian-liang" class="header-anchor">#</a></h1><h2><span id="zheng-ce-bian-liang">政策变量</span><a href="#zheng-ce-bian-liang" class="header-anchor">#</a></h2><p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006112428053.png" alt="image-20201006112428053"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006112510991.png" alt="image-20201006112510991"></p>
<h2><span id="xu-ni-bian-liang-de-yin-ru">虚拟变量的引入</span><a href="#xu-ni-bian-liang-de-yin-ru" class="header-anchor">#</a></h2><p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006111902652.png" alt="image-20201006111902652"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006112111251.png" alt="image-20201006112111251"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006112012221.png" alt="image-20201006112012221"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006112035220.png" alt="image-20201006112035220"></p>
<h1><span id="mian-ban-shu-ju">面板数据</span><a href="#mian-ban-shu-ju" class="header-anchor">#</a></h1><ol>
<li><p>不变参数模型和变截距模型</p>
<p>混合回归模型与截面固定效应（P&lt;5%,拒绝混合回归模型）</p>
<p>混合回归模型与时间固定效应模型的检测</p>
<p>不变参数模型与截面和时间的联合检测</p>
</li>
<li><p>变截距模型是固定还是随机效应模型</p>
<p>Wald统计量</p>
<p>显著：固定效应</p>
<p>截面上检验</p>
<p>时间上检验</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006131019054.png" alt="image-20201006131019054"></p>
</li>
</ol>
<p>   混合回归模型：</p>
<p>   变截距模型</p>
<p>   <img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006131522580.png" alt="image-20201006131522580"></p>
<p>   混合回归模型</p>
<p>   变截距模型</p>
<p>   ​    固定效应模型</p>
<p>   ​    随机效应模型</p>
<p>   变系数模型（不考虑</p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006132238571.png" alt="image-20201006132238571"></p>
<p><img src="/2020/09/13/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201006132615287.png" alt="image-20201006132615287"></p>
<p>OLS的假设。如果不满足相应的假设，则需要补救。</p>
<p>常用的三大问题。</p>
<p>面板数据的模型设置和检测。有哪些类别。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor">#</a></h1><ol>
<li><p>截面数据</p>
<ol>
<li>异方差</li>
</ol>
</li>
<li><p>面板数据</p>
<ol>
<li>模型设置的形式</li>
<li>检测方法</li>
</ol>
</li>
<li><p>虚拟变量</p>
<ol>
<li><p>应用</p>
<ol>
<li>双重差分模型(原理)</li>
<li>基本</li>
<li>固定的</li>
</ol>
</li>
<li><p>虚拟变量设置方法</p>
<p>n-1</p>
</li>
</ol>
</li>
<li><p>普通最小二乘法的假设</p>
<ol>
<li>三大问题的检测方法和补救措施</li>
</ol>
</li>
</ol>
<h1><span id="ke-cheng-da-gang">课程大纲</span><a href="#ke-cheng-da-gang" class="header-anchor">#</a></h1><p>01</p>
<p>回归分析概述</p>
<p>课时</p>
<p>1.1 计量经济学引论</p>
<p>1.2 回归分析的基本概念</p>
<p>1.3 回归方程的设定与估计</p>
<p>02</p>
<p>普通最小二乘法</p>
<p>课时</p>
<p>2.1 一元回归模型的OLS估计</p>
<p>2.2 多元回归模型的OLS估计</p>
<p>2.3 OLS估计量的性质</p>
<p>2.4 多元回归模型实例</p>
<p>2.5 回归模型的拟合优度</p>
<p>03</p>
<p>假设检验</p>
<p>课时</p>
<p>3.1 假设检验的基本原理</p>
<p>3.2 假设检验的方法</p>
<p>3.3 F检验及其应用</p>
<p>3.4 正态性检验</p>
<p>04</p>
<p>模型设定</p>
<p>课时</p>
<p>4.1 选择解释变量</p>
<p>4.2 选择函数形式</p>
<p>05</p>
<p>多重共线性</p>
<p>课时</p>
<p>5.1 多重共线性的定义和后果</p>
<p>5.2 多重共线性的诊断与补救</p>
<p>5.3 多重共线性的案例</p>
<p>06</p>
<p>序列相关性</p>
<p>课时</p>
<p>6.1 序列相关性的概念与类型</p>
<p>6.2 序列相关性的后果和检验</p>
<p>6.3 序列相关性的补救措施</p>
<p>07</p>
<p>异方差性</p>
<p>课时</p>
<p>7.1 异方差性的概念和后果</p>
<p>7.2 异方差性的检验</p>
<p>7.3 异方差性的补救措施</p>
<p>7.4 异方差性的案例</p>
<p>08</p>
<p>虚拟变量</p>
<p>课时</p>
<p>8.1 虚拟变量的含义</p>
<p>8.2 虚拟变量的设置与引入</p>
<p>8.3 虚拟变量的应用</p>
<p>09</p>
<p>虚拟应变量</p>
<p>课时</p>
<p>9.1 虚拟应变量的概念</p>
<p>9.2 线性概率模型</p>
<p>9.3 Logit模型和Probit模型</p>
<p>10</p>
<p>预测</p>
<p>课时</p>
<p>10.1 预测及其步骤</p>
<p>10.2 精度指标</p>
<p>10.3 时间序列模型预测</p>
<p>11</p>
<p>时间序列模型</p>
<p>课时</p>
<p>11.1 分布滞后模型及其估计</p>
<p>11.2 Granger因果关系</p>
<p>11.3 平稳性与单位根检验</p>
<p>11.4 协整与误差修正模型</p>
<p>12</p>
<p>面板数据模型</p>
<p>课时</p>
<p>12.1 认识面板数据</p>
<p>12.2 面板数据模型的设定</p>
<p>12.3 面板数据模型的参数估计</p>
<p>12.4 面板数据模型的设定检验</p>
<h1><span id="fen-zu-hui-gui-he-jiao-hu-xing-qu-bie">分组回归和交互性区别</span><a href="#fen-zu-hui-gui-he-jiao-hu-xing-qu-bie" class="header-anchor">#</a></h1><p>分组回归：控制变量系数不一样，样本量少了，估计不太好</p>
<p>稳健性检测</p>
<h1><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h1><p>截面数据</p>
<p>面板数据</p>
<p>时间序列</p>
<p>DID的特殊性（两种形式)</p>
<p>模型的形式</p>
<p>估计方法（OLS,GLS)</p>
<p>标准稳健性偏误</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>计量经济学</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-科研tool</title>
    <url>/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94tool/</url>
    <content><![CDATA[<h1><span id="hui-tu">绘图</span><a href="#hui-tu" class="header-anchor">#</a></h1><h2><span id="di-tu">地图</span><a href="#di-tu" class="header-anchor">#</a></h2><h3><span id="arcgis">ArcGis</span><a href="#arcgis" class="header-anchor">#</a></h3><p>在要素图层中，经常会出现点要素重叠的情况，对于少量数据，我们可以使用手动删除，但对于大量数据，手动删除不够现实，可万一使用ArcToolBox来批量删除。</p>
<h3><span id="1-wei-yao-su-tian-jia-xy-zuo-biao">1）为要素添加XY坐标</span><a href="#1-wei-yao-su-tian-jia-xy-zuo-biao" class="header-anchor">#</a></h3><p>打开【数据管理工具】|【要素】|【添加 XY 坐标】工具，输入要素 为“pts”；</p>
<p>之后要素属性表中增加了POINT_X，POINT_Y字段。</p>
<h3><span id="2-shan-chu-zuo-biao-xiang-tong-dian">2）删除坐标相同点</span><a href="#2-shan-chu-zuo-biao-xiang-tong-dian" class="header-anchor">#</a></h3><p>打开【数据管理工具】|【常规】|【删除相同的】工具，输入要素设置为“pts”，字 段勾选“POINT_X”和“POINT_Y”。</p>
<p>重叠的要素会只保留一个。</p>
<h1><span id="tu-pian-da-xiao">图片大小</span><a href="#tu-pian-da-xiao" class="header-anchor">#</a></h1><div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>要求</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>分辨率</td>
<td>线图</td>
<td>&gt;  1000dpi</td>
</tr>
<tr>
<td>彩/灰图</td>
<td>&gt;  300dpi</td>
<td></td>
</tr>
<tr>
<td>组合图</td>
<td>&gt;  500dpi</td>
<td></td>
</tr>
<tr>
<td>图片尺寸</td>
<td>单栏</td>
<td>8-9  cm</td>
</tr>
<tr>
<td>1.5栏</td>
<td>12-15  cm</td>
<td></td>
</tr>
<tr>
<td>双栏</td>
<td>17-19  cm</td>
<td></td>
</tr>
<tr>
<td>图片格式</td>
<td>TIFF（位图）、EPS（矢量图）</td>
<td></td>
</tr>
<tr>
<td>字体</td>
<td>Arial、Times New Roman</td>
<td></td>
</tr>
<tr>
<td>文字大小</td>
<td>推荐6-12号</td>
<td></td>
</tr>
<tr>
<td>线条粗细</td>
<td>0.2-1.5pt</td>
<td></td>
</tr>
<tr>
<td>色彩模式</td>
<td>CMYK（印刷版）、RGB（数码版）</td>
<td></td>
</tr>
<tr>
<td>文件大小</td>
<td>&lt;10M</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94tool/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201017212356513.png" alt="image-20201017212356513"></p>
<h2><span id="dan-wei">单位</span><a href="#dan-wei" class="header-anchor">#</a></h2><p>1 inch = 2.54cm</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研工具</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-科研实验记录</title>
    <url>/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>记录</p>
<a id="more"></a>
<h1><span id="2020">2020</span><a href="#2020" class="header-anchor">#</a></h1><h2><span id="11-results">11 Results</span><a href="#11-results" class="header-anchor">#</a></h2><h3><span id="1104-ba-shu-ju-zhun-bei-hao-ba-shu-ju-zhun-bei-hao-bu-yao-he-fen-xi-tong-bu">1104 把数据准备好，把数据准备好（不要和分析同步)</span><a href="#1104-ba-shu-ju-zhun-bei-hao-ba-shu-ju-zhun-bei-hao-bu-yao-he-fen-xi-tong-bu" class="header-anchor">#</a></h3><p>数据分析<br>    Step1:<br>        清洗理解数据<br>        把数据文件准备好<br>    Step2:<br>        分析<br>            这个部分如果没有什么大变动不要修改文件了</p>
<p>科研进度慢的原因：</p>
<pre><code>1. 多次尝试错误想法。主要是没有很好的理解数据。
 2. 准备文件。 一开始数据文件没有一次准备好，反反复复的修改。一定要一开始就把文件准备好，后期分析，没有什么大变动，就不会修改了。**一定是先把数据准备好，后期不改动了。**
 3. 过得纠结细节。一些样本的保留和删除一直犹豫（这个是无关紧要的啊，这里我纠结了很久）。这不是什么大问题，根本不需要额外讨论
</code></pre><p>不同数据间拼接，一定要把键值修改成统一。现在已经搞定了。</p>
<h2><span id="10-built-a-pipeline-to-handle-complex-data-at-city-level">10 built a pipeline to handle complex data at city level</span><a href="#10-built-a-pipeline-to-handle-complex-data-at-city-level" class="header-anchor">#</a></h2><h3><span id="1030-improving-efficiency">1030 improving efficiency</span><a href="#1030-improving-efficiency" class="header-anchor">#</a></h3><ol>
<li><p>data: do not think about too many details(delete)</p>
<p>我发现我就是考虑太多的details（小的，细微的地方，没什么影响的)</p>
<p>newall1 = newall1[newall1[‘All_inflow-all-GDP’]!=0]</p>
<p>newall1 = newall1[newall1[‘All_inflow-grad-GDP’]!=0]</p>
<p>newall1 = newall1[newall1[‘All_outflow-all-GDP’]!=0]</p>
<p>newall1 = newall1[newall1[‘All_outflow-grad-GDP’]!=0]</p>
<p>newall1 = newall1[newall1[‘All_inflow-all-Dis’]!=0]</p>
<p>newall1 = newall1[newall1[‘All_outflow-all-Dis’]!=0]</p>
<p>不需要考虑是否分类（不需要啊)</p>
</li>
<li><p>figure: too quick(template)</p>
</li>
</ol>
<h3><span id="1027">1027</span><a href="#1027" class="header-anchor">#</a></h3><ol>
<li><p>ArcGIS</p>
<p>attention： the value equals to 0 can be showed; the name must be modify()</p>
<p><a href="http://xzqh.mca.gov.cn/defaultQuery?shengji=%D0%C2%BD%AE%CE%AC%CE%E1%B6%FB%D7%D4%D6%CE%C7%F8(%D0%C2">官方</a>&amp;diji=-1&amp;xianji=-1)</p>
<p>the graph of Chinese map at city level(including 自治区直辖县级行政单位，城市，直辖市，地级市，（but it does not affect the visualization(blank ))</p>
</li>
<li><p>the right(correct) process of data analyse is that the first step is to prepare clean data file, and then draw figure. preparing data file and vision.</p>
</li>
<li><p>think &amp; doing</p>
</li>
</ol>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/1027Z_1.gif" style="zoom:50%;"></p>
<ol>
<li>the first step is to prepare data file. the plot(Do not doing at the same time)</li>
</ol>
<h3><span id="1025">1025</span><a href="#1025" class="header-anchor">#</a></h3><p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/1025Z_1.gif" style="zoom: 33%;"></p>
<h3><span id="1019">1019</span><a href="#1019" class="header-anchor">#</a></h3><p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/MyBlog\MyBlog\hexo\source\_posts\科研-科研实验记录\1019city1bg2.gif" style="zoom:33%;"></p>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/MyBlog\MyBlog\hexo\source\_posts\科研-科研实验记录\1019city2bg.gif" alt="1019city2bg" style="zoom:50%;"></p>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/MyBlog\MyBlog\hexo\source\_posts\科研-科研实验记录\1019city1bg.gif" alt="1019city1bg" style="zoom:50%;"></p>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/MyBlog\MyBlog\hexo\source\_posts\科研-科研实验记录\1019citybg.gif" alt="1019citybg" style="zoom:50%;"></p>
<h3><span id="1015-handling-data-set">1015 handling data set</span><a href="#1015-handling-data-set" class="header-anchor">#</a></h3><p>data preprocessing</p>
<p>cleaning the data includes missing data and outlier</p>
<h3><span id="1015-city-level-or-province-level-social-economic-data">1015 city-level or province-level social-economic data</span><a href="#1015-city-level-or-province-level-social-economic-data" class="header-anchor">#</a></h3><p>The secret of handling city-level or province-level social-economic data is one that filling and outlier. The blank data should be filled. And the outlier should be look for.</p>
<p>Attention:</p>
<ol>
<li>How to joint?  the key is city-name</li>
<li>the unit(1000(per captial ,1000,1000(total))</li>
</ol>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/1015city.gif" alt></p>
<h3><span id="1014">1014</span><a href="#1014" class="header-anchor">#</a></h3><p>I process city-level data.  The data from Urban Statistical Yearbook is too bad. Some data exists error.</p>
<p>It’s really a bad problem. I require write a demo to catch it automatically.</p>
<p>I wish I can finish it next week.</p>
<p><img src="/2020/09/11/%E7%A7%91%E7%A0%94-%E7%A7%91%E7%A0%94%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/1014city.gif" alt="1014city"></p>
<h3><span id="1013">1013</span><a href="#1013" class="header-anchor">#</a></h3><p>coding coding coding</p>
<p>think clear to do it using defining function</p>
<p>统一化，好编程</p>
<h3><span id="1012">1012</span><a href="#1012" class="header-anchor">#</a></h3><p>think about how to do next</p>
<h3><span id="1004">1004</span><a href="#1004" class="header-anchor">#</a></h3><p>coding</p>
<h3><span id="1001-1003">1001-1003</span><a href="#1001-1003" class="header-anchor">#</a></h3><ol>
<li>testing</li>
</ol>
<h2><span id="09">09</span><a href="#09" class="header-anchor">#</a></h2><h3><span id="0908">0908</span><a href="#0908" class="header-anchor">#</a></h3><ol>
<li>how to use Excel tool to analyse small-scale data set.\</li>
<li>how to select fixed effect and random effect.</li>
</ol>
<h3><span id="0920">0920</span><a href="#0920" class="header-anchor">#</a></h3><p>too bad</p>
<h3><span id="0919">0919</span><a href="#0919" class="header-anchor">#</a></h3><p>I finished process the resume data set. </p>
<h3><span id="0917">0917</span><a href="#0917" class="header-anchor">#</a></h3><p>do literature survey(population flows pattern)</p>
<p>think about how to explain and show the mechanism framework</p>
<p>review codes</p>
<h3><span id="0915">0915</span><a href="#0915" class="header-anchor">#</a></h3><p>handling missing socioeconomic data at city level</p>
<h3><span id="0914-duo-yuan-shu-ju">0914 多源数据</span><a href="#0914-duo-yuan-shu-ju" class="header-anchor">#</a></h3><p>行政区单位（肯定不会错)以此为基础拼接</p>
<p>city-level缺失和填充</p>
<p>hsr data</p>
<p>resumedata 分类汇总</p>
<h3><span id="0912-pao-shu-ju">0912 跑数据</span><a href="#0912-pao-shu-ju" class="header-anchor">#</a></h3><p>分治算法。要把问题拆解一个个小步骤。再合并再一起。把小问题解决了，不然改起来很麻烦啊。</p>
<p>尽快出结果吧！感觉要重新写报告了。结果不一定好。照理说，处理数据花不了多少时间啊。关键是没有现成的已经可以用的数据。还有每年数据给的不一致，有些年有，有的没有。</p>
<h3><span id="0911-shi-yan-kuang-jia">0911 实验框架</span><a href="#0911-shi-yan-kuang-jia" class="header-anchor">#</a></h3><p>重新规划了论文思路。界定了问题，我要解决什么问题。</p>
<p>下一步要把代码和数据检查一下。</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>科研-文献阅读</title>
    <url>/2020/09/11/%E7%A7%91%E7%A0%94-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="5bb9030adfee3d4002596e187009146b3f2c3844d23e0ea595b30e3804878594">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化</title>
    <url>/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/</url>
    <content><![CDATA[<p>此篇将详细给出如何利用origin绘制常用的图，包括单变量，双变量，以及多变量相关的图。同时，也要讲解如何配色，字体设置，子图设置等等技巧。</p>
<a id="more"></a>
<p>[TOC]</p>
<h1><span id="ke-shi-hua">可视化</span><a href="#ke-shi-hua" class="header-anchor">#</a></h1><h2><span id="ji-ben-yao-su">基本要素</span><a href="#ji-ben-yao-su" class="header-anchor">#</a></h2><p><img src="https://picb.zhimg.com/v2-57401ff1fa2474eebebbdb4cd88730d1_r.jpg" alt="img"></p>
<p>线条：标记符，线性（实线，双划线，虚线，点），粗细，颜色</p>
<h2><span id="lei-xing">类型</span><a href="#lei-xing" class="header-anchor">#</a></h2><p>数据间的关系大致可以分为：构成；比较；趋势；分布；联系。</p>
<p><img src="https://pic3.zhimg.com/v2-1464719076c4c95e903e67d54ea60e98_r.jpg" alt="img"></p>
<p>数据间的流向：和弦图和桑基图</p>
<p><a href="https://zhuanlan.zhihu.com/p/27746129">https://zhuanlan.zhihu.com/p/27746129</a></p>
<p><a href="https://echarts.apache.org/examples/zh/index.html#chart-type-graphGL">https://echarts.apache.org/examples/zh/index.html#chart-type-graphGL</a></p>
<h1><span id="origin">Origin</span><a href="#origin" class="header-anchor">#</a></h1><h2><span id="origin-hui-zhi-bian-ji-zhi-fang-tu">Origin绘制边际直方图</span><a href="#origin-hui-zhi-bian-ji-zhi-fang-tu" class="header-anchor">#</a></h2><p>plot-statistics-边际直方图</p>
<h2><span id="origin-hui-zhi-xiang-guan-xing-tu">Origin绘制相关性图</span><a href="#origin-hui-zhi-xiang-guan-xing-tu" class="header-anchor">#</a></h2><p>Step1: 绘制散点图</p>
<p>Step2：analysis 增加线性回归</p>
<p>可以得到回归曲线，回归系数和相关性系数</p>
<p>当然也可以通过计算，直接添加到图里面</p>
<p>注意：回归分析+图形是通过多部完成的。先绘制你需要的图形，在加入统计分析，以及一些计算数据。</p>
<h2><span id="origin-zhi-zuo-xiang-guan-xi-shu-ju-zhen-tu">Origin制作相关系数矩阵图</span><a href="#origin-zhi-zuo-xiang-guan-xi-shu-ju-zhen-tu" class="header-anchor">#</a></h2><h1><span id="python">Python</span><a href="#python" class="header-anchor">#</a></h1><h2><span id="jian-jie-python-de-hui-tu-ku">简介Python的绘图库</span><a href="#jian-jie-python-de-hui-tu-ku" class="header-anchor">#</a></h2><h4><span id="linestyle">linestyle</span><a href="#linestyle" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">'-'       solid line style  </span><br><span class="line">'--'      dashed line style  </span><br><span class="line">'-.'      dash-dot line style  </span><br><span class="line">':'       dotted line style  </span><br></pre></td></tr></tbody></table></figure>
<h4><span id="marker">marker</span><a href="#marker" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">'.'       point marker  </span><br><span class="line">','       pixel marker  </span><br><span class="line">'o'       circle marker  </span><br><span class="line">'v'       triangle_down marker  </span><br><span class="line">'^'       triangle_up marker  </span><br><span class="line">'&lt;'       triangle_left marker  </span><br><span class="line">'&gt;'       triangle_right marker  </span><br><span class="line">'1'       tri_down marker  </span><br><span class="line">'2'       tri_up marker  </span><br><span class="line">'3'       tri_left marker  </span><br><span class="line">'4'       tri_right marker  </span><br><span class="line">'s'       square marker  </span><br><span class="line">'p'       pentagon marker  </span><br><span class="line">'*'       star marker  </span><br><span class="line">'h'       hexagon1 marker  </span><br><span class="line">'H'       hexagon2 marker  </span><br><span class="line">'+'       plus marker  </span><br><span class="line">'x'       x marker  </span><br><span class="line">'D'       diamond marker  </span><br><span class="line">'d'       thin_diamond marker  </span><br><span class="line">'|'       vline marker  </span><br><span class="line">'_'       hline marker  </span><br></pre></td></tr></tbody></table></figure>
<h2><span id="yao-su-cao-zuo">要素操作</span><a href="#yao-su-cao-zuo" class="header-anchor">#</a></h2><p><img src="https://img-blog.csdn.net/20180514122113530?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3><span id="xian-tiao-lei-xing-yan-se-he-san-dian-lei-xing">线条类型，颜色和散点类型</span><a href="#xian-tiao-lei-xing-yan-se-he-san-dian-lei-xing" class="header-anchor">#</a></h3><h3><span id="tu-li">图例</span><a href="#tu-li" class="header-anchor">#</a></h3><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">font1 = {'family' : 'Times New Roman',</span><br><span class="line">'weight' : 'normal',</span><br><span class="line">'size'   : 23,</span><br><span class="line">}</span><br><span class="line">A,=plt.plot(x1,y1,'-r',label='A',linewidth=5.0,ms=10)</span><br><span class="line">legend = plt.legend(handles=[A,B],prop=font1)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="zuo-biao-zhou-ke-du">坐标轴刻度</span><a href="#zuo-biao-zhou-ke-du" class="header-anchor">#</a></h3><p>方法一：利用matplotlib库</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">plt.rc("legend", fontsize=15)</span><br><span class="line">plt.xlabel('Longitude', fontsize=15)</span><br><span class="line">plt.ylabel('Latitude', fontsize=15)</span><br><span class="line">plt.title('Spatial Location Plot', fontsize=15 )</span><br><span class="line">plt.tick_params(axis="both", labelsize=15)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="zuo-biao-zhou-biao-qian">坐标轴标签</span><a href="#zuo-biao-zhou-biao-qian" class="header-anchor">#</a></h3><h2><span id="seaborn">Seaborn</span><a href="#seaborn" class="header-anchor">#</a></h2><h2><span id="ju-zhen-tu">矩阵图</span><a href="#ju-zhen-tu" class="header-anchor">#</a></h2><h4><span id="v1">v1</span><a href="#v1" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy import stats</span><br><span class="line">import seaborn as sns</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def corrdot(*args, **kwargs):</span><br><span class="line">    corr_r = args[0].corr(args[1], 'pearson')</span><br><span class="line">    corr_text = round(corr_r, 2)</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    font_size = abs(corr_r) * 80 + 5</span><br><span class="line">    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",</span><br><span class="line">                ha='center', va='center', fontsize=font_size)</span><br><span class="line"></span><br><span class="line">def corrfunc(x, y, **kws):</span><br><span class="line">    r, p = stats.pearsonr(x, y)</span><br><span class="line">    p_stars = ''</span><br><span class="line">    if p &lt;= 0.05:</span><br><span class="line">        p_stars = '*'</span><br><span class="line">    if p &lt;= 0.01:</span><br><span class="line">        p_stars = '**'</span><br><span class="line">    if p &lt;= 0.001:</span><br><span class="line">        p_stars = '***'</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    ax.annotate(p_stars, xy=(0.65, 0.6), xycoords=ax.transAxes,</span><br><span class="line">                color='red', fontsize=70)</span><br><span class="line"></span><br><span class="line">sns.set(style='white', font_scale=1.6)</span><br><span class="line">iris = sns.load_dataset('iris')</span><br><span class="line">g = sns.PairGrid(iris, aspect=1.5, diag_sharey=False, despine=False)</span><br><span class="line">g.map_lower(sns.regplot, lowess=True, ci=False,</span><br><span class="line">            line_kws={'color': 'red', 'lw': 1},</span><br><span class="line">            scatter_kws={'color': 'black', 's': 20})</span><br><span class="line">g.map_diag(sns.distplot, color='black',</span><br><span class="line">           kde_kws={'color': 'red', 'cut': 0.7, 'lw': 1},</span><br><span class="line">           hist_kws={'histtype': 'bar', 'lw': 2,</span><br><span class="line">                     'edgecolor': 'k', 'facecolor':'grey'})</span><br><span class="line">g.map_diag(sns.rugplot, color='black')</span><br><span class="line">g.map_upper(corrdot)</span><br><span class="line">g.map_upper(corrfunc)</span><br><span class="line">g.fig.subplots_adjust(wspace=0, hspace=0)</span><br><span class="line"></span><br><span class="line"># Remove axis labels</span><br><span class="line">for ax in g.axes.flatten():</span><br><span class="line">    ax.set_ylabel('')</span><br><span class="line">    ax.set_xlabel('')</span><br><span class="line"></span><br><span class="line"># Add titles to the diagonal axes/subplots</span><br><span class="line">for ax, col in zip(np.diag(g.axes), iris.columns):</span><br><span class="line">    ax.set_title(col, y=0.82, fontsize=26)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.stack.imgur.com/F1LYt.png" alt="enter image description here"></p>
<h4><span id="zi-ding-yi">自定义</span><a href="#zi-ding-yi" class="header-anchor">#</a></h4><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line">def corrdot(*args, **kwargs):</span><br><span class="line">    corr_r = args[0].corr(args[1], 'pearson')</span><br><span class="line">    corr_text = f"{corr_r:2.2f}".replace("0.", ".")</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    ax.set_axis_off()</span><br><span class="line">    marker_size = abs(corr_r) * 10000</span><br><span class="line">    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",</span><br><span class="line">               vmin=-1, vmax=1, transform=ax.transAxes)</span><br><span class="line">    font_size = abs(corr_r) * 40 + 5</span><br><span class="line">    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",</span><br><span class="line">                ha='center', va='center', fontsize=font_size)</span><br><span class="line"></span><br><span class="line">sns.set(style='white', font_scale=1.6)</span><br><span class="line">iris = sns.load_dataset('iris')</span><br><span class="line">g = sns.PairGrid(iris, aspect=1.4, diag_sharey=False)</span><br><span class="line">g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})</span><br><span class="line">g.map_diag(sns.distplot, kde_kws={'color': 'black'})</span><br><span class="line">g.map_upper(corrdot)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://qph.fs.quoracdn.net/main-qimg-36b54c8efc0221302924e558384bdfdd" alt="img"></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from scipy import stats</span><br><span class="line">i = 0</span><br><span class="line">j =0 </span><br><span class="line">def corrfunc(x, y, **kws):</span><br><span class="line">    # global j</span><br><span class="line">    # j = j+1</span><br><span class="line">    # print(j)</span><br><span class="line">    r, p = stats.pearsonr(x, y)</span><br><span class="line">    p_stars = ''</span><br><span class="line">    if p &lt;= 0.05:</span><br><span class="line">        p_stars = '*'</span><br><span class="line">    if p &lt;= 0.01:</span><br><span class="line">        p_stars = '**'</span><br><span class="line">    if p &lt;= 0.001:</span><br><span class="line">        p_stars = '***'</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    if i%3==0:</span><br><span class="line">        ax.annotate(p_stars, xy=(0.2, 0.2), xycoords=ax.transAxes,</span><br><span class="line">                color='red', fontsize=20)</span><br><span class="line">    if i%3==1:</span><br><span class="line">        ax.annotate(p_stars, xy=(0.65, 0.6), xycoords=ax.transAxes,</span><br><span class="line">                color='red', fontsize=40)</span><br><span class="line"></span><br><span class="line">def corrdot(*args, **kwargs):</span><br><span class="line">    global i</span><br><span class="line">    # print(args[0])</span><br><span class="line">    corr_r = args[0].corr(args[1], 'pearson')</span><br><span class="line">    # print('r',corr_r)</span><br><span class="line">    </span><br><span class="line">    corr_text = f"{corr_r:2.2f}"</span><br><span class="line">    #.replace("0.", ".")</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    # ax.set_axis_off()</span><br><span class="line">    marker_size = abs(corr_r) * 1000</span><br><span class="line">    if i%3==0:</span><br><span class="line">        ax.scatter([.2], [.2], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",</span><br><span class="line">               vmin=-1, vmax=1, transform=ax.transAxes)</span><br><span class="line">        font_size = abs(corr_r) * 20 + 5</span><br><span class="line">        ax.annotate(corr_text, [.2, .2,],  xycoords="axes fraction",</span><br><span class="line">                ha='center', va='center', fontsize=font_size)</span><br><span class="line">    if i%3==1:</span><br><span class="line">        ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",</span><br><span class="line">               vmin=-1, vmax=1, transform=ax.transAxes)</span><br><span class="line">        font_size = abs(corr_r) * 20 + 5</span><br><span class="line">        ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",</span><br><span class="line">                ha='center', va='center', fontsize=font_size)</span><br><span class="line"></span><br><span class="line">    i = i+1</span><br><span class="line">    # font_size = abs(corr_r) * 40 + 5</span><br><span class="line">    # ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",</span><br><span class="line">    #             ha='center', va='center', fontsize=font_size)</span><br><span class="line"></span><br><span class="line">sns.set(style='white', font_scale=1.6)</span><br><span class="line">iris = sns.load_dataset('iris')</span><br><span class="line">g = sns.PairGrid(iris,hue="species", aspect=1.4, diag_sharey=False)</span><br><span class="line">g.map_lower(sns.regplot)</span><br><span class="line"># , lowess=True, ci=False, line_kws={'color': 'black'}</span><br><span class="line">g.map_diag(sns.distplot, kde_kws={'color': 'black'})</span><br><span class="line">g.map_upper(corrdot)</span><br><span class="line">g.map_upper(corrfunc)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/MyBlog\MyBlog\hexo\source\_posts\可视化之origin\Figure_1.svg" alt></p>
<h2><span id="can-kao">参考</span><a href="#can-kao" class="header-anchor">#</a></h2><p><a href="https://www.coder.work/article/383225">https://www.coder.work/article/383225</a></p>
<h1><span id="r">R</span><a href="#r" class="header-anchor">#</a></h1><h2><span id="r-hui-zhi-xiang-guan-xi-shu-ju-zhen">R绘制相关系数矩阵</span><a href="#r-hui-zhi-xiang-guan-xi-shu-ju-zhen" class="header-anchor">#</a></h2><p><code>PerformanceAnalytics</code>r包的相关矩阵：<br><a href="https://www.rdocumentation.org/packages/PerformanceAnalytics/versions/1.4.3541/topics/chart.Correlation"><code>chart.Correlation</code> function</a></p>
<p><img src="https://i.stack.imgur.com/WTQV2.png" alt="PerformanceAnalytics chart.Correlation result"></p>
<p>Pairwise scatter plot matrix, histogram and correlation coefficients of all related variables for the microinverters connected with different brands of PV module. Pairwise scatter plots are in lower triangle boxes, histograms are in the diagonal boxes, and upper triangle boxes give the correlation coefficients between variables. Brand, Ambient. T, Wind speed, Module. T, Power and Micro. T denote PV module brand, ambient temperature, 5 point moving average wind speed, PV module temperature, AC power output and microinverter temperature respectively.</p>
<p><a href="https://ask.hellobi.com/blog/R_shequ/18431">https://ask.hellobi.com/blog/R_shequ/18431</a></p>
<h2><span id="r-hui-zhi-sang-ji-tu">R绘制桑基图</span><a href="#r-hui-zhi-sang-ji-tu" class="header-anchor">#</a></h2><h1><span id="arcgis">Arcgis</span><a href="#arcgis" class="header-anchor">#</a></h1><p>感觉Arcgis 是专门针对地理信息绘图的，明显使用起来方便很多。</p>
<h2><span id="arcgis-hui-zhi-di-tu">Arcgis绘制地图</span><a href="#arcgis-hui-zhi-di-tu" class="header-anchor">#</a></h2><h2><span id="arcgis-hui-zhi-jing-xiang-tu">Arcgis绘制径向图</span><a href="#arcgis-hui-zhi-jing-xiang-tu" class="header-anchor">#</a></h2><h2><span id="global-mapper">Global Mapper</span><a href="#global-mapper" class="header-anchor">#</a></h2><h1><span id="ai">AI</span><a href="#ai" class="header-anchor">#</a></h1><p>origin对eps编辑会出问题。最好是对wmf文件进行修改。</p>
<h2><span id="tu-pian-yan-se-xiu-gai">图片颜色修改</span><a href="#tu-pian-yan-se-xiu-gai" class="header-anchor">#</a></h2><p><a href="https://www.bilibili.com/video/BV1D4411P7s2?p=9">https://www.bilibili.com/video/BV1D4411P7s2?p=9</a></p>
<p>编辑-&gt;编辑颜色-&gt;重新着色图稿。在此界面，可以建立自己的颜色面板，可以调整颜色，调节饱和度等等。</p>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/image-20200909143450645.png" alt="image-20200909143450645" style="zoom: 50%;"></p>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/image-20200909143444890.png" alt="image-20200909143444890" style="zoom:50%;"></p>
<h2><span id="tu-xing-xiu-gai">图形修改</span><a href="#tu-xing-xiu-gai" class="header-anchor">#</a></h2><p>图片填色和描边设置：shift选择区域内的内容</p>
<h3><span id="fang-suo">放缩</span><a href="#fang-suo" class="header-anchor">#</a></h3><p>alt+鼠标</p>
<p>放缩shift等比例</p>
<h1><span id="echart">Echart</span><a href="#echart" class="header-anchor">#</a></h1><p><a href="https://echarts.apache.org/examples/zh/index.html#chart-type-map">https://echarts.apache.org/examples/zh/index.html#chart-type-map</a></p>
<h2><span id="shang-shou-di-yi-ge">上手第一个</span><a href="#shang-shou-di-yi-ge" class="header-anchor">#</a></h2><figure class="highlight js"><table><tbody><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=<span class="string">"utf-8"</span>&gt;</span><br><span class="line">    &lt;title&gt;ECharts&lt;/title&gt;</span><br><span class="line">    &lt;!-- 引入 echarts.js --&gt;</span><br><span class="line">    &lt;script src=<span class="string">"echarts.min.js"</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">    &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt;</span><br><span class="line">    &lt;div id=<span class="string">"main"</span> style=<span class="string">"width: 600px;height:400px;"</span>&gt;&lt;/div&gt;</span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">        <span class="comment">// 基于准备好的dom，初始化echarts实例</span></span><br><span class="line">        <span class="keyword">var</span> myChart = echarts.init(<span class="built_in">document</span>.getElementById(<span class="string">'main'</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定图表的配置项和数据</span></span><br><span class="line">        <span class="keyword">var</span> option = {</span><br><span class="line">            title: {</span><br><span class="line">                text: <span class="string">'ECharts 入门示例'</span></span><br><span class="line">            },</span><br><span class="line">            tooltip: {},</span><br><span class="line">            legend: {</span><br><span class="line">                data:[<span class="string">'销量'</span>]</span><br><span class="line">            },</span><br><span class="line">            xAxis: {</span><br><span class="line">                data: [<span class="string">"衬衫"</span>,<span class="string">"羊毛衫"</span>,<span class="string">"雪纺衫"</span>,<span class="string">"裤子"</span>,<span class="string">"高跟鞋"</span>,<span class="string">"袜子"</span>]</span><br><span class="line">            },</span><br><span class="line">            yAxis: {},</span><br><span class="line">            series: [{</span><br><span class="line">                name: <span class="string">'销量'</span>,</span><br><span class="line">                type: <span class="string">'bar'</span>,</span><br><span class="line">                data: [<span class="number">5</span>, <span class="number">20</span>, <span class="number">36</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">            }]</span><br><span class="line">        };</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用刚指定的配置项和数据显示图表。</span></span><br><span class="line">        myChart.setOption(option);</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="pei-zhi">配置</span><a href="#pei-zhi" class="header-anchor">#</a></h3><p>需要配置什么找到需要的标签，然后设置成对应的值就好了！</p>
<h1><span id="stata">Stata</span><a href="#stata" class="header-anchor">#</a></h1><h1><span id="matlab">Matlab</span><a href="#matlab" class="header-anchor">#</a></h1><p><a href="https://ww2.mathworks.cn/help/matlab/creating_plots/types-of-matlab-plots.html">https://ww2.mathworks.cn/help/matlab/creating_plots/types-of-matlab-plots.html</a></p>
<h1><span id="delphi">Delphi</span><a href="#delphi" class="header-anchor">#</a></h1><h1><span id="liu-cheng-tu">流程图</span><a href="#liu-cheng-tu" class="header-anchor">#</a></h1><h2><span id="visio">Visio</span><a href="#visio" class="header-anchor">#</a></h2><h2><span id="yi-tu">亿图</span><a href="#yi-tu" class="header-anchor">#</a></h2><h1><span id="origin-lab-2020-amp-python">Origin lab 2020 &amp; python</span><a href="#origin-lab-2020-amp-python" class="header-anchor">#</a></h1><h1><span id="se-cai-da-pei">色彩搭配</span><a href="#se-cai-da-pei" class="header-anchor">#</a></h1><h2><span id="se-xiang-liang-du-bao-he-du">色相，亮度，饱和度</span><a href="#se-xiang-liang-du-bao-he-du" class="header-anchor">#</a></h2><p>从可视化编码的角度对颜色进行分析，可以将颜色分为色相、亮度和饱和度三个视觉通道。</p>
<p><strong>色相</strong> 即色彩的相貌和特征。自然界中色彩的种类很多，色相指色彩的种类和名称。如；红、橙、黄、绿、青、蓝、紫等等颜色的种类变化就叫色相。</p>
<p><strong>明度</strong> 指色彩的亮度。颜色有深浅、明暗的变化。比如，深黄、中黄、淡黄、柠檬黄等黄色在明度上就不一样，紫红、深红、玫瑰红、大红、朱红、桔红等红颜色在亮度上也不尽相同。这些颜色在明暗、深浅上的不同变化，也就是色彩的又一重要特征一一明度变化。</p>
<p><strong>饱和度</strong> 色彩的鲜艳程度，饱和度越高,表现越鲜明,饱和度较低,表现则较黯淡。</p>
<p><a href="https://vis.baidu.com/chartcolor/color/">https://vis.baidu.com/chartcolor/color/</a></p>
<p><a href="https://antv.vision/zh/docs/specification/principles/visual">https://antv.vision/zh/docs/specification/principles/visual</a></p>
<p><a href="https://flourish.studio/2019/03/18/pokemon-go/">https://flourish.studio/2019/03/18/pokemon-go/</a></p>
<p><a href="https://www.webdesignrankings.com/resources/lolcolors/">https://www.webdesignrankings.com/resources/lolcolors/</a></p>
<p><a href="https://nipponcolors.com/#sodenkaracha">https://nipponcolors.com/#sodenkaracha</a></p>
<h1><span id="zi-ti-he-zi-hao-chu-li">字体和字号处理</span><a href="#zi-ti-he-zi-hao-chu-li" class="header-anchor">#</a></h1><h1><span id="ai-xuan-ran-hou-qi-chu-li">AI渲染——后期处理</span><a href="#ai-xuan-ran-hou-qi-chu-li" class="header-anchor">#</a></h1><ol>
<li>可以自备一些搭配较好的颜色方案。</li>
</ol>
<h2><span id="sci">SCI</span><a href="#sci" class="header-anchor">#</a></h2><h3><span id="ban-mian">版面</span><a href="#ban-mian" class="header-anchor">#</a></h3><p>一般占半个版面宽度，是8cm。高度不限但不能太高。</p>
<p>2/3版图</p>
<p>一般占2/3个版面宽度，是12-15cm，高度不限，但不能太高。</p>
<p>全版图</p>
<p>总宽度是17cm，占两栏，高度不限，但不能太高。</p>
<p>A4 21cm * 29.7cm</p>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201017151830388.png" alt="image-20201017151830388"></p>
<p><img src="https://pic1.zhimg.com/d7fc70f4a60ffc10c495167b494e72d4_r.jpg" alt="preview"></p>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201017154840184.png" alt="image-20201017154840184"></p>
<p><img src="/2020/09/04/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Borigin/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20201017160310517.png" alt="image-20201017160310517"></p>
<p><a href="http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWtablefigs.html">http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWtablefigs.html</a></p>
<p>字号的对应关系 复制图形时，会根据Preferences-Options菜单下Page选项卡中Ratio进行缩放，在默认设置下，Ratio为40%，则字号的对应关系就是： 复制到Word中的字号 = Origin中的字号 <em> Ratio 简单的说，Origin中设置字号30，复制到Word就是30</em>40%=12磅。线宽同理计算。 导出图形时，默认比例为100%，则相应的字号为30磅。  重要提示！重要提示：以上字号对应关系成立的重要前提是： ①Scale Factor = 1，如图5所示。（否则在Origin中字形的大小会随着图层大小而变化，但字号显示却不会变） ②复制到Word之后不要缩放图形！！！但是有时候图形尺寸（Page或Layer的尺寸）过大，Word会自动适应宽度，等于是压缩了图形大小，这个也一定要注意！！ Tips：我的Ratio习惯设为50%，是由于旧版本Origin字号不支持10.5号的输入，所以输入21复制到Word中就是10.5，但Origin2017及之后的版本可以直接输入10.5号字。</p>
]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>绘图</tag>
        <tag>origin</tag>
      </tags>
  </entry>
  <entry>
    <title>生活攻略</title>
    <url>/2020/08/28/%E7%94%9F%E6%B4%BB%E6%94%BB%E7%95%A5/</url>
    <content><![CDATA[<p>生活小技巧</p>
<a id="more"></a>
<h2><span id="2020">2020</span><a href="#2020" class="header-anchor">#</a></h2><h2><span id="202027-tu-pian-shu-xing">202027 图片属性</span><a href="#202027-tu-pian-shu-xing" class="header-anchor">#</a></h2><p>像素点是最小的图像单元，一张图片由好多的像素点组成。像素是指由图像的小方格组成的，这些小方块都有一个明确的位置和被分配的色彩数值，小方格颜色和位置就决定该图像所呈现出来的样子。</p>
<p>一个像素点对应一个颜色(RGB,)</p>
<p>图像分辨率是指每英寸图像内的像素点数</p>
<p>像素点：就是一个小方格（不可再分)</p>
<h2><span id="202010-zi-yuan-guan-li-xin-de">202010 资源管理心得</span><a href="#202010-zi-yuan-guan-li-xin-de" class="header-anchor">#</a></h2><ol>
<li>命名。把杂乱资料同一放在某个文件夹里面</li>
<li>每周抽出1-2h资源汇总和整合</li>
<li>形成共享意识。</li>
</ol>
<h2><span id="202008">202008</span><a href="#202008" class="header-anchor">#</a></h2><p>为什么你的转化率那么低呢？</p>
<h2><span id="202008-dian-nao-wen-jian-guan-li">202008 电脑文件管理</span><a href="#202008-dian-nao-wen-jian-guan-li" class="header-anchor">#</a></h2><p>第一次就要把文件位置确定。</p>
<p>主要是梳理电脑文件管理的技巧，方便以后提高工作效率。</p>
<ol>
<li><p>命名方法。采用3W1X的理念，when-work-who-X(备注)，“3W”分别指：When时间——20200630；Work事项——销售部营销计划；Who主体——客户A。</p>
<p>“1X”是指：X备注——第三版。</p>
</li>
<li><p>同一文件，多个修改版本。文件名前面加上修改人，在最后加上版本号。</p>
</li>
<li><p>同一任务（主题）文件夹管理。<strong>同级最多7个文件夹，最多5层。</strong></p>
</li>
</ol>
<p>及时复盘很关键。</p>
<ol>
<li><p>每星期要整理一次文件夹。</p>
<ol>
<li>重要文件要同步。</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>生活攻略</category>
      </categories>
      <tags>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析之SQL</title>
    <url>/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BSQL/</url>
    <content><![CDATA[<h1><span id="sql">SQL</span><a href="#sql" class="header-anchor">#</a></h1><p><img src="/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BSQL/MyBlog/MyBlog/hexo/source/_posts/数据分析技能/数据分析——SQL.png" alt></p>
<a id="more"></a>
<h2><span id="xue-hui-zhi-jie-ba-bie-ren-de-zhi-shi-zhuang-jin-zi-ji-de-nao-zi-li">学会直接把别人的知识装进自己的脑子里</span><a href="#xue-hui-zhi-jie-ba-bie-ren-de-zhi-shi-zhuang-jin-zi-ji-de-nao-zi-li" class="header-anchor">#</a></h2><p><a href="https://www.zhihu.com/xen/market/personal-works-all/houziliaorenwu?zh_hide_tab_bar=true">https://www.zhihu.com/xen/market/personal-works-all/houziliaorenwu?zh_hide_tab_bar=true</a></p>
<h2><span id="leetcode-amp-nowcoder">LeetCode &amp; NowCoder</span><a href="#leetcode-amp-nowcoder" class="header-anchor">#</a></h2><p><a href="https://leetcode.com/problemset/database/">https://leetcode.com/problemset/database/</a></p>
<p><a href="https://www.nowcoder.com/ta/sql">https://www.nowcoder.com/ta/sql</a></p>
<h2><span id="guan-xi-xing-shu-ju-ku">关系型数据库</span><a href="#guan-xi-xing-shu-ju-ku" class="header-anchor">#</a></h2><h3><span id="mysql-shu-ju-lei-xing">MySQL数据类型</span><a href="#mysql-shu-ju-lei-xing" class="header-anchor">#</a></h3><p>主要提供了三种类型，分别是文本，数字和日期。</p>
<h2><span id="mysql-shu-ju-lei-xing">MySQL 数据类型</span><a href="#mysql-shu-ju-lei-xing" class="header-anchor">#</a></h2><p>在 MySQL 中，有三种主要的类型：文本、数字和日期/时间类型。</p>
<h3><span id="text-lei-xing">Text 类型：</span><a href="#text-lei-xing" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">CHAR(size)</td>
<td style="text-align:left">保存固定长度的字符串（可包含字母、数字以及特殊字符）。在括号中指定字符串的长度。最多 255 个字符。</td>
</tr>
<tr>
<td style="text-align:left">VARCHAR(size)</td>
<td style="text-align:left">保存可变长度的字符串（可包含字母、数字以及特殊字符）。在括号中指定字符串的最大长度。最多 255 个字符。注释：如果值的长度大于 255，则被转换为 TEXT 类型。</td>
</tr>
<tr>
<td style="text-align:left">TINYTEXT</td>
<td style="text-align:left">存放最大长度为 255 个字符的字符串。</td>
</tr>
<tr>
<td style="text-align:left">TEXT</td>
<td style="text-align:left">存放最大长度为 65,535 个字符的字符串。</td>
</tr>
<tr>
<td style="text-align:left">BLOB</td>
<td style="text-align:left">用于 BLOBs (Binary Large OBjects)。存放最多 65,535 字节的数据。</td>
</tr>
<tr>
<td style="text-align:left">MEDIUMTEXT</td>
<td style="text-align:left">存放最大长度为 16,777,215 个字符的字符串。</td>
</tr>
<tr>
<td style="text-align:left">MEDIUMBLOB</td>
<td style="text-align:left">用于 BLOBs (Binary Large OBjects)。存放最多 16,777,215 字节的数据。</td>
</tr>
<tr>
<td style="text-align:left">LONGTEXT</td>
<td style="text-align:left">存放最大长度为 4,294,967,295 个字符的字符串。</td>
</tr>
<tr>
<td style="text-align:left">LONGBLOB</td>
<td style="text-align:left">用于 BLOBs (Binary Large OBjects)。存放最多 4,294,967,295 字节的数据。</td>
</tr>
<tr>
<td style="text-align:left">ENUM(x,y,z,etc.)</td>
<td style="text-align:left">允许你输入可能值的列表。可以在 ENUM 列表中列出最大 65535 个值。如果列表中不存在插入的值，则插入空值。注释：这些值是按照你输入的顺序存储的。可以按照此格式输入可能的值：ENUM(‘X’,’Y’,’Z’)</td>
</tr>
<tr>
<td style="text-align:left">SET</td>
<td style="text-align:left">与 ENUM 类似，SET 最多只能包含 64 个列表项，不过 SET 可存储一个以上的值。</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="number-lei-xing">Number 类型：</span><a href="#number-lei-xing" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">TINYINT(size)</td>
<td style="text-align:left">-128 到 127 常规。0 到 255 无符号*。在括号中规定最大位数。</td>
</tr>
<tr>
<td style="text-align:left">SMALLINT(size)</td>
<td style="text-align:left">-32768 到 32767 常规。0 到 65535 无符号*。在括号中规定最大位数。</td>
</tr>
<tr>
<td style="text-align:left">MEDIUMINT(size)</td>
<td style="text-align:left">-8388608 到 8388607 普通。0 to 16777215 无符号*。在括号中规定最大位数。</td>
</tr>
<tr>
<td style="text-align:left">INT(size)</td>
<td style="text-align:left">-2147483648 到 2147483647 常规。0 到 4294967295 无符号*。在括号中规定最大位数。</td>
</tr>
<tr>
<td style="text-align:left">BIGINT(size)</td>
<td style="text-align:left">-9223372036854775808 到 9223372036854775807 常规。0 到 18446744073709551615 无符号*。在括号中规定最大位数。</td>
</tr>
<tr>
<td style="text-align:left">FLOAT(size,d)</td>
<td style="text-align:left">带有浮动小数点的小数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。</td>
</tr>
<tr>
<td style="text-align:left">DOUBLE(size,d)</td>
<td style="text-align:left">带有浮动小数点的大数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。</td>
</tr>
<tr>
<td style="text-align:left">DECIMAL(size,d)</td>
<td style="text-align:left">作为字符串存储的 DOUBLE 类型，允许固定的小数点。</td>
</tr>
</tbody>
</table>
</div>
<p>* 这些整数类型拥有额外的选项 UNSIGNED。通常，整数可以是负数或正数。如果添加 UNSIGNED 属性，那么范围将从 0 开始，而不是某个负数。</p>
<h3><span id="date-lei-xing">Date 类型：</span><a href="#date-lei-xing" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">DATE()</td>
<td style="text-align:left">日期。格式：YYYY-MM-DD注释：支持的范围是从 ‘1000-01-01’ 到 ‘9999-12-31’</td>
</tr>
<tr>
<td style="text-align:left">DATETIME()</td>
<td style="text-align:left">*日期和时间的组合。格式：YYYY-MM-DD HH:MM:SS注释：支持的范围是从 ‘1000-01-01 00:00:00’ 到 ‘9999-12-31 23:59:59’</td>
</tr>
<tr>
<td style="text-align:left">TIMESTAMP()</td>
<td style="text-align:left">*时间戳。TIMESTAMP 值使用 Unix 纪元(‘1970-01-01 00:00:00’ UTC) 至今的描述来存储。格式：YYYY-MM-DD HH:MM:SS注释：支持的范围是从 ‘1970-01-01 00:00:01’ UTC 到 ‘2038-01-09 03:14:07’ UTC</td>
</tr>
<tr>
<td style="text-align:left">TIME()</td>
<td style="text-align:left">时间。格式：HH:MM:SS 注释：支持的范围是从 ‘-838:59:59’ 到 ‘838:59:59’</td>
</tr>
<tr>
<td style="text-align:left">YEAR()</td>
<td style="text-align:left">2 位或 4 位格式的年。注释：4 位格式所允许的值：1901 到 2155。2 位格式所允许的值：70 到 69，表示从 1970 到 2069。</td>
</tr>
</tbody>
</table>
</div>
<p>* 即便 DATETIME 和 TIMESTAMP 返回相同的格式，它们的工作方式很不同。在 INSERT 或 UPDATE 查询中，TIMESTAMP 自动把自身设置为当前的日期和时间。TIMESTAMP 也接受不同的格式，比如 YYYYMMDDHHMMSS、YYMMDDHHMMSS、YYYYMMDD 或 YYMMDD。</p>
<h2><span id="day-ox00-ru-men">Day Ox00 入门：</span><a href="#day-ox00-ru-men" class="header-anchor">#</a></h2><p><a href="https://www.w3school.com.cn/sql/sql_syntax.asp">https://www.w3school.com.cn/sql/sql_syntax.asp</a></p>
<p>通过SQL使得数据操作员有能力查询，修改数据库。</p>
<h3><span id="what">What</span><a href="#what" class="header-anchor">#</a></h3><p>SQL：结构化查询语言。SQL对大小写不敏感。</p>
<p>SQL语句后面的分号？有些数据库系统要求每条SQL命令的末端使用分号，来表示语句的结束。</p>
<p>分类：</p>
<ol>
<li><p>数据操作语言（DML)</p>
<p>这部分包括查询和更新指令。</p>
<ul>
<li>SELECT - 从数据库表中获取数据</li>
<li>UPDATE - 更新数据库表中的数据</li>
<li>DELETE - 从数据库表中删除数据</li>
<li>INSERT INTO - 向数据库表中插入数据</li>
</ul>
</li>
<li><p>数据定义语言 (DDL)</p>
<p>这部分包括创建和删除表格，还有定义索引(键)，规定表之间的链接，和表间的约束。</p>
<p>SQL 中最重要的 DDL 语句:</p>
<p>​        CREATE DATABASE - 创建新数据库</p>
<p>​        ALTER DATABASE - 修改数据库</p>
<p>​        CREATE TABLE - 创建新表</p>
<p>​        ALTER TABLE - 变更（改变）数据库表</p>
<p>​        DROP TABLE - 删除表</p>
<p>​        CREATE INDEX - 创建索引（搜索键）</p>
<p>​        DROP INDEX - 删除索引</p>
</li>
</ol>
<h2><span id="day-ox00-jian-dan-cha-xun">Day Ox00 简单查询：</span><a href="#day-ox00-jian-dan-cha-xun" class="header-anchor">#</a></h2><h3><span id="select-cha-xun-yu-ju">SELECT : 查询语句</span><a href="#select-cha-xun-yu-ju" class="header-anchor">#</a></h3><p>不带条件查询语句</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">SELECT 列名称1, 列名称2 FROM 表名称</span><br><span class="line">SELECT * FROM 表名称</span><br><span class="line">SELECT DISTINCT 列名称 FROM 表名称 //去除重复值</span><br></pre></td></tr></tbody></table></figure>
<p>有条件查询语句</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">SELECT 列名称 FROM 表名称 WHERE 列 运算符 值</span><br></pre></td></tr></tbody></table></figure>
<p>WHERE 过滤单条记录</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">操作符	描述</span><br><span class="line">=	等于</span><br><span class="line">&lt;&gt;	不等于</span><br><span class="line">&gt;	大于</span><br><span class="line">&lt;	小于</span><br><span class="line">&gt;=	大于等于</span><br><span class="line">&lt;=	小于等于</span><br><span class="line">BETWEEN	在某个范围内</span><br><span class="line">LIKE	搜索某种模式</span><br><span class="line">注释：在某些版本的 SQL 中，操作符 &lt;&gt; 可以写为 !=。</span><br></pre></td></tr></tbody></table></figure>
<p>WHERE 过滤两个以上的条件记录</p>
<p> AND / OR</p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析之业务分析</title>
    <url>/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%9A%E5%8A%A1%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>业务能力</p>
<a id="more"></a>
<h1><span id="ye-wu-neng-li">业务能力</span><a href="#ye-wu-neng-li" class="header-anchor">#</a></h1><h2><span id="ru-he-li-jie-ye-wu-shu-ju">如何理解业务数据</span><a href="#ru-he-li-jie-ye-wu-shu-ju" class="header-anchor">#</a></h2><ol>
<li>每列的含义</li>
<li>数据分析<ol>
<li>用户数据<ol>
<li>主要是指用户的基本信息（性别，年龄)</li>
</ol>
</li>
<li>行为数据<ol>
<li>指用户做了什么（停留时间)</li>
<li>做了什么，时长，点击次数等等</li>
</ol>
</li>
<li>产品数据<ol>
<li>产品名称，类别等等</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2><span id="chang-jian-de-ye-wu-zhi-biao">常见的业务指标</span><a href="#chang-jian-de-ye-wu-zhi-biao" class="header-anchor">#</a></h2><ol>
<li><p>指标的含义？维度的含义？如果你不能衡量就不能增长</p>
</li>
<li><p>用户的相关的指标</p>
<ol>
<li><p>活跃用户</p>
<ol>
<li>定义：时间：日DAU(Daily Active User)；周WAU(Weekly Active User)；月MAU(Monthly Active User)；</li>
<li>动作：打开；支付；</li>
<li>活跃率：活跃用户/总人数</li>
<li>去重用户(统计，至少)</li>
<li>是否与用户保持粘性</li>
</ol>
</li>
<li><p>留存</p>
<ol>
<li><p>定义：如关注公众号；取消关注；</p>
</li>
<li><p>相反：流失</p>
</li>
<li><p>第一天新增用户，在第N天使用过产业的用户数/第一天新增用户数</p>
<p>次日留存率；第N日留存率。</p>
</li>
<li><p>不同时期的流失情况，找流失原因</p>
</li>
</ol>
</li>
<li><p>新增</p>
<ol>
<li>日新增用户</li>
<li>通过不同渠道的分析，可以判断不同渠道的推广效果</li>
</ol>
</li>
</ol>
</li>
<li><p>行为数据</p>
<ol>
<li><p>PV：访问次数 Page View；页面浏览的字数；看了几个页面</p>
<p>看用户的喜好</p>
</li>
<li><p>UV: 访问人数 Unique Visitor；</p>
</li>
</ol>
</li>
</ol>
<ol>
<li><p>转发率</p>
<ol>
<li>转化率：转发某功能的用户数/看见该功能的用户数</li>
</ol>
</li>
<li><p>转化率</p>
<ol>
<li>如店铺转化率</li>
<li>广告转化率</li>
</ol>
</li>
<li><p>K因子</p>
<ol>
<li>衡量推荐效果</li>
<li>=平均某个用户向多少人发出邀请/接受到邀请的人转化为新用户的转化率</li>
</ol>
</li>
</ol>
<ol>
<li><p>产品指标</p>
<ol>
<li>总量：成交额；成交量；访问时长</li>
<li>人均：人均付费；付费用户；人均访问时长</li>
<li>付费：付费率（付费人数）；复购率（消费两次以上的人数)；</li>
<li>产品：热销；好评；差评产；</li>
</ol>
</li>
</ol>
<h2><span id="ru-he-xuan-ze-zhi-biao">如何选择指标</span><a href="#ru-he-xuan-ze-zhi-biao" class="header-anchor">#</a></h2><ol>
<li>好的指标是比率</li>
<li>北极星指标：核心指标</li>
</ol>
<p><img src="/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%9A%E5%8A%A1%E5%88%86%E6%9E%90/MyBlog/MyBlog/hexo/source/_posts/数据分析技能/image-20200904160525186.png" alt="image-20200904160525186" style="zoom:25%;"></p>
<hr>
<p><img src="/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%9A%E5%8A%A1%E5%88%86%E6%9E%90/MyBlog/MyBlog/hexo/source/_posts/数据分析技能/image-20200904160536010.png" alt="image-20200904160536010" style="zoom: 25%;"></p>
<h1><span id="shu-ju-fen-xi">数据分析</span><a href="#shu-ju-fen-xi" class="header-anchor">#</a></h1><h2><span id="si-wei">思维</span><a href="#si-wei" class="header-anchor">#</a></h2><h3><span id="jie-gou-hua-si-wei">结构化思维</span><a href="#jie-gou-hua-si-wei" class="header-anchor">#</a></h3><p>金字塔原理：</p>
<pre><code>   1. 核心论点
              1. 假设
              2. 问题
              3. 原因
              4. 预测
            2. 结构拆解
                     1. 
         3. MECE
                4. 验证
</code></pre><p>层层拆解</p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析技能</title>
    <url>/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD/</url>
    <content><![CDATA[<p><img src="/2020/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD/数据分析的职业规划.png" alt="数据分析的职业规划"></p>
<p><a href="https://ask.hellobi.com/blog/qinlu/sitemap/">https://ask.hellobi.com/blog/qinlu/sitemap/</a></p>
<h2><span id>#</span><a href="#" class="header-anchor">#</a></h2>]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析技能</tag>
      </tags>
  </entry>
  <entry>
    <title>研二の学习</title>
    <url>/2020/08/15/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1><span id="li-yong-ting-li-cai-liao-xue-ying-yu-zui-gao-xiao">利用听力材料学英语最高效</span><a href="#li-yong-ting-li-cai-liao-xue-ying-yu-zui-gao-xiao" class="header-anchor">#</a></h1><p><strong>有效输出应该是稍高于现有水平的、更准确、更得体的表达输出</strong></p>
<p>先记住以下两点，下面我们再一一解析应该要怎么做到。</p>
<p><strong>1.</strong> <strong>学习需要反馈，来告诉我们这输出是正确的，可以继续；或者是错误的，需要修正。</strong></p>
<p><strong>2.</strong> <strong>将输入内化，变成自己的知识。</strong></p>
<a id="more"></a>
<p>我一般会听<strong>四遍</strong>，但不是全听原文。</p>
<p><strong>第一遍：泛听原文，不要看字幕或脚本，清楚录音的内容。</strong>听第一遍的时候我通常连笔记都不做，目的是为了让自己流畅的听完，对听力的内容有一个整体的掌握。</p>
<p><strong>第二遍</strong>：<strong>先听原文，根据内容段落，开始复述内容，并录音</strong>。这一步的目的是强迫自己调用已经学过的知识，组织语言和进行练习。</p>
<p><strong>第三遍，听自己的录音，然后做出修正。</strong>这一步很重要，可以让你了解自己的发音问题和语法问题，并把可听出来的语法问题进行修改。通过这一步，我们就可以把简单重复输入的语言材料，转化为有效输出。</p>
<p><strong>第四遍，听原文看字幕和脚本，看把听不懂的地方标注</strong>，说明为什么听不懂（比如是因为自己发音不准导致的听不出，或者就是因为这个词没背过、不熟悉）。不熟悉的用法和自己用错的地方总结，背下来，下次试着用。</p>
<p>Tips：</p>
<ol>
<li><p>不要选择太难的材料，太难的材料容易使自己丧失学习兴趣。</p>
</li>
<li><p>一开始，不要选择太长的听力材料。10分钟左右最佳。在这里推荐ted，可以选择有字幕或关闭字幕。</p>
</li>
<li><p>在一个相近的时间段内，选择相近题材的材料。比如我会在两个星期内选择“心理”题材的录音。这样我就会有更大的几率用上刚学过的结构和词汇。</p>
</li>
<li><p>及时总结，及时复习已背过的材料，复习的重要性大家都懂，这里就不多说了。</p>
</li>
</ol>
<h1><span id="zhen-ti-ting-xie">真题听写</span><a href="#zhen-ti-ting-xie" class="header-anchor">#</a></h1><ol>
<li><p>材料选择</p>
<p>能够听得懂 70%的材料</p>
</li>
</ol>
<p>   2 具体执行方法</p>
<ol>
<li><p>先泛听一篇</p>
</li>
<li><p>再循环听几遍</p>
</li>
<li><p>再逐句逐句的听</p>
</li>
</ol>
<h2><span id="mei-ju-jing-ting">美剧精听</span><a href="#mei-ju-jing-ting" class="header-anchor">#</a></h2><ol>
<li>先看中文听</li>
<li>英文，查</li>
<li>听找</li>
<li>台词，跟读</li>
<li>重复三四<br>至少10</li>
</ol>
]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习之模型性能</title>
    <url>/2020/08/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/</url>
    <content><![CDATA[<p><img src="/2020/08/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/模型性能.png" alt="模型性能"></p>
<h1><span id="ji-qi-xue-xi-mo-xing-de-xing-neng-ping-jie">机器学习模型的性能评价</span><a href="#ji-qi-xue-xi-mo-xing-de-xing-neng-ping-jie" class="header-anchor">#</a></h1><h2><span id="xun-lian-wu-chai-he-fan-hua-wu-chai">训练误差和泛化误差</span><a href="#xun-lian-wu-chai-he-fan-hua-wu-chai" class="header-anchor">#</a></h2><p>训练误差即经验误差；学习器在新样本上的误差称为泛化误差。</p>
<a id="more"></a>
<h2><span id="mo-xing-ping-gu-de-fang-fa">模型评估的方法</span><a href="#mo-xing-ping-gu-de-fang-fa" class="header-anchor">#</a></h2><p>留出法</p>
<p>交叉验证法</p>
<h2><span id="xing-neng-du-liang">性能度量</span><a href="#xing-neng-du-liang" class="header-anchor">#</a></h2><h3><span id="yu-ce">预测</span><a href="#yu-ce" class="header-anchor">#</a></h3><h3><span id="fen-lei">分类</span><a href="#fen-lei" class="header-anchor">#</a></h3><h3><span id="zhun-que-du-he-cuo-wu-lu">准确度和错误率</span><a href="#zhun-que-du-he-cuo-wu-lu" class="header-anchor">#</a></h3><p><strong>准确率</strong>：指的是分类正确的样本数量占样本总数的比例</p>
<p><strong>错误率</strong>：指分类错误的样本占样本总数的比例</p>
<h2><span id="zhun-que-lu-he-zhao-hui-lu">准确率和召回率</span><a href="#zhun-que-lu-he-zhao-hui-lu" class="header-anchor">#</a></h2><p>精确率，也被称作查准率，是指<strong>所有预测为正类的结果中，真正的正类的比例</strong>。</p>
<p>召回率，也被称作查全率，是指所有正类中，被分类器找出来的比例。</p>
<h2><span id="guo-ni-he">过拟合</span><a href="#guo-ni-he" class="header-anchor">#</a></h2><p>学习器在未知数据上的表现差</p>
<h3><span id="yuan-yin">原因</span><a href="#yuan-yin" class="header-anchor">#</a></h3><ol>
<li>数据上<ol>
<li>建模样本。样本数量少；</li>
<li>噪音干扰大</li>
</ol>
</li>
<li>模型<ol>
<li>参数过得，模型太复杂</li>
</ol>
</li>
</ol>
<h3><span id="jie-jue-fang-an">解决方案</span><a href="#jie-jue-fang-an" class="header-anchor">#</a></h3><ol>
<li>交叉验证</li>
<li>添加正则项<ol>
<li>范数；</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>交叉验证</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>Book-Learning</title>
    <url>/2020/07/29/Book-Learning/</url>
    <content><![CDATA[<p>分享看书笔记</p>
<a id="more"></a>
<h1><span id="shu-ju-wa-jue-gai-nian-yu-ji-zhu">数据挖掘——概念与技术</span><a href="#shu-ju-wa-jue-gai-nian-yu-ji-zhu" class="header-anchor">#</a></h1><h2><span id="shu-ju-wa-jue-de-gai-nian">数据挖掘的概念</span><a href="#shu-ju-wa-jue-de-gai-nian" class="header-anchor">#</a></h2><p>数据挖掘是从大量数据中提取或挖掘知识。强调：从大量的，未加工的材料中发现少量金块这一过程。</p>
<p>基本过程：</p>
<ol>
<li>数据清理。(消除噪音或者不一致的数据)</li>
<li>数据集成（多种数据集成可以组合在一起）</li>
<li>数据选择（从数据库中提取与分析任务相关的数据)</li>
<li>数据变换。(数据变换或统一成适合挖掘形式)</li>
<li>数据挖掘(使用智能方法提取数据)</li>
<li>模式评估</li>
<li>知识表示（可视化等手段，展示挖掘的知识)</li>
</ol>
<h2><span id="shu-ju-yu-chu-li">数据预处理</span><a href="#shu-ju-yu-chu-li" class="header-anchor">#</a></h2><p>问题：现实中数据极易受噪音数据、遗漏数据和不一致性数据的干扰。</p>
<p>不完整（属性值缺失）</p>
<p>噪音(错误属性值，离群值)</p>
<p>不一致性（编码)</p>
<ol>
<li>数据清理。包括填写遗漏值，平滑噪音数据，识别、删除局外者，并解决不一致来“清理数据”。脏数据转化为干净数据。</li>
<li>数据集成。不同数据源的数据集成。</li>
</ol>
<p><img src="/2020/07/29/Book-Learning/image-20200729213701724.png" alt="image-20200729213701724"></p>
<h3><span id="shu-ju-qing-xi">数据清洗</span><a href="#shu-ju-qing-xi" class="header-anchor">#</a></h3><h4><span id="yi-lou-zhi">遗漏值</span><a href="#yi-lou-zhi" class="header-anchor">#</a></h4><ol>
<li>忽略</li>
<li>人工填写</li>
<li>全局变量</li>
<li>平均值</li>
<li>分组填充</li>
</ol>
<h4><span id="zao-yin-shu-ju">噪音数据</span><a href="#zao-yin-shu-ju" class="header-anchor">#</a></h4><p>噪音是指测量变量的随机误差或偏差。平滑数据。</p>
<ol>
<li>分箱。存储值被分布到一些箱子中，然后局部平滑。按平均值平滑。按中值平滑。按边界平滑。</li>
<li><p>聚类。</p>
</li>
<li><p>回归</p>
</li>
</ol>
<h4><span id="bu-yi-zhi-shu-ju">不一致数据</span><a href="#bu-yi-zhi-shu-ju" class="header-anchor">#</a></h4><h3><span id="shu-ju-ji-cheng">数据集成</span><a href="#shu-ju-ji-cheng" class="header-anchor">#</a></h3><p>实体识别。</p>
<p>冗余。</p>
<h3><span id="shu-ju-bian-huan">数据变换</span><a href="#shu-ju-bian-huan" class="header-anchor">#</a></h3><p>将数据转换成适用于挖掘的形式</p>
<p>平滑。</p>
<p>聚集。对数据进行汇总和聚集。日、月和年销售额。多粒度数据分析构造数据方</p>
<p>数据泛化。street-&gt;city; age—&gt;young, middle-age,senior</p>
<p>规范化。 属性数据按比例缩放，缩放到特定区间。</p>
<p>属性构造。</p>
<p>最小-最大规范化</p>
<script type="math/tex; mode=display">
v = \frac{v-min}{max-min}(new_max-new_min)+new_min</script><p>z-score规范化</p>
<script type="math/tex; mode=display">
v = \frac{v-mean}{\delta}</script><h3><span id="shu-ju-gui-yue">数据归约</span><a href="#shu-ju-gui-yue" class="header-anchor">#</a></h3><p>数据方聚集</p>
<p>维归约</p>
<p>数据压缩</p>
<p>数值压缩</p>
<h1><span id="gai-nian-miao-shu-te-zheng-yu-bi-jiao">概念描述：特征与比较</span><a href="#gai-nian-miao-shu-te-zheng-yu-bi-jiao" class="header-anchor">#</a></h1><p>数据挖掘分为两类: 描述式和预测式挖掘。描述式提供数据的有趣的一般性质。建立一个或一组模型，并试图预测新数据集的行为。</p>
<p>描述式数据挖掘称为概念描述。不同的粒度和角度描述数据集。</p>
<p>度量可以根据其所用的聚集函数分成三类：</p>
<p>分布的：一个聚集函数是分布的，如果它能以如下分布方式进行计算：设数据被划分为 n 个集合，函数在每一部分上的计算得到一个聚集值。如果将函数用于 n 个聚集值得到的结果，与将函数<br>用于所有数据得到的结果一样，则该函数可以用分布方式计算。  </p>
<p>代数的：ave(); </p>
<p>整体的：rank(),median()</p>
<h3><span id="miao-shu-xing-tong-ji-du-liang">描述性统计度量</span><a href="#miao-shu-xing-tong-ji-du-liang" class="header-anchor">#</a></h3><h4><span id="du-liang-zhong-xin-qu-shi">度量中心趋势</span><a href="#du-liang-zhong-xin-qu-shi" class="header-anchor">#</a></h4><p>平均值</p>
<p>加权算术平均（加权平均)</p>
<p>中位数</p>
<p>分位数</p>
<h1><span id="wa-jue-da-xing-shu-ju-ku-zhong-de-guan-lian-gui-ze">挖掘大型数据库中的关联规则</span><a href="#wa-jue-da-xing-shu-ju-ku-zhong-de-guan-lian-gui-ze" class="header-anchor">#</a></h1><p>关联规则挖掘发现大量数据中项集的关联或者相关联系。可用于于分类设计，交叉购物和贱卖分析。</p>
<p>购买计算机也趋向于同时购买财务管理软件的关联规则</p>
<script type="math/tex; mode=display">
computer => finanical_management_software\\
[support = 2\%, confidence = 60\%]</script><p>支持度：有用性。同时购买计算机和财务管理软件。</p>
<p>置信度：确定性。购买计算机的顾客有多少购买财务管理软件。</p>
<p>可最小支持度阙值和最小置信度阙值。</p>
<h3><span id="apriori-suan-fa-shi-yong-hou-xuan-xiang-ji-zhao-pin-fan-xiang-ji">Apriori 算法： 使用候选项集找频繁项集</span><a href="#apriori-suan-fa-shi-yong-hou-xuan-xiang-ji-zhao-pin-fan-xiang-ji" class="header-anchor">#</a></h3><h1><span id="fen-lei-he-yu-ce">分类和预测</span><a href="#fen-lei-he-yu-ce" class="header-anchor">#</a></h1><p>分类和预测是数据分析的两种形式，可以用于提取描述重要数据类的模型或预测未来的数据趋势。 分类预测分类标号（类），而预测建立连续值函数模型。  </p>
<p>预测的准确率、计算速度、鲁棒性、可规模性和可解释性是评估分类和预测方法的五条标准。  </p>
<h1><span id="ju-lei-fen-xi">聚类分析</span><a href="#ju-lei-fen-xi" class="header-anchor">#</a></h1><p>数据对象的集合进行分析，但与分类不同的是，聚类分析 (clustering) 属于非监督学习，也就是不知道要划分类是未知的。聚类分析就是要将数据对象分组成为多个类或簇(cluster)。每一个簇中的对象之间具有较高的相似度，而不同簇对象差别较大。常常采用距离作为相异度的衡量。</p>
]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title>向着永恒出发</title>
    <url>/2020/07/28/prepare-for-work/</url>
    <content><![CDATA[<p>编程语言的学习</p>
<a id="more"></a>
<h1><span id> </span><a href="#" class="header-anchor">#</a></h1><h1><span id="di-qi-zhou-he-ba-zhou-ji-qi-xue-xi-suan-fa-he-shu-ju-wa-jue">第七周和八周：机器学习算法和数据挖掘</span><a href="#di-qi-zhou-he-ba-zhou-ji-qi-xue-xi-suan-fa-he-shu-ju-wa-jue" class="header-anchor">#</a></h1>]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析之Python</title>
    <url>/2020/07/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPython/</url>
    <content><![CDATA[<p>Python</p>
<a id="more"></a>
<h1><span id="di-yi-zhou-python-ji-chu-zhi-shi">第一周：Python基础知识</span><a href="#di-yi-zhou-python-ji-chu-zhi-shi" class="header-anchor">#</a></h1><p><a href="https://github.com/jackfrued/Python-100-Days/tree/master/Day01-15">https://github.com/jackfrued/Python-100-Days/tree/master/Day01-15</a></p>
<h2><span id="day-ox00">Day Ox00</span><a href="#day-ox00" class="header-anchor">#</a></h2><ol>
<li>基本的数据结构类型，以及提供的常用方法。</li>
<li>编码的风格和规范性</li>
</ol>
<h3><span id="chu-shi-python">初识python</span><a href="#chu-shi-python" class="header-anchor">#</a></h3><ul>
<li>Python简介 - Python的历史 / Python的优缺点 / Python的应用领域</li>
<li>搭建编程环境 - Windows环境 / Linux环境 / MacOS环境</li>
<li>从终端运行Python程序 - Hello, world / print函数 / 运行程序</li>
<li>使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE</li>
<li>注释 - 注释的作用 / 单行注释 / 多行注释</li>
</ul>
<p>代码注释风格</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def func(arg1, arg2):</span><br><span class="line">    """在这里写函数的一句话总结(如: 计算平均值).</span><br><span class="line"></span><br><span class="line">    这里是具体描述.</span><br><span class="line"></span><br><span class="line">    参数</span><br><span class="line">    ----------</span><br><span class="line">    arg1 : int</span><br><span class="line">        arg1的具体描述</span><br><span class="line">    arg2 : int</span><br><span class="line">        arg2的具体描述</span><br><span class="line"></span><br><span class="line">    返回值</span><br><span class="line">    -------</span><br><span class="line">    int</span><br><span class="line">        返回值的具体描述</span><br></pre></td></tr></tbody></table></figure>
<p>变量的定义</p>
<p>模块尽量使用小写命名，首字母保持小写，尽量不要用下划线(除非多个单词，且数量不多的情况)</p>
<p>类名使用驼峰(CamelCase)命名风格，首字母大写，私有类可用一个下划线开头</p>
<p>函数名一律小写，如有多个单词，用下划线隔开</p>
<p>变量名尽量小写, 如有多个单词，用下划线隔开</p>
<p>常量采用全大写，如有多个单词，使用下划线隔开</p>
<h3><span id="yu-yan-yuan-su">语言元素</span><a href="#yu-yan-yuan-su" class="header-anchor">#</a></h3><ul>
<li>程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制</li>
<li>变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换</li>
<li>数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码</li>
<li>运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级</li>
</ul>
<h3><span id="fen-zhi-jie-gou">分支结构</span><a href="#fen-zhi-jie-gou" class="header-anchor">#</a></h3><ul>
<li>分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图</li>
<li>if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if</li>
</ul>
<h3><span id="xun-huan-jie-gou">循环结构</span><a href="#xun-huan-jie-gou" class="header-anchor">#</a></h3><ul>
<li>循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图</li>
<li>while循环 - 基本结构 / break语句 / continue语句</li>
<li>for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序</li>
</ul>
<h3><span id="han-shu-he-mo-kuai-de-shi-yong">函数和模块的使用</span><a href="#han-shu-he-mo-kuai-de-shi-yong" class="header-anchor">#</a></h3><ul>
<li><p>函数的作用 - 代码的坏味道 / 用函数封装功能模块</p>
</li>
<li><p>定义函数 - def语句 / 函数名 / 参数列表 / return语句 / 调用自定义函数</p>
</li>
<li><p>调用函数 - Python内置函数 / 导入模块和函数</p>
</li>
<li><p>函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数</p>
</li>
<li><p>函数的返回值 - 没有返回值 / 返回单个值 / 返回多个值</p>
</li>
<li><p>作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字</p>
</li>
<li><p>用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）</p>
</li>
<li><p>Lambda表达式</p>
</li>
</ul>
<h3><span id="zi-fu-chuan-he-chang-yong-de-shu-ju-jie-gou">字符串和常用的数据结构</span><a href="#zi-fu-chuan-he-chang-yong-de-shu-ju-jie-gou" class="header-anchor">#</a></h3><p>这一句非常重要，python提供的数据结构，和内置的方法很实用。</p>
<ul>
<li>字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法</li>
<li>列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历</li>
</ul>
<p>像 <code>insert</code> ，<code>remove</code> 或者 <code>sort</code> 方法，只修改列表，没有打印出返回值——它们<strong>返回默认值 <code>None</code></strong> 。这是Python中所有可变数据结构的设计原则。</p>
<ul>
<li><p>列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找</p>
</li>
<li><p>生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器</p>
</li>
<li><p>元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换</p>
</li>
<li><p>集合基本用法 - 集合和列表的区别 / 创建集合 / 添加元素 / 删除元素 / 清空</p>
</li>
<li><p>集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集</p>
</li>
<li><p>字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空</p>
</li>
<li><p>字典常用操作 - keys()方法 / values()方法 / items()方法 / setdefault()方法</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">list(d1.keys())</span><br><span class="line">in not in</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
<p>python list 常用方法总结</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">1. 创建</span><br><span class="line">	1.1 l = [1, 2]</span><br><span class="line">	1.2 list = []</span><br><span class="line">2. 添加</span><br><span class="line">	2.1 末尾增加一个 append</span><br><span class="line">	2.2 指定位置增加一个 insert</span><br><span class="line">	2.3 增加list extend +</span><br><span class="line">3. 查</span><br><span class="line">	[start: stop: step]</span><br><span class="line">4. 改</span><br><span class="line">5. 删</span><br><span class="line">	1. pop</span><br><span class="line">	2. remove(元素)</span><br><span class="line">	3. del 列表</span><br><span class="line">6. 排序和反转</span><br><span class="line">	reverse(); sort(); sort(reverse = True)</span><br><span class="line">7. 属性</span><br><span class="line">	len</span><br><span class="line">	max/min</span><br><span class="line">8. 复制</span><br></pre></td></tr></tbody></table></figure>
<p>python string </p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">1. 检测 str 是否包含在 mystr中，如果是，返回开始的索引值；否则返回-1。也可以指定在一定的范围内。</span><br><span class="line"></span><br><span class="line">mystr.find(str, start=0, end=len(mystr))</span><br><span class="line"></span><br><span class="line">2. 跟find()方法一样，只不过如果str不在 mystr中会报一个异常.</span><br><span class="line"></span><br><span class="line">mystr.index(str, start=0, end=len(mystr))</span><br><span class="line"></span><br><span class="line">3. 把 mystr 中的 str1 替换成 str2,如果 count 指定，则替换不超过 count 次.</span><br><span class="line"></span><br><span class="line">mystr.replace(str1, str2,  mystr.count(str1))</span><br><span class="line"></span><br><span class="line">4. 以 str 为分隔符切片 mystr，如果 maxsplit有指定值，则仅分隔 maxsplit 个子字符串</span><br><span class="line"></span><br><span class="line">mystr.split(str=" ", 2)</span><br><span class="line"></span><br><span class="line">5. 删除 mystr 左边的空白字符</span><br><span class="line"></span><br><span class="line">mystr.lstrip()</span><br><span class="line">6. 删除 mystr 字符串末尾的空白字符</span><br><span class="line"></span><br><span class="line">mystr.rstrip()</span><br><span class="line">7. 删除mystr字符串两端的空白字符</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="day-ox01">Day Ox01</span><a href="#day-ox01" class="header-anchor">#</a></h2><h3><span id="ding-yi-lei">定义类</span><a href="#ding-yi-lei" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">classname</span>(<span class="params">object</span>):</span></span><br></pre></td></tr></tbody></table></figure>
<pre><code>    __init__是创建对象时进行初始化操作
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">    def __init__(self, name, age):</span><br><span class="line">    	self.name = name</span><br><span class="line">    	self.age = age</span><br><span class="line"></span><br><span class="line">创建实列</span><br><span class="line">std = classname('xiemay','24')</span><br><span class="line"></span><br><span class="line">私有和公开的属性和函数：用__开头</span><br><span class="line"></span><br><span class="line">@property 装饰器</span><br><span class="line">如果想访问属性可以通过属性的getter（访问器）和setter（修改器）方法进行对应的操作。</span><br><span class="line">    # 访问器 - getter方法</span><br><span class="line">​    @property</span><br><span class="line">​    def age(self):</span><br><span class="line">​        return self._age</span><br><span class="line">静态方法和类方法</span><br><span class="line">    # 修改器 - setter方法</span><br><span class="line">​    @age.setter</span><br><span class="line">​    def age(self, age):</span><br><span class="line">​        self._age = age</span><br><span class="line">​    @staticmethod</span><br><span class="line">​    def is_valid(a, b, c):</span><br><span class="line">​        return a + b &gt; c and b + c &gt; a and a + c &gt; b</span><br><span class="line"></span><br><span class="line">## 继承和多态</span><br><span class="line"></span><br><span class="line">class person():</span><br><span class="line">    </span><br><span class="line">class woman(person):</span><br><span class="line">    super.__init__()</span><br><span class="line"></span><br><span class="line">## 文件操作</span><br><span class="line"></span><br><span class="line">文件操作基本上没什么问题了</span><br><span class="line"></span><br><span class="line">| 操作模式 | 具体含义                                                     |</span><br><span class="line">| -------- | ------------------------------------------------------------ |</span><br><span class="line">| a+       | 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 |</span><br><span class="line">| w+       | 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 |</span><br><span class="line">| `'r'`    | 读取 （默认）                                                |</span><br><span class="line">| `'w'`    | 写入（会先截断之前的内容）                                   |</span><br><span class="line">| `'x'`    | 写入，如果文件已经存在会产生异常                             |</span><br><span class="line">| `'a'`    | 追加，将内容写入到已有文件的末尾                             |</span><br><span class="line">| `'b'`    | 二进制模式                                                   |</span><br><span class="line">| `'t'`    | 文本模式（默认）                                             |</span><br><span class="line">| `'+'`    | 更新（既可以读又可以写）                                     |</span><br><span class="line">| r+       | 打开一个文件用于读写。文件指针将会放在文件的开头。           |</span><br><span class="line"></span><br><span class="line">![img](https://github.com/jackfrued/Python-100-Days/raw/master/Day01-15/res/file-open-mode.png)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
</code></pre><p>with open(filename, moder, encoding) as f1:<br>    for line in f1:<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 读取方法</span><br><span class="line"></span><br><span class="line">#### 按字节</span><br><span class="line"></span><br><span class="line">fileObject.read([count]);</span><br><span class="line">  在这里，被传递的参数是要从已打开文件中读取的字节计数。该方法从文件的开头开始读入，如果没有传入count，它会尝试尽可能多地读取更多的内容，很可能是直到文件的末尾。</span><br><span class="line"></span><br><span class="line">#### 单独一行</span><br><span class="line"></span><br><span class="line">readline()方法</span><br><span class="line">f.readline() 会从文件中读取单独的一行。换行符为 '\n'。f.readline() 如果返回一个空字符串, 说明已经已经读取到最后一行。</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>with open() as f1:<br>    while True:<br>        lines = f.readline()<br>        if lines:<br><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 全部行</span><br><span class="line"></span><br><span class="line">readlines()方法</span><br><span class="line">  f.readlines() 将以列表的形式返回该文件中包含的所有行，列表中的一项表示文件的一行。如果设置可选参数 sizehint, 则读取指定长度的字节, 并且将这些字节按行分割。</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>with open() as f1:<br>    lines = f1.readlines()<br><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### json 文件读取</span><br><span class="line"></span><br><span class="line">json模块主要有四个比较重要的函数，分别是：</span><br><span class="line"></span><br><span class="line">dump - 将Python对象按照JSON格式序列化到文件中</span><br><span class="line">dumps - 将Python对象处理成JSON格式的字符串</span><br><span class="line">	infor = json.dumps(neww)</span><br><span class="line">	f.write(infor+'\n')</span><br><span class="line">load - 将文件中的JSON数据反序列化成对象</span><br><span class="line">loads - 将字符串的内容反序列化成Python对象</span><br><span class="line">	b = json.loads(lines)</span><br><span class="line"></span><br><span class="line">#### excel</span><br><span class="line"></span><br><span class="line">单独操作excel的库</span><br><span class="line"></span><br><span class="line">创建/打开(对象)，定位(sheet)，定位行列, 保存</span><br><span class="line"></span><br><span class="line">就是这几个方法</span><br><span class="line"></span><br><span class="line">##### xlrd</span><br><span class="line"></span><br><span class="line">### 导入 xlrd 库</span><br><span class="line"></span><br><span class="line">import xlrd</span><br><span class="line"></span><br><span class="line">### 打开刚才我们写入的 test_w.xls 文件</span><br><span class="line"></span><br><span class="line">wb = xlrd.open_workbook("test_w.xls")</span><br><span class="line"></span><br><span class="line">### 获取并打印 sheet 数量</span><br><span class="line"></span><br><span class="line">print( "sheet 数量:", wb.nsheets)</span><br><span class="line"></span><br><span class="line">### 获取并打印 sheet 名称</span><br><span class="line"></span><br><span class="line">print( "sheet 名称:", wb.sheet_names())</span><br><span class="line"></span><br><span class="line">### 根据 sheet 索引获取内容</span><br><span class="line"></span><br><span class="line">sh1 = wb.sheet_by_index(0)</span><br><span class="line"></span><br><span class="line">### 或者</span><br><span class="line"></span><br><span class="line">### 也可根据 sheet 名称获取内容</span><br><span class="line"></span><br><span class="line">### sh = wb.sheet_by_name('成绩')</span><br><span class="line"></span><br><span class="line">### 获取并打印该 sheet 行数和列数</span><br><span class="line"></span><br><span class="line">print( u"sheet %s 共 %d 行 %d 列" % (sh1.name, sh1.nrows, sh1.ncols))</span><br><span class="line"></span><br><span class="line">### 获取并打印某个单元格的值</span><br><span class="line"></span><br><span class="line">print( "第一行第二列的值为:", sh1.cell_value(0, 1))</span><br><span class="line"></span><br><span class="line">### 获取整行或整列的值</span><br><span class="line"></span><br><span class="line">rows = sh1.row_values(0) # 获取第一行内容</span><br><span class="line">cols = sh1.col_values(1) # 获取第二列内容</span><br><span class="line"></span><br><span class="line">### 打印获取的行列值</span><br><span class="line"></span><br><span class="line">print( "第一行的值为:", rows)</span><br><span class="line">print( "第二列的值为:", cols)</span><br><span class="line"></span><br><span class="line">### 获取单元格内容的数据类型</span><br><span class="line"></span><br><span class="line">print( "第二行第一列的值类型为:", sh1.cell(1, 0).ctype)</span><br><span class="line"></span><br><span class="line">### 遍历所有表单内容</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>for sh in wb.sheets():<br>    for r in range(sh.nrows):<p></p>
<pre><code>    # 输出指定行
    print( sh.row(r))
</code></pre><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">##### xlwt</span><br><span class="line"></span><br><span class="line">### excel_w.py</span><br><span class="line"></span><br><span class="line">### 导入 xlwt 库</span><br><span class="line"></span><br><span class="line">import xlwt</span><br><span class="line"></span><br><span class="line">### 创建 xls 文件对象</span><br><span class="line"></span><br><span class="line">wb = xlwt.Workbook()</span><br><span class="line"></span><br><span class="line">### 新增两个表单页</span><br><span class="line"></span><br><span class="line">sh1 = wb.add_sheet('成绩')</span><br><span class="line">sh2 = wb.add_sheet('汇总')</span><br><span class="line"></span><br><span class="line">### 然后按照位置来添加数据,第一个参数是行，第二个参数是列</span><br><span class="line"></span><br><span class="line">### 写入第一个sheet</span><br><span class="line"></span><br><span class="line">sh1.write(0, 0, '姓名')</span><br><span class="line">sh1.write(0, 1, '成绩')</span><br><span class="line">sh1.write(1, 0, '张三')</span><br><span class="line">sh1.write(1, 1, 88)</span><br><span class="line">sh1.write(2, 0, '李四')</span><br><span class="line">sh1.write(2, 1, 99.5)</span><br><span class="line"></span><br><span class="line">### 写入第二个sheet</span><br><span class="line"></span><br><span class="line">sh2.write(0, 0, '总分')</span><br><span class="line">sh2.write(1, 0, 187.5)</span><br><span class="line"></span><br><span class="line">### 最后保存文件即可</span><br><span class="line"></span><br><span class="line">wb.save('test_w.xls')</span><br><span class="line"></span><br><span class="line">pandas 也可以操作</span><br><span class="line"></span><br><span class="line">ExcelWriter: Class for writing DataFrame objects into excel sheets. </span><br><span class="line"></span><br><span class="line">    with ExcelWriter('path_to_file.xlsx', mode='a') as writer:</span><br><span class="line">           df.to_excel(writer, sheet_name='Sheet3')</span><br><span class="line">           writer = pd.ExcelWriter('exam.xlsx')</span><br><span class="line">    for i in all_grad_pair:</span><br><span class="line">        inflowdata = pd.read_csv(newfiledir+i+precitypair+'.txt',\</span><br><span class="line">            sep = '\t',header = 0, index_col = 0,encoding = 'utf-8')</span><br><span class="line">        </span><br><span class="line">        inflowdata.to_excel(writer,sheet_name = CityName[j])</span><br><span class="line"></span><br><span class="line">writer.save() </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>​```python<br>import pandas as pd</p>
<h1><span id="du-qu-liang-ge-biao-ge">读取两个表格</span><a href="#du-qu-liang-ge-biao-ge" class="header-anchor">#</a></h1><p>data1=pd.read_excel(‘文件路径’)<br>data2=pd.read_excel(‘文件路径’)</p>
<h1><span id="jiang-liang-ge-biao-ge-shu-chu-dao-yi-ge-excel-wen-jian-li-mian">将两个表格输出到一个excel文件里面</span><a href="#jiang-liang-ge-biao-ge-shu-chu-dao-yi-ge-excel-wen-jian-li-mian" class="header-anchor">#</a></h1><p>writer=pd.ExcelWriter(‘D:新表.xlsx’)<br>data1.to_excel(writer,sheet_name=’sheet1’)<br>data2.to_excel(writer,sheet_name=’sheet2’)</p>
<h1><span id="bi-xu-yun-xing-writer-save-bu-ran-bu-neng-shu-chu-dao-ben-di">必须运行writer.save()，不然不能输出到本地</span><a href="#bi-xu-yun-xing-writer-save-bu-ran-bu-neng-shu-chu-dao-ben-di" class="header-anchor">#</a></h1><p>writer.save()<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 第二周：numpy</span><br><span class="line"></span><br><span class="line">https://cloudxlab.com/blog/numpy-pandas-introduction/</span><br><span class="line"></span><br><span class="line">## 对象</span><br><span class="line"></span><br><span class="line">Numpy 提供了n维数组对象（ndarray, A multidimensional array object)</span><br><span class="line"></span><br><span class="line">## 创建</span><br><span class="line"></span><br><span class="line">&lt;img src="F:/MyBlog/MyBlog/hexo/source/_posts/prepare-for-work/image-20200807212039210.png" alt="image-20200807212039210" style="zoom:50%;" /&gt;</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">np.arange(6)</span><br><span class="line">array([0,1, 2, 3, 4, 5])</span><br><span class="line">np.zeros(10) # 1-n</span><br><span class="line">np.zeros((3, 6)) # 2-n</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3><span id="types">types</span><a href="#types" class="header-anchor">#</a></h3><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">arr1 = np.array([1, 2, 3], dtype=np.float64)</span><br><span class="line">arr1.dtype</span><br><span class="line">float_arr = arr.astype(np.float64)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="suo-yin">索引</span><a href="#suo-yin" class="header-anchor">#</a></h2><p>就是用中括号，注意切片是一维数组还是多少维数组！[, ], 如果，后面省略表示全部索引。</p>
<p><img src="/2020/07/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPython/MyBlog/MyBlog/hexo/source/_posts/prepare-for-work/image-20200807213131859.png" alt="image-20200807213131859" style="zoom:33%;"></p>
<p>布尔类型索引</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">data[names == 'Bob']</span><br><span class="line">表示索引行</span><br><span class="line"></span><br><span class="line">data[names == 'Bob', 2:]</span><br><span class="line">表示行列索引</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="zeng-cha-ru">增(插入)</span><a href="#zeng-cha-ru" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.array([[1,2,3],[4,5,6],[7,8,9]])</span><br><span class="line">b = np.array([[0,0,0]])</span><br><span class="line">c = np.insert(a, 0, values=b, axis=0)</span><br><span class="line">d = np.insert(a, 0, values=b, axis=1)</span><br><span class="line">print c</span><br><span class="line">print d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">a = np.array([[1,2,3],[4,5,6],[7,8,9]])</span><br><span class="line">b = np.array([[0,0,0]])</span><br><span class="line">c = np.row_stack((a,b))</span><br><span class="line">d = np.column_stack((a,b.T)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2><span id="gai">改</span><a href="#gai" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">result = np.where(cond, xarr, yarr)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="suo-yin">索引</span><a href="#suo-yin" class="header-anchor">#</a></h3><h3><span id="zhi">值</span><a href="#zhi" class="header-anchor">#</a></h3><h2><span id="shan">删</span><a href="#shan" class="header-anchor">#</a></h2><h2><span id="ji-suan-han-shu">计算函数</span><a href="#ji-suan-han-shu" class="header-anchor">#</a></h2><p>+-<em>/ *</em>都是元素对齐</p>
<p>np.sqrt()</p>
<p>np.exp()</p>
<p><img src="/2020/07/28/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPython/MyBlog/MyBlog/hexo/source/_posts/prepare-for-work/image-20200807214032542.png" alt="image-20200807214032542" style="zoom:50%;"></p>
<h1><span id="pandas">pandas</span><a href="#pandas" class="header-anchor">#</a></h1><h2><span id="dui-xiang">对象</span><a href="#dui-xiang" class="header-anchor">#</a></h2><p>Series, DataFrame</p>
<h2><span id="chuang-jian">创建</span><a href="#chuang-jian" class="header-anchor">#</a></h2><ol>
<li><h2><span id="suo-yin">索引</span><a href="#suo-yin" class="header-anchor">#</a></h2></li>
</ol>
<h2><span id="zeng">增</span><a href="#zeng" class="header-anchor">#</a></h2><p>新增列：索引要对应，或者提供df[‘ ‘] = ;</p>
<p>新增行：列名要对应。一定要新建一个列名相同的，空DataFrame</p>
<h2><span id="gai">改</span><a href="#gai" class="header-anchor">#</a></h2><h3><span id="guo-lu">过滤</span><a href="#guo-lu" class="header-anchor">#</a></h3><p>过滤出来就是满足条件的部分，不满足条件的，不被修改</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">d = pd.DataFrame({<span class="string">'a'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],<span class="string">'b'</span>:[<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>,<span class="number">55</span>]})</span><br><span class="line">print(d)</span><br><span class="line">print(d[d&gt;<span class="number">10</span>])</span><br><span class="line">d[d&gt;<span class="number">10</span>] = <span class="number">10</span></span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line">   a   b</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">11</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">22</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">33</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">44</span></span><br><span class="line"><span class="number">4</span>  <span class="number">5</span>  <span class="number">55</span></span><br><span class="line">    a   b</span><br><span class="line"><span class="number">0</span> NaN  <span class="number">11</span></span><br><span class="line"><span class="number">1</span> NaN  <span class="number">22</span></span><br><span class="line"><span class="number">2</span> NaN  <span class="number">33</span></span><br><span class="line"><span class="number">3</span> NaN  <span class="number">44</span></span><br><span class="line"><span class="number">4</span> NaN  <span class="number">55</span></span><br><span class="line">   a   b</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">10</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">10</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">10</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">10</span></span><br><span class="line"><span class="number">4</span>  <span class="number">5</span>  <span class="number">10</span></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="suo-yin">索引</span><a href="#suo-yin" class="header-anchor">#</a></h3><h3><span id="zhi">值</span><a href="#zhi" class="header-anchor">#</a></h3><h2><span id="shan">删</span><a href="#shan" class="header-anchor">#</a></h2><h2><span id="ji-suan-han-shu">计算函数</span><a href="#ji-suan-han-shu" class="header-anchor">#</a></h2><h3><span id="pandas-date-range">pandas.date_range</span><a href="#pandas-date-range" class="header-anchor">#</a></h3><p>产生时间日期索引。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">data.index = pd.date_range(start = train.index[<span class="number">-1</span>]+timedelta(hours=<span class="number">1</span>),end = train.index[<span class="number">-1</span>]+timedelta(hours = <span class="number">12</span>),freq = <span class="string">'1h'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>apply() 最后的是经过函数处理，数据以 Series 或 DataFrame 格式返回。</p>
<h1><span id="matplotlib">matplotlib</span><a href="#matplotlib" class="header-anchor">#</a></h1><h1><span id="seaborn">seaborn</span><a href="#seaborn" class="header-anchor">#</a></h1><h1><span id="di-san-zhou-sklearn">第三周：sklearn</span><a href="#di-san-zhou-sklearn" class="header-anchor">#</a></h1><h1><span id="di-wu-zhou-sql">第五周：sql</span><a href="#di-wu-zhou-sql" class="header-anchor">#</a></h1><h1><span id="di-liu-zhou-linux">第六周： Linux</span><a href="#di-liu-zhou-linux" class="header-anchor">#</a></h1><h1><span id="datetime-ri-qi-he-shi-jian-ku">datetime日期和时间库</span><a href="#datetime-ri-qi-he-shi-jian-ku" class="header-anchor">#</a></h1><h2><span id="datetime-he-timestamp">datetime 和timestamp</span><a href="#datetime-he-timestamp" class="header-anchor">#</a></h2><p>在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为epoch time，记为<code>0</code>（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。</p>
<h2><span id="str-he-datetime">str和datetime</span><a href="#str-he-datetime" class="header-anchor">#</a></h2><h3><span id="str-zhuan-huan-wei-datetime">str转换为datetime</span><a href="#str-zhuan-huan-wei-datetime" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cday = datetime.strptime(<span class="string">'2015-6-1 18:19:59'</span>, <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cday)</span><br><span class="line"><span class="number">2015</span><span class="number">-06</span><span class="number">-01</span> <span class="number">18</span>:<span class="number">19</span>:<span class="number">59</span></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="datetime-to-str">datetime to str</span><a href="#datetime-to-str" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now = datetime.now()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(now.strftime(<span class="string">'%a, %b %d %H:%M'</span>))</span><br><span class="line">Mon, May <span class="number">05</span> <span class="number">16</span>:<span class="number">28</span></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="datatime-ji-suan">datatime 计算</span><a href="#datatime-ji-suan" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now = datetime.now()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now</span><br><span class="line">datetime.datetime(<span class="number">2015</span>, <span class="number">5</span>, <span class="number">18</span>, <span class="number">16</span>, <span class="number">57</span>, <span class="number">3</span>, <span class="number">540997</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now + timedelta(hours=<span class="number">10</span>)</span><br><span class="line">datetime.datetime(<span class="number">2015</span>, <span class="number">5</span>, <span class="number">19</span>, <span class="number">2</span>, <span class="number">57</span>, <span class="number">3</span>, <span class="number">540997</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now - timedelta(days=<span class="number">1</span>)</span><br><span class="line">datetime.datetime(<span class="number">2015</span>, <span class="number">5</span>, <span class="number">17</span>, <span class="number">16</span>, <span class="number">57</span>, <span class="number">3</span>, <span class="number">540997</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now + timedelta(days=<span class="number">2</span>, hours=<span class="number">12</span>)</span><br><span class="line">datetime.datetime(<span class="number">2015</span>, <span class="number">5</span>, <span class="number">21</span>, <span class="number">4</span>, <span class="number">57</span>, <span class="number">3</span>, <span class="number">540997</span>)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpuy</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><img src="/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MyBlog\hexo\source\_posts\机器学习\机器学习算法系列.png" alt></p>
<h1><span id="xian-xing-hui-gui">线性回归</span><a href="#xian-xing-hui-gui" class="header-anchor">#</a></h1><h1><span id="luo-ji-si-te-hui-gui">逻辑斯特回归</span><a href="#luo-ji-si-te-hui-gui" class="header-anchor">#</a></h1><h1><span id="jue-ce-shu">决策树</span><a href="#jue-ce-shu" class="header-anchor">#</a></h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习与非监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Analysis</title>
    <url>/2020/07/25/Data-Analysis/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学</title>
    <url>/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/</url>
    <content><![CDATA[<p>统计学</p>
<a id="more"></a>
<h1><span id="tong-ji-xue-day-ox00">统计学 day Ox00</span><a href="#tong-ji-xue-day-ox00" class="header-anchor">#</a></h1><p>day Ox00 首先介绍随机实验的基本概念，包括随机实验，样本点，样本空间，基本事件，随机事件；其次介绍概率论的基本概念，包括概率的公理化定义，古典概率，条件概率，全概率，贝叶斯公式等等。特别注意两个容易混淆的概念：事件的独立性和互斥。</p>
<p>day Ox01 首先引 出随机变量的定义，从离散随机变量和连续随机变量两个维度，介绍典型的分布函数。其中概率函数和分布函数是非常重要的概念。</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/概率论与数量统计.png" alt></p>
<!-- more -->
<h2><span id="ji-ben-gai-nian">基本概念</span><a href="#ji-ben-gai-nian" class="header-anchor">#</a></h2><p>随机试验：记作$E$</p>
<p>样本点： 随机试验中出现的可能结果称为样本点，记作 $\omega$</p>
<p>样本空间： 所有样本点组成的集合称为样本空间，随机实验所有的结果的集合，记作$\Omega$</p>
<p>事件： 样本空间的子集，叫做随机事件，记作A,B,C。</p>
<p>​    分类：基本事件（由一个样本点构成），不可能事件（不包含任何样本点），必然事件（样本空间的所有样本点组成）</p>
<p>事件的关系和运算</p>
<p>​    A与B互斥（互不相容），并为空集。不可能同时发生。</p>
<p>​    对立（互逆）：A,B在一次实验中有且仅有一个发生。</p>
<p><img src="https://img-blog.csdnimg.cn/20200510094048966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW5femFuX3l1Xw==,size_16,color_FFFFFF,t_70" alt="img"></p>
<h2><span id="shi-jian-jian-de-guan-xi">事件间的关系</span><a href="#shi-jian-jian-de-guan-xi" class="header-anchor">#</a></h2><p>包含</p>
<p>相等</p>
<p>互不相容性：不可能同时发生，没有交集</p>
<h2><span id="shi-jian-de-gai-lu">事件的概率</span><a href="#shi-jian-de-gai-lu" class="header-anchor">#</a></h2><h3><span id="gai-lu-de-gong-li-hua-ding-yi">概率的公理化定义</span><a href="#gai-lu-de-gong-li-hua-ding-yi" class="header-anchor">#</a></h3><h3><span id="ji-suan-fang-fa">计算方法</span><a href="#ji-suan-fang-fa" class="header-anchor">#</a></h3><h4><span id="gu-dian-fang-fa">古典方法</span><a href="#gu-dian-fang-fa" class="header-anchor">#</a></h4><p>随机事件的要求：(1). 涉及的随机现象只有有限个基本结果（2). 每个基本结果出现的可能性是相同的（等可能性）</p>
<p>事件的基本结果：</p>
<script type="math/tex; mode=display">
P(A) = \frac{k}{n} = \frac{事件包含的基本事件的个数}{全空间包含的基本结果总数}</script><h4><span id="shi-jian-de-du-li-xing">事件的独立性</span><a href="#shi-jian-de-du-li-xing" class="header-anchor">#</a></h4><p>两个事件的独立性是指一个事件的发生不影响另一个事件的发生，</p>
<script type="math/tex; mode=display">
P(AB) = P(A)P(B)</script><p>多个事件的独立性</p>
<script type="math/tex; mode=display">
P(A_iA_j) = P(A_i)P(A_j)\\
P(A_iA_jA_k) = P(A_i)P(A_j)P(A_k)\\
\vdots
P(A_1A_2\cdots A_n) = P(A_1)P(A_2)\cdots P(A_n)</script><h3><span id="shi-yan-de-du-li-xing">实验的独立性</span><a href="#shi-yan-de-du-li-xing" class="header-anchor">#</a></h3><p>实验$E_1$的任意一个结果（事件）与实验$E_2$的任一个结果都是相互独立的事件，则称实验相互独立</p>
<h3><span id="tiao-jian-gai-lu">条件概率</span><a href="#tiao-jian-gai-lu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
P(A|B) = \frac{P(AB)}{P(B)}</script><h3><span id="cheng-fa-gong-shi">乘法公式</span><a href="#cheng-fa-gong-shi" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
P(A_1A_2A_3) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)</script><h3><span id="quan-gai-lu-gong-shi">全概率公式</span><a href="#quan-gai-lu-gong-shi" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
P(A) = P(A|B)P(B)+P(A|\hat{B})P(\hat{B})</script><script type="math/tex; mode=display">
P(A) = \sum_{i = 1}^nP(A|B_i)P(B_i)</script><h3><span id="bei-xie-si-gong-shi">贝叶斯公式</span><a href="#bei-xie-si-gong-shi" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i = 1}^nP(A|B_k)P(B_k)}</script><h1><span id="sui-ji-bian-liang-day-ox01">随机变量 day Ox01</span><a href="#sui-ji-bian-liang-day-ox01" class="header-anchor">#</a></h1><p>随机变量表示随机现象结果，一般大写字母X,Y,Z,</p>
<p>随机变量取值用小写字母x,y,z等表示。</p>
<p>用等号或者不等号把X与x联系起来就很多有趣的事件，X=x,Y&lt;y,等等构成了事件。</p>
<h2><span id="sui-ji-bian-liang">随机变量</span><a href="#sui-ji-bian-liang" class="header-anchor">#</a></h2><p>定义在基本空间$\Omega$上的实值函数$X = X(w)$成为随机空间</p>
<script type="math/tex; mode=display">
X:
w->实数域（映射)</script><h2><span id="sui-ji-bian-liang-de-fen-bu-han-shu">随机变量的分布函数</span><a href="#sui-ji-bian-liang-de-fen-bu-han-shu" class="header-anchor">#</a></h2><p>分布函数的定义</p>
<script type="math/tex; mode=display">
F(x) = P(X<=x)</script><p>离散随机变量：分段函数</p>
<p>连续随机变量：连续函数</p>
<h2><span id="chi-san-sui-ji-bian-liang">离散随机变量</span><a href="#chi-san-sui-ji-bian-liang" class="header-anchor">#</a></h2><p>${p(x)}$ 成为随机变量X的分布列，或概率分布 $X ~ {p(x_i)}$</p>
<p>其分布函数$F(x) = \sum_{x_i&lt;x}p(x_i)$</p>
<h3><span id="chang-jian-fen-bu">常见分布</span><a href="#chang-jian-fen-bu" class="header-anchor">#</a></h3><h3><span id="er-xiang-fen-bu">二项分布</span><a href="#er-xiang-fen-bu" class="header-anchor">#</a></h3><p>特点</p>
<ol>
<li>重复进行$n$次相互独立的实验</li>
<li>每次实验只可能有两个结果</li>
<li>每次实验出现成功的概率相同，且为p</li>
</ol>
<p>符号说明</p>
<p>$ B_{n,k}$表示n重贝努里实验中成功出现的k次</p>
<p>X:n重贝努里实验中成功的次数，则有</p>
<p>$B_{n,k} =’X=k’$</p>
<script type="math/tex; mode=display">
P(X=x)=\left ( \begin{matrix}
n\\ 
x
\end{matrix}\right )p^x(1-p)^{n-x}</script><h3><span id="bo-song-fen-bu">泊松分布</span><a href="#bo-song-fen-bu" class="header-anchor">#</a></h3><p>$P(X = x) = \frac{\lambda^x}{x!}e^{-\lambda}$</p>
<h2><span id="lian-xu-sui-ji-bian-liang">连续随机变量</span><a href="#lian-xu-sui-ji-bian-liang" class="header-anchor">#</a></h2><p>概率密度函数</p>
<h3><span id="jun-yun-fen-bu">均匀分布</span><a href="#jun-yun-fen-bu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
p(x) = \left\{\begin{array}{c}
\frac{1}{b-a}& a<=x<=b\\
0, others
\end{array} \right.</script><p>分布函数</p>
<script type="math/tex; mode=display">
F(x)=\left\{\begin{array}{c}
0, x<a\\
\frac{x-a}{b-a}, a<=x<=b\\
1, x>b
\end{array}\right.</script><p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/image-20200629204515883.png" alt="image-20200629204515883"></p>
<h3><span id="zhi-shu-fen-bu-x-exp-lambda">指数分布 X~Exp($\lambda$)</span><a href="#zhi-shu-fen-bu-x-exp-lambda" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
p(x) = \left\{\begin{array}{c}
\lambda e^{-\lambda x},x>=0\\
0, x<0
\end{array}\right.</script><script type="math/tex; mode=display">
F(x) = \left\{\begin{array}{c}
0,x<0\\
1-e^{-\lambda x}, x>=0
\end{array}
\right.</script><p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/image-20200629204540374.png" alt="image-20200629204540374"></p>
<h3><span id="zheng-tai-fen-bu-x-n-u-sigma-2">正太分布X~N(u,$\sigma^2$)</span><a href="#zheng-tai-fen-bu-x-n-u-sigma-2" class="header-anchor">#</a></h3><p>概率密度函数</p>
<script type="math/tex; mode=display">
p(x) = \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(x-u)^2}{2\sigma^2}}</script><p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/image-20200629204309479.png" alt="image-20200629204309479"></p>
<h3><span id="jia-ma-fen-bu-ga-a-lambda">伽马分布Ga(a,$\lambda$)</span><a href="#jia-ma-fen-bu-ga-a-lambda" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
p(x)=\left\{\begin{array}{c}
\frac{\lambda^{a}}{\Gamma(a)}x^{a-1}e^{-\lambda x},x>0\\
0,x<=0
\end{array}\right.</script><p>含义两个参数$a$和$\lambda$,$a&gt;0$称为形状参数，$\lambda&gt;0$ 称为尺度参数。当$a&gt;1$密度函数是单峰，峰值位于$x = (a-1)/\lambda$ ; 当$1<a<=2$,其密度函数是先上凸，后下凸；对$a>2$, 其密度是先下凸，中间上凸，最后又下凸</a<=2$,其密度函数是先上凸，后下凸；对$a></p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/image-20200629210747963.png" alt="image-20200629210747963"></p>
<h1><span id="sui-ji-bian-liang-de-shu-zi-te-zheng-day-ox02">随机变量的数字特征 day Ox02</span><a href="#sui-ji-bian-liang-de-shu-zi-te-zheng-day-ox02" class="header-anchor">#</a></h1><h2><span id="qi-wang-e-x">期望 E(X)</span><a href="#qi-wang-e-x" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
E(X) = \left\{
\begin{array}{c}
\sum_{i}x_ip(x_i)\\
\int_{-\infty}^{+\infty}xp(x)dx
\end{array}\right.</script><h3><span id="fang-chai-var-x">方差 Var(X)</span><a href="#fang-chai-var-x" class="header-anchor">#</a></h3><p>偏差平方的数学期望</p>
<script type="math/tex; mode=display">
Var(X) = E(X-E(X))^2</script><h3><span id="biao-zhun-chai-sigma-x">标准差 $\sigma_X$</span><a href="#biao-zhun-chai-sigma-x" class="header-anchor">#</a></h3><p>方差的正平方根（开根号）</p>
<h3><span id="k-jie-yuan-dian-ju-u-k">k阶（原点）矩($u_k$)</span><a href="#k-jie-yuan-dian-ju-u-k" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
u_k = E(X^k)</script><h2><span id="k-jie-zhong-xin-ju-v-k">k阶中心矩 $v_k$</span><a href="#k-jie-zhong-xin-ju-v-k" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
v_k = E(X-E(X))^k</script><h3><span id="bian-yi-xi-shu-c-v">变异系数$C_v$</span><a href="#bian-yi-xi-shu-c-v" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
C_V = \frac{\sqrt{Var(X)}}{E(X)}</script><h3><span id="pian-du">偏度</span><a href="#pian-du" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\beta_s = \frac{E(X-E(X))^3}{[Var(X)]^{3/2}}</script><p>称为偏度系数。$\beta_s&gt;0$ 称该分布为正偏，或右偏。$\beta_s&lt;0$ 称该分布为负偏，又称左偏。刻画的是描述分布偏离对称性程度的一个特征数。</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/Users\23230\AppData\Roaming\Typora\typora-user-images\image-20200630112633750.png" alt="image-20200630112633750"></p>
<h3><span id="feng-du">峰度</span><a href="#feng-du" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\beta_k = \frac{v_4}{v_2^2}-3 = \frac{E(X-EX)^4}{var(X)^2}-3</script><p>叫峰度系数，简称峰度。峰度是描述分布尖梢程度和/或尾部粗细的一个特征数。相对于正态分布而言的超出量。</p>
<p>$\beta_k&gt;0$ 表示标准化后的分布比标准正态分布更尖梢和/或尾部更粗</p>
<p>$\beta_k &lt; 0 $ 表示标准化后的分布比标准化状态分布更平坦和/或尾部更细</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/Users\23230\AppData\Roaming\Typora\typora-user-images\image-20200630113431889.png" alt="image-20200630113431889"></p>
<p>偏度和峰度都是描述分布形状的特征数</p>
<h3><span id="zhong-wei-shu">中位数</span><a href="#zhong-wei-shu" class="header-anchor">#</a></h3><p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/Users\23230\AppData\Roaming\Typora\typora-user-images\image-20200630142321659.png" alt="image-20200630142321659"></p>
<h3><span id="fen-wei-shu">分位数</span><a href="#fen-wei-shu" class="header-anchor">#</a></h3><p>概率累积</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/Users\23230\AppData\Roaming\Typora\typora-user-images\image-20200630142946479.png" alt="image-20200630142946479"></p>
<h3><span id="zhong-shu">众数</span><a href="#zhong-shu" class="header-anchor">#</a></h3><h1><span id="duo-wei-sui-ji-bian-liang-ji-qi-lian-he-fen-bu-day-ox03">多维随机变量及其联合分布 day Ox03</span><a href="#duo-wei-sui-ji-bian-liang-ji-qi-lian-he-fen-bu-day-ox03" class="header-anchor">#</a></h1><h2><span id="n-wei-sui-ji-xiang-liang">n维随机向量</span><a href="#n-wei-sui-ji-xiang-liang" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
X(\omega) = (X_1(\omega),X_2(\omega),...,X_n(\omega))</script><h2><span id="lian-he-fen-bu-han-shu">联合分布函数</span><a href="#lian-he-fen-bu-han-shu" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
F(x_1,x_2,...,x_n) = \\P(X_1<=x_1,X_2<=x_2,X_3<=x_3,...,X_n<=x_n)</script><h2><span id="bian-ji-mi-du-han-shu">边际密度函数</span><a href="#bian-ji-mi-du-han-shu" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
p_Y(y) = \int_{-\infty}^{+\infty}p(x,y)dx</script><h2><span id="du-li-xing">独立性</span><a href="#du-li-xing" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
F(x_1,x_2,..,x_n) = F(x_1)F(x_2)...F(x_3)</script><h2><span id="juan-ji-gong-shi">卷积公式</span><a href="#juan-ji-gong-shi" class="header-anchor">#</a></h2><p>X与Y相互独立的连续随机变量，其密度函数$P_X(x)$和$P_Y(y)$，则其和$Z = X+Y$ 的密度函数为</p>
<script type="math/tex; mode=display">
p_Z(z) = \int_{-\infty}^{+\infty}p_X(z-y)p_Y(y)dy</script><h2><span id="shu-zi-te-zheng">数字特征</span><a href="#shu-zi-te-zheng" class="header-anchor">#</a></h2><p>期望</p>
<h3><span id="xie-fang-chai">协方差</span><a href="#xie-fang-chai" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
Cov(X,Y) = E[(X-E(X))(Y-E(Y))]</script><h4><span id="xiang-guan-xi-shu">相关系数</span><a href="#xiang-guan-xi-shu" class="header-anchor">#</a></h4><script type="math/tex; mode=display">
Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}</script><h2><span id="tiao-jian-fen-bu">条件分布</span><a href="#tiao-jian-fen-bu" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
P(X=x_i|Y = y_i) = \frac{P(X=x_i,Y = y_i)}{P(Y = y_i)} = \frac{p_{ij}}{p_{.j}}</script><script type="math/tex; mode=display">
p(x|y) = \frac{p(x,y)}{P_Y(y)}</script><h1><span id="shu-li-tong-ji-day-ox04">数理统计 day Ox04</span><a href="#shu-li-tong-ji-day-ox04" class="header-anchor">#</a></h1><p>概率论研究的是概率、各种分布的性质。数量统计则是对随机现象的观察或试验来获取数据，对已知的数据进行分析，并推断隐藏在数据背后的统计规律性。</p>
<h2><span id="xiang-guan-zhu-yu">相关术语</span><a href="#xiang-guan-zhu-yu" class="header-anchor">#</a></h2><p>研究对象的全体成为总体；构成总体的每个成员叫个体。</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/MyBlog\hexo\source\_posts\统计学\image-20200704205119931.png" alt="image-20200704205119931"></p>
<h2><span id="cong-yang-ben-qu-ren-shi-zong-ti">从样本去认识总体</span><a href="#cong-yang-ben-qu-ren-shi-zong-ti" class="header-anchor">#</a></h2><h3><span id="pin-shu-pin-lu-fen-bu">频数频率分布</span><a href="#pin-shu-pin-lu-fen-bu" class="header-anchor">#</a></h3><p>样本直方图描述总体概率密度函数的大致形状</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/MyBlog\hexo\source\_posts\统计学\image-20200704205517901.png" alt="image-20200704205517901"></p>
<h3><span id="jing-yan-fen-bu-han-shu">经验分布函数</span><a href="#jing-yan-fen-bu-han-shu" class="header-anchor">#</a></h3><p>描述总体分布函数的大致形状</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/MyBlog\hexo\source\_posts\统计学\image-20200704205839356.png" alt="image-20200704205839356"></p>
<h3><span id="tong-ji-liang-yu-chou-yang-fen-bu">统计量与抽样分布</span><a href="#tong-ji-liang-yu-chou-yang-fen-bu" class="header-anchor">#</a></h3><p>统计量（抽样分布）,$X = (X_1,X_2,..,X_n)$是来自总体的一个容量$n$的样本。</p>
<script type="math/tex; mode=display">
T = T(X_1,X_2,...,X_n)</script><p>则$T$为统计量，统计量的分布称为抽样分布。</p>
<p>观测值，$x = x_1,x_2,…,x_n$后，带入统计量</p>
<script type="math/tex; mode=display">
t = T(x) = T(x_1,x_2,..,x_n)</script><p>统计量不能含有未知参数。</p>
<h3><span id="yang-ben-jun-zhi-ji-qi-fen-bu">样本均值及其分布</span><a href="#yang-ben-jun-zhi-ji-qi-fen-bu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\hat{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i</script><h3><span id="yang-ben-fang-chai-yu-yang-ben-biao-zhun-chai">样本方差与样本标准差</span><a href="#yang-ben-fang-chai-yu-yang-ben-biao-zhun-chai" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
S_n^2 = \frac{1}{n}\sum_{i = 1}^n(X_i-\hat{X})^2</script><h3><span id="yang-ben-de-gao-jie-ju">样本的高阶矩</span><a href="#yang-ben-de-gao-jie-ju" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
A_k = \frac{1}{n}\sum_{i = 1}^nX_i^k</script><script type="math/tex; mode=display">
B_k = \frac{1}{n}\sum_{i = 1}^n(X_i-\hat{X})^k</script><h3><span id="fen-wei-shu">分位数</span><a href="#fen-wei-shu" class="header-anchor">#</a></h3><p>$m_d$为样本中位数</p>
<p>$X_1,X_2,..,X_n$是来自总体的一个样本，其次序统计量为$X_{(1)}&lt;=X_{(2)}&lt;=…&lt;=X_{(n)}$，样本的p分位数$m_p$</p>
<script type="math/tex; mode=display">
m_p=\left\{\begin{array}{c}
X_{(k)},\frac{k}{n+1} = p\\
X_{(k)}+[X_{k+1}-X_{k}][(n+1)p-k],\frac{k}{n+1}<p<\frac{k+1}{n+1}
\end{array}\right.</script><h4><span id="xiang-xian-tu">箱线图</span><a href="#xiang-xian-tu" class="header-anchor">#</a></h4><p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/MyBlog\hexo\source\_posts\统计学\image-20200704214857964.png" alt="image-20200704214857964"></p>
<h2><span id="ju-gu-ji-ox05">矩估计 Ox05</span><a href="#ju-gu-ji-ox05" class="header-anchor">#</a></h2><h3><span id="ju-gu-ji">矩估计</span><a href="#ju-gu-ji" class="header-anchor">#</a></h3><p>想法:样本矩估计代替总体矩</p>
<h3><span id="wu-pian-gu-ji">无偏估计</span><a href="#wu-pian-gu-ji" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\hat{\theta} = \hat{\theta}(X_1,X_2,...,X_n)</script><p>为参数$\theta$的估计量</p>
<script type="math/tex; mode=display">
E(\hat{\theta}) = \theta</script><h2><span id="ji-da-si-ran-gu-ji">极大似然估计</span><a href="#ji-da-si-ran-gu-ji" class="header-anchor">#</a></h2><h2><span id="qu-jian-gu-ji">区间估计</span><a href="#qu-jian-gu-ji" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
p(|\bar{x}-\mu|<\Delta)=1-\alfha</script><p>如果$\hat{x}$的分布已知，则可以求解出$\Delta$,就是区间概率，求解边界</p>
<h2><span id="dian-gu-ji">点估计</span><a href="#dian-gu-ji" class="header-anchor">#</a></h2><p><img src="https://pic4.zhimg.com/80/v2-a7de61cc93aad8aeb5cfad6090f12ffb_720w.jpg" alt="img"></p>
<h2><span id="bei-xie-si-gu-ji">贝叶斯估计</span><a href="#bei-xie-si-gu-ji" class="header-anchor">#</a></h2><h2><span id="jia-she-jian-yan">假设检验</span><a href="#jia-she-jian-yan" class="header-anchor">#</a></h2><p>Step 1: 建立假设</p>
<p>原假设和备择假设</p>
<p>Step 2: 选择检验统计量</p>
<p>Step3: 选择显著性水平</p>
<p>第一类错误：原假设H_0为真，样本观察值落入拒绝域W,其发生的概率为，$\alfha$</p>
<p><img src="/2020/07/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6/Users\23230\AppData\Roaming\Typora\typora-user-images\image-20200705214507303.png" alt="image-20200705214507303"></p>
<p>Step 4: 确定临界值，给出拒绝域</p>
<h1><span id="zhuan-ti-xi-lie">专题系列</span><a href="#zhuan-ti-xi-lie" class="header-anchor">#</a></h1><h2><span id="zhuan-ti-yi-can-shu-gu-ji">专题一：参数估计</span><a href="#zhuan-ti-yi-can-shu-gu-ji" class="header-anchor">#</a></h2><p>数据-&gt; 推断：概率密度函数-&gt;先确定：概率密度函数的形式和参数。</p>
<p>参数估计多大方法: 极大似然估计、最大后验估计、贝叶斯估计、最大熵估计、混合模型估计。</p>
<h3><span id="ji-da-si-ran-gu-ji-mle">极大似然估计(MLE)</span><a href="#ji-da-si-ran-gu-ji-mle" class="header-anchor">#</a></h3><p><strong>形式: 模型已定，参数未知</strong></p>
<p>属于频率派-》优化问题》点估计</p>
<p>假设： 采样独立同分布</p>
<p>想法：似然函数最大，就是不断改变参数，使得事件集发生的概率最大。$\theta$是定值。</p>
<script type="math/tex; mode=display">
p(x|\theta)</script><p>如果$\theta$确定，$x$是变量，这个函数叫做概率函数（probability function)</p>
<p>如果$x$已知，$\theta$未知，叫似然函数（likelihood function),描述对于不同参数的模型，在这种情况下，出现这个样本点的概率是多少</p>
<script type="math/tex; mode=display">
L(\theta) = P(x_1,x_2,...,x_n|\theta)=\Pi_{i=1}^{n}P(x_i|\theta)</script><script type="math/tex; mode=display">
\theta = arg max_{\theta}\Pi_{i = 1}^{n}P(x_i|\theta)</script><h3><span id="zui-da-hou-yan-gu-ji-map">最大后验估计（MAP)</span><a href="#zui-da-hou-yan-gu-ji-map" class="header-anchor">#</a></h3><p>$\theta$是随机变量，最大后验估计要求$P(\theta|X)$最大。</p>
<script type="math/tex; mode=display">
P(\theta|X) = \frac{P(\theta)P(X|\theta)}{p(X)}</script><p>因为$P(X)$和$\theta$是独立的，所以直接忽略$p(X)$</p>
<script type="math/tex; mode=display">
\theta_{max}= argmax_{\theta}p(\theta)p(X|\theta)</script><p>$p(\theta)$称为先验概率。如果服从均匀分布，$p(\theta)$服从均匀分布，</p>
<p><img src="https://img-blog.csdn.net/20170606223618992" alt="img"></p>
<h2><span id="zhuan-ti-er-chang-jian-fen-bu">专题二： 常见分布</span><a href="#zhuan-ti-er-chang-jian-fen-bu" class="header-anchor">#</a></h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247510542&amp;idx=1&amp;sn=40a49b52f65ea475a71c3756bc3443d5&amp;chksm=e8737b43df04f25565b4c1d58a8c0e7e5985271da20a60c289ac3c4a7d4427ebe33bc9efc48d&amp;scene=0&amp;xtrack=1#rd">https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247510542&amp;idx=1&amp;sn=40a49b52f65ea475a71c3756bc3443d5&amp;chksm=e8737b43df04f25565b4c1d58a8c0e7e5985271da20a60c289ac3c4a7d4427ebe33bc9efc48d&amp;scene=0&amp;xtrack=1#rd</a></p>
<h2><span id="zhuan-ti-san-jia-she-jian-yan">专题三 ：假设检验</span><a href="#zhuan-ti-san-jia-she-jian-yan" class="header-anchor">#</a></h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&amp;mid=2648938280&amp;idx=2&amp;sn=dc6184ab880a735be9aadd0bce3383ed&amp;chksm=87940502b0e38c1499e6247e91d65b15e8354b202aba53790859d2d0ffc9081c4cb4b7ae8369&amp;scene=0&amp;xtrack=1#rd">https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&amp;mid=2648938280&amp;idx=2&amp;sn=dc6184ab880a735be9aadd0bce3383ed&amp;chksm=87940502b0e38c1499e6247e91d65b15e8354b202aba53790859d2d0ffc9081c4cb4b7ae8369&amp;scene=0&amp;xtrack=1#rd</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2NzIzNzg4NQ==&amp;mid=2247483764&amp;idx=1&amp;sn=5ad227215a9c634cb7114c36289e2cea&amp;chksm=cebfebe6f9c862f0ff6c2bb71648740c21129494b34da747ddd0a4e2028404401b1487d44577&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=Mzg2NzIzNzg4NQ==&amp;mid=2247483764&amp;idx=1&amp;sn=5ad227215a9c634cb7114c36289e2cea&amp;chksm=cebfebe6f9c862f0ff6c2bb71648740c21129494b34da747ddd0a4e2028404401b1487d44577&amp;scene=21#wechat_redirect</a></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title>西瓜书</title>
    <url>/2020/07/17/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2><span id="yue-du-mu-lu"><em>阅读目录</em></span><a href="#yue-du-mu-lu" class="header-anchor">#</a></h2><p>[TOC]</p>
<h2><span id="di-yi-zhang">第一章</span><a href="#di-yi-zhang" class="header-anchor">#</a></h2><a id="more"></a>
<ol>
<li><p>What is the machine learning?<br>非常官方的定义： Tom mitchell(1998) Well-posed Learning<br>Problem:A compute program is said to learn from experience E with respect to same task T and some performance measure P,if its performance on T,as measured by P, improves with experience E。（这个我莫法翻译喔）<br>大概意思是强大的计算机能够事先地完成人为非显示编程好的任务，怎么完成呢？对于某个任务T,给定一个性能度量方法P,在经验E的影响下，如果P对T的测量结果得到了改进，则说明该程序从E中学习了<br>机器学习的过程大致如此：让计算机从数据中产生模型(model)，首先提供经验数据，给定学习算法(learning algorithm)和性能测量方法，它就能根据数据产生模型。<br>模型： 泛指从数据中学得的结果<br>模式： 局部性的结果</p>
<ol>
<li>基本术语<br>数据集: data set<br>样本： sample<br>属性（特征）： attribute（feature)<br>属性值： attribute value<br>属性空间（特征空间）： attribute space （ sample space）<br>特征向量： feature vector<br>学习（训练）：learning（training）<br>训练数据： training data<br>训练集： training set<br>假设：hypothesis 学得模型对应了关于数据的某种潜在规律<br>泛函能力: generalization</li>
<li>假设空间<br>归纳（induction）： 从特殊到一般的“泛化”(generalization)过程<br>演绎（deduction)： 从一般到特殊的“特化”(specialization)过程<br>机器学习显然是归纳学习（inductive learning)<br>归纳学习分狭义与广义，狭义是指要求从training set 中学得概念，广义是指从sample中学习</li>
</ol>
<p>学习过程（训练过程）看作是在所以假设组成的空间中进行搜索的过程，搜索目标是找到与training set匹配的假设。如果假设的表示一旦确定，假设空间与其规模就确定了。想更详细了解假设空间，<a href="http://www.deeplearningbook.org/contents/ml.html">戳我啦</a>5.2<br>现实问题中常面临很大的假设空间，我们可以寻找一个与训练集一致的假设集合，称之为版本空间。版本空间从假设空间剔除了与正例不一致和与反例一致的假设，它可以看成是对正例的最大泛化。<br>归纳偏好<br>机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias),也就是学习算法在一个可能很庞大的假设空间中对假设进行选择的启发式或者“价值观”<br>奥卡姆剃刀定律： 若有多个假设与观测一致，则选择做简单的哪个。<br>没有免费的无餐定理（No Free Lunch Theorem[NFL]) 在所以问题出现的机会相同，或者所以问题同等重要下，所有算法的期望一样。但在实际问题中，针对具体的问题，不同的算法才会出现相对优劣。                        </p>
<ol>
<li>发展历程<br>推理期：二十世纪五十年代到七十年代初，AI处于推理区，代表性工作主要是A.Newell 和H.Simon的“逻辑理论家”程序和此后的“通用问题求解”程序等。“逻辑理论家”程序证明了数学家罗素和怀特海的《数学原理》里面的某些定理，获得图灵奖。<br>知识期：从二十世纪七十年代中期开始，AI的研究进入了“知识期”，大量的专家系统出现，E.A.Feigenbaum（知识工程之父）在1994获得图灵奖。人们意识到，专家系统面临“知识工程瓶颈”,在那个时候，有人把知识总结出来再教给计算机是相当困难的。<br>1950年，图灵再关于图灵测试的文章中，曾提到机器学习的可能<br>二十世纪五十年代初，A.Samuel著名跳棋程序。五十年代中后期，基于神经网络的”连接主义“学习，如F.Rosenblatt的感知器（Perceptro），B.Widrem的Adaline,六七十年代，基于逻辑表示的”符号主义学习技术蓬勃发展<br>学习期：二十世纪八十年代是机器学习百花初放的时期。一大主流是符号主义学习，代表决策树（decision tree).二十世纪九十年代中期之前，另外一大主流技术是基于神经网络的连接主义学习。二十世纪九十年代中期，”统计学习“占据主流，代表支持向量机。二十一世纪初，连接主义学习掀起了”深度学习“为名的热潮。</li>
</ol>
</li>
</ol>
<h2><span id="di-er-zhang-mo-xing-ping-gu-yu-xuan-ze">第二章 ： 模型评估与选择</span><a href="#di-er-zhang-mo-xing-ping-gu-yu-xuan-ze" class="header-anchor">#</a></h2><h3><span id="jing-yan-wu-chai-yu-guo-ni-he-qian-ni-he">经验误差与过拟合、欠拟合</span><a href="#jing-yan-wu-chai-yu-guo-ni-he-qian-ni-he" class="header-anchor">#</a></h3><p>训练误差（training error) or 经验误差（empirical error): 学习器在训练集上的输出与训练集之间的差异<br>过拟合（over fitting）：在训练集上表现非常好，泛化能力太差，最常见的情况是学习能力太强学习到不太一般的特性，无法彻底避免，只能“缓解”<br>欠拟合（under fitting）：这种情况容易克服<br>模型选择(model selection): 不同的参数配置，产生不同的模型。理论上最好的模型是对泛化能力进行评估，最好的就是泛化误差最小的，泛化误差是无法直接获取的</p>
<h3><span id="ping-gu-fang-fa">评估方法</span><a href="#ping-gu-fang-fa" class="header-anchor">#</a></h3><p>设置一个”测试集（testing set)”来测试学习器在新样本的判断能力，用测试误差近似泛化误差<br>要求：</p>
<ol>
<li>测试样本与训练样本独立同分布的</li>
<li>测试集应该尽可能与训练集互斥，测试样本尽量不出现在训练集中<h4><span id="ru-he-chan-sheng-training-set-he-testing-set">如何产生training set 和 testing set</span><a href="#ru-he-chan-sheng-training-set-he-testing-set" class="header-anchor">#</a></h4></li>
<li>留出法（hold-out)<br>要求：数据集($D$)划分成两个互斥的集合（训练集($S$,测试集$T$),需要注意的是，划分后，尽量可能的保持数据分布的一致性。<br>不同的划分结果，得到不同的测试误差。单次使用留出法得到的结果是不够稳定的，所以一般采用若干次的随机划分，重复进行实验评估后去平均值</li>
<li>交叉验证法（cross validation)<br>I. 将数据($D$)划分成$k$个大小相似的互斥子集，每个子集$D_i$都尽可能保持数据分布的一致性<br>II. 每次都用$k-1$作为训练集，余下的哪个子集作为测试集，于是乎都到了k个测试结果的均值<br>值得注意的是，$k$的取值对结果的稳定性和保真性有很大的影响，因此也叫k者交叉验证（k-flold cross validation) k的通常取值是10<br>同样的，数据集$D$划分为$k$个子集有很多的划分方式，可重复$P$次$k$折交叉验证。</li>
<li>自助法 (bootstrapping)<br>注意的是我们希望通过所以的训练集（$D$)训练出模型，但是流出法和交叉验证的方法，都保留一部分作为测试集，因此实际评估的模型所使用的训练集更下，这也许会导致估计偏差。<br>自助法： 可重复采样或者有放回采样</li>
<li>记采样产生的数据集（$D’$),每次从$D$中挑选应该样本，将其拷贝至($D’$),并再将采样的样本放回数据集($D$),重复($m$)次以后，得到了包含($m$)个样本的数据集($D’$)</li>
<li><p>对于可重复采样，样本始终不采到的概率是$(1-\frac{1}{m})^m$,取极限得到：初式数据集中$36.8%$为出现在采样数据集中，因此可将($D$)作为训练集，($D\D’$)作为测试集，又称外包估计(out-of-bag estimate)<br>自助法适用于数据量少，难区别测试集和训练集时，自助法会改变初始数据的分布，在初始数据足够的情况下，流出法和交叉验证更常用一些</p>
<h4><span id="diao-can-he-zui-zhong-de-mo-xing">调参和最终的模型</span><a href="#diao-can-he-zui-zhong-de-mo-xing" class="header-anchor">#</a></h4><p>学习算法都有参数(parameter),不同的参数配置，学得模型的性能也往往不同<br>验证集(validation set): 模型评估和选择中用于估计测试的数据集称为的数据集<br>往往将训练集划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参</p>
<h4><span id="xing-neng-du-liang-performance-measure">性能度量(performance measure)</span><a href="#xing-neng-du-liang-performance-measure" class="header-anchor">#</a></h4><h4><span id="jia-she-jian-yan">假设检验</span><a href="#jia-she-jian-yan" class="header-anchor">#</a></h4><p>（其实我一直都并不是特别了解）</p>
<ol>
<li>假设检验的基本原理<br>是重要的统计推断问题之一，根据样本提供的信息，检验关于总体某个假设是否正确。包括参数的假设检验（均值、方差等）和非参数（分布啊）的假设检验。 </li>
</ol>
</li>
<li>参数检验： 提出假设H—-&gt;在构造统计量，确定统计量的分布—-&gt; 确定拒绝域和接受域的分界线—-&gt; 在根据样本计算统计量的值u —-&gt; 推断</li>
<li>分布拟合检验</li>
</ol>
<h4><span id="pian-chai-he-fang-chai">偏差和方差</span><a href="#pian-chai-he-fang-chai" class="header-anchor">#</a></h4><p>通过概率论分析对学习算法的期望泛化错误率进行拆解<br>$x$: 测试样本<br>$y_D$： $x$在数据集中的标记<br>$y$: $x$的真实标记<br>$f(x:D)$: 在训练集上学得的模型$f$在$x$上预测输出<br>以回归任务为例子：<br>学习算法的期望预测为：</p>
<script type="math/tex; mode=display">\hat{f}(x) = E_D[f(x;D)]</script><p>方差：度量同样的样本大小的训练集的变动所导致的学习性能的变化，即刻画数据扰动所造成的影响</p>
<script type="math/tex; mode=display">var(x)= E_D[(f(x;D)-\hat{f}(x))^2]</script><p>噪声： 表达了当前任务上任务学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<script type="math/tex; mode=display">\epsilon^2=E_D[(y_D-y)^2]</script><p>期望输出和真实标记的差别称为偏差(bias): 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</p>
<script type="math/tex; mode=display">bias^2(x)=(f(x)-y)^2</script><p>若假设噪声期望为零，那么算法的期望泛化误差：</p>
<script type="math/tex; mode=display">
E(f;D)=E_D[(f(x;D)-y)^2]\\
=....=E_D[(f(x;D)-\hat{f}(x))^2]+(\hat{f}(x)-y)^2+E_D[(y_D-y)^2]</script><script type="math/tex; mode=display">E(f;D)=bias^2(x)+var(x)+\epsilon^2</script><p>由上式可知，泛化能力由学习算法的能力、数据的充分性、学习任务本身的难度共同决定的。<br>underfitting: 偏差主导泛化误差<br>over fitting： 训练数据发生的扰动渐渐被学习到，方差主导了泛化误差</p>
<h2><span id="di-san-zhang-xian-xing-mo-xing">第三章 线性模型</span><a href="#di-san-zhang-xian-xing-mo-xing" class="header-anchor">#</a></h2><p>我自己其实是一直停留在线性模型学习过程，因为每次开头都是这一张，所以我就学习了很多次。这次不准备再细看了。</p>
<h3><span id="xian-xing-pan-bie-fen-xi-linear-discriminant-analysis-lda">线性判别分析 Linear Discriminant Analysis (LDA)</span><a href="#xian-xing-pan-bie-fen-xi-linear-discriminant-analysis-lda" class="header-anchor">#</a></h3><p>基本思想： 在训练样例集上，设法将样本例子投影到一条直线上使得同类样例的投影尽可能接近、异类投影点尽可能远离。<br>数学表达：<br>$D={(x_i,y_i)}_{i=1}^{m}$: data set<br>$X_i$: 第$i$类集合<br>$u_i$: 第$i$类集合均值向量<br>$\sum{i}$: 第$i$类集合协方差矩阵<br>$ w^Tu_i$： 第$i$类集合在直线上的投影<br>$ w^T\sum_{i}w$: 样本点的在直线上的投影<br>学习算法：<br>同类更近：<br>$\min \sum_{i=1}^{n}(w^T\sum_{i}w)$<br>类中心越大：<br>$\max ||w^{T}u_1-(\sum_{i=2}(w^{T}u_i))||_2^2$<br>因此，想最大化的目标<br>考虑$i = 2$的情况</p>
<script type="math/tex; mode=display">J  = \frac{||w^Tu_0-w^Tu_1||_2^2}{w^T\sum_{i=1}w+w^T\sum_{i=2}w}
=\frac{w^T(u_0-u_1)(u_0-u_1)^Tw}{w^T(\sum_1+\sum_2)w}</script><script type="math/tex; mode=display">
应用空间几何和矩阵的关系描述
类内散度矩阵($S_W$)</script><script type="math/tex; mode=display">\sum_1+\sum_2</script><script type="math/tex; mode=display">
类间散度矩阵：</script><script type="math/tex; mode=display">(u_0-u_1)(u_0-u_1)^T</script><script type="math/tex; mode=display">
所以，我们想优化目标如下：</script><script type="math/tex; mode=display">J = \frac{w^T_Sbw}{w^TS_ww}</script><p>如何确定$w$呢？注意到分子分母都是关于$w$的二次型，因此解这和w的方向有关系，因此，可令 $w^TS_ww=1$<br>,优化问题可是如下：</p>
<script type="math/tex; mode=display">\min -w^TS_bw \\
s.t. w^TS_ww = 1</script><p>构造lagrange 函数</p>
<script type="math/tex; mode=display">L = -w^TS_bw+r(w^TS_ww-1)</script><p>对$w$求导可得：</p>
<script type="math/tex; mode=display">S_bw =rS_ww</script><p>$S_b w$和$ u_0 - u_1 $ 方向是$u_0-u_1$,不妨设</p>
<script type="math/tex; mode=display">S_nw=r(u_0-u_1)</script><p>so,<script type="math/tex">w = s_w^{-1}(u_0-u_1)</script><br>这里考虑到数值解的稳定性，因此往往把$S_w$进行<a href="https://www.cnblogs.com/xiemaycherry/p/10460464.html">奇异值分解</a></p>
<h2><span id="di-si-zhang-jue-ce-shu">第四章 决策树</span><a href="#di-si-zhang-jue-ce-shu" class="header-anchor">#</a></h2><p>决策树是一种特别普通的符合生活做决策的过程。</p>
<h2><span id="di-wu-zhang-shen-jing-wang-luo">第五章 神经网络</span><a href="#di-wu-zhang-shen-jing-wang-luo" class="header-anchor">#</a></h2><p>神经网络最开始出现是根据生物神经网络来的。</p>
<h3><span id="zui-jian-dan-de-shen-jing-wang-luo-shen-jing-yuan-mo-xing-neuron-unit">最简单的神经网络：神经元模型(neuron|unit)</span><a href="#zui-jian-dan-de-shen-jing-wang-luo-shen-jing-yuan-mo-xing-neuron-unit" class="header-anchor">#</a></h3><p>McCulloch and Pitts抽象出“M-P神经元模型”<br><img src="/2020/07/17/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/5.1.png" alt="tup"></p>
<h3><span id="gan-zhi-qi-perceptron">感知器（Perceptron)</span><a href="#gan-zhi-qi-perceptron" class="header-anchor">#</a></h3><p>输入层和输出层，输出层：M-P神经元<br>感知器的学习过程一定是收敛的</p>
<h3><span id="duo-ceng-qian-kui-shen-jing-wang-luo-multi-layer-feddforward-neural-networks">多层前馈神经网络 （multi-layer feddforward neural networks)</span><a href="#duo-ceng-qian-kui-shen-jing-wang-luo-multi-layer-feddforward-neural-networks" class="header-anchor">#</a></h3><p>前馈：网络的拓扑结构不存在环或者回路<br>神经元的学习过程：就是根据训练数据来调整神经元之间的”连接权”(connection weight),以及每个功能神经元的阙值</p>
<h3><span id="wu-chai-ni-chuan-bo-suan-fa-error-backpropagation-bp">误差逆传播算法： error BackPropagation (BP)</span><a href="#wu-chai-ni-chuan-bo-suan-fa-error-backpropagation-bp" class="header-anchor">#</a></h3><h2><span id="quan-ju-zui-xiao-he-ju-bu-zui-xiao">全局最小和局部最小</span><a href="#quan-ju-zui-xiao-he-ju-bu-zui-xiao" class="header-anchor">#</a></h2><p>神经网络的训练过程其实也就是参数寻优的过程，基于梯度的搜素是使用最为广泛的参数寻优方法，但是如果误差函数在当前点的梯度为零，则很有可能达到局部极小。</p>
<h2><span id="di-liu-zhang-zhi-chi-xiang-liang-ji">第六章 支持向量机</span><a href="#di-liu-zhang-zhi-chi-xiang-liang-ji" class="header-anchor">#</a></h2><p>支持向量机的学习原理很简单也很有趣，从分类问题，怎么一步一步建立的优化问题，一步一步的完善优化问题以及求解，从硬间隔到软间隔，分类问题是考虑分对，而回归问题希望预测值和原始值尽可能的接近，这样就造成了约束条件，目标性的不同。</p>
<p>最重要的是引入了核方法，低维空间的非线性关系映射成了高维空间线性关系，这是特别重要的思想</p>
<h1><span id="di-ba-zhang-ji-cheng-xue-xi">第八章 集成学习</span><a href="#di-ba-zhang-ji-cheng-xue-xi" class="header-anchor">#</a></h1><h2><span id="ji-ben-si-xiang">基本思想</span><a href="#ji-ben-si-xiang" class="header-anchor">#</a></h2><p>构建一组基学习器（base learner)，在结合</p>
<p>a. 如果集成中是相同类型的个体学习器，如决策树，全是神经网络的集成“同质”（homogeneous),个体学习器叫基学习器</p>
<p>b. 不同的学习器，异质（heterogeneous)，个体学习器叫组件学习器</p>
<h3><span id="wei-shi-me-you-xiao">为什么有效</span><a href="#wei-shi-me-you-xiao" class="header-anchor">#</a></h3><ol>
<li><p>多样性的基学习器</p>
<ol>
<li>不同的模型取长补短</li>
<li>每个基学习器都犯错误，综合起来可能性不大</li>
</ol>
<p>举个栗子</p>
<ol>
<li>也许一个线性模型不能简单分类，但是多个线性模型综合，可将数据集成功分类</li>
</ol>
</li>
</ol>
<h3><span id="gou-jian-bu-tong-de-ji-qi-xue-xi">构建不同的机器学习</span><a href="#gou-jian-bu-tong-de-ji-qi-xue-xi" class="header-anchor">#</a></h3><p>Q 1: 如何建立基学习器</p>
<p>尽量满足多样性</p>
<p>M1: 不同的学习算法</p>
<p>M2: 相同学习算法、不同的参数</p>
<p>M3: 不同的数据集（不同的样本子集、数据集上不同的特征）</p>
<p>homogenous ensemble</p>
<ol>
<li><p>采用相同的学习算法、不同的训练集</p>
<p>Bagging Boosting</p>
</li>
<li><p>相同算法，不同的参数设置</p>
</li>
<li><p>相同的训练集，不同的学习算法</p>
</li>
</ol>
<p>Q2: 如何综合呢？</p>
<ol>
<li><p>t投票法：majority voting</p>
</li>
<li><p>weighted voting</p>
</li>
<li><p>训练一个新模型确定如何综合</p>
<ol>
<li>Stacking</li>
<li>偏好的简单模型</li>
</ol>
</li>
</ol>
<h3><span id="zong-he">综合</span><a href="#zong-he" class="header-anchor">#</a></h3><h2><span id="bagging-boostrap-aggregating">Bagging = Boostrap AGGregatING</span><a href="#bagging-boostrap-aggregating" class="header-anchor">#</a></h2><p>有放回采样，同质学习器</p>
<h4><span id="suan-fa">算法</span><a href="#suan-fa" class="header-anchor">#</a></h4><figure class="highlight tex"><table><tbody><tr><td class="code"><pre><span class="line">Input : </span><br><span class="line">	训练集 D={(x1,y1)}</span><br><span class="line">	基学习算法A</span><br><span class="line">	训练轮数 T</span><br><span class="line">过程</span><br><span class="line">	for t = 1,2,...,T do</span><br><span class="line">		h_t= A(D,Dt) // Dt第t次采样的分布</span><br><span class="line">	end for</span><br><span class="line">输出</span><br><span class="line">	回归：Average</span><br><span class="line">	分类：投票法</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor">#</a></h4><p>没有用于建模的样本，可以用作验证集来对泛化能力进行包外估计，可以得出Bagging泛化误差的包外估计</p>
<h4><span id="random-forest-rf">random forest（RF)</span><a href="#random-forest-rf" class="header-anchor">#</a></h4><p>输入为样本集$D={(x,y1),(x2,y2),…(xm,ym)}$，弱分类器迭代次数T。</p>
<p>输出为最终的强分类器f(x)f(x)</p>
<p>1）对于t=1,2…,T:</p>
<p>a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$Dt$</p>
<p>b)用采样集$Dt$训练第t个决策树模型$Gt(x)$，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分</p>
<p>2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。</p>
<p>参数设置</p>
<p>利用00B样本评估变量的重要性</p>
<h2><span id="boosting-ti-gao">Boosting 提高</span><a href="#boosting-ti-gao" class="header-anchor">#</a></h2><p>顺次建立学习器，就是先从训练集上训练一个基学习器，再根据学习器的表现对训练集分布进行调整，让先学习器错误训练的样本在后续收到更多的关注，然后基于调整的分布训练下一个学习器，最后，在将这T个学习器进行加权结合</p>
<p>基学习器的线性组合</p>
<script type="math/tex; mode=display">
H_N(x;P)=\sum_{t=1}^{N}\alpha_th_t(x;a_t)</script><p>$a_t$是第$i$个弱学习器的最优参数，$\alpha_t$是在强分类器中的比重，$P$是$a_t$和$\alpha_t$的组合</p>
<p>最小化指数损失函数</p>
<script type="math/tex; mode=display">
l_{exp}(H|D)=E_{x~D}[e^{-f(x)H(x)}]</script><script type="math/tex; mode=display">
H_n(x)=H_{n-1}(x)+\alpha_{n}h_{n}(x,a_n)</script><script type="math/tex; mode=display">l(h_i(x,a_t)|D)=E_{x~D}(exp(-f(x)h_i(x)))\\=p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))</script><script type="math/tex; mode=display">\frac{\partial l(h_i(x,a_t)|D)}{\partial h_i(x,a_t)}=\\
-p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))=0</script><script type="math/tex; mode=display">h(x)=\frac{1}{2}ln\frac{P(f(x)=1)}{P(f(x)=-1)}</script><p>采取不同的损失函数，得到不同的类型</p>
<p><a href="https://blog.csdn.net/luanpeng825485697/article/details/79383492">https://blog.csdn.net/luanpeng825485697/article/details/79383492</a></p>
<h3><span id="gbdt">GBDT</span><a href="#gbdt" class="header-anchor">#</a></h3><h2><span id="stacking">Stacking</span><a href="#stacking" class="header-anchor">#</a></h2><p><img src="/2020/07/17/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/MyBlog\hexo\source\_posts\学习の历程-《机器学习》周志华学习记录\Stacking.png" alt></p>
<p>不同学习器，相同数据集</p>
<p>第一层</p>
<p>第二层：不用第一层的数据</p>
<p>可用交叉验证</p>
<p>注意事项：</p>
<p>过拟合问题：第二层线性回归</p>
<p>第一层尽可能的多样性：</p>
<p>综合好的模型</p>
<p>防止过拟合</p>
<pre><code> 1. 随机性
 2. 
</code></pre><p>Bagging Boosting Stacking</p>
<h2><span id="ji-da-si-ran-gu-ji">极大似然估计</span><a href="#ji-da-si-ran-gu-ji" class="header-anchor">#</a></h2><p>似然： 相似的样子</p>
<p>对于一组数据，假设符合正态分布，希望已知点在这个正态分布的情况下，所有点对于的概率之和或者积最大，</p>
<p><img src="/2020/07/17/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/MyBlog\hexo\source\_posts\学习の历程-《机器学习》周志华学习记录\极大似然估计.jpg" alt>，蓝色表示数据，红色就是做得正态分布</p>
<h2><span id="di-shi-zhang-jiang-wei-yu-du-liang-xue-xi">第十章 降维与度量学习</span><a href="#di-shi-zhang-jiang-wei-yu-du-liang-xue-xi" class="header-anchor">#</a></h2><h3><span id="k-jin-lin-xue-xi">k近邻学习</span><a href="#k-jin-lin-xue-xi" class="header-anchor">#</a></h3><p>k-Nearest Neighbor</p>
<p>原理： 基于某种距离度量找出训练集中与其最靠近的k个训练样本，根据k个邻居的信息进行预测。</p>
<p>给定测试样本$x$,如果最邻近样本$z$,最邻近分类器出错的概率就是$x$与$z$不再同一类</p>
<script type="math/tex; mode=display">
p(err) = 1-\sum_{c \in y}p(c|x)P(c|z)</script><h3><span id="di-wei-qian-ru">低维嵌入</span><a href="#di-wei-qian-ru" class="header-anchor">#</a></h3><p>缓解维数灾难的重要途经之一是降维（dimension reduction）这样使得子空间中样本密度大幅度提高，距离计算变得更容易，</p>
<h4><span id="duo-wei-suo-fang-multiple-dimensional-scaling">多维缩放（Multiple Dimensional,Scaling）</span><a href="#duo-wei-suo-fang-multiple-dimensional-scaling" class="header-anchor">#</a></h4><p>MDS</p>
<p>假定m个样本在原始空间的距离矩阵$D$,在低维空间中，两个样本欧式距离等于原空间的距离，$||z_i-z_j|| = dist_{ij}$, 令$B=Z^TZ$为降维后样本的内积矩阵,</p>
<script type="math/tex; mode=display">
dist_{ij}^2=||z_i||^2+||z_j||^2-2z_iz_j=b_{ii}+b_{jj}-2b_{ij}</script><p>对降维后数据中心化，均值为0,$\sum_{i=1}^{m}z_i$,于是乎就有$\sum_{i=1}^{M}b_{ij}=z_j(z_1+z_2+…+z_m)=0=\sum_{j=1}^{m}x_{ij}$</p>
<p>,可得</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{m}dist_{ij}^2=\sum_{i=1}^{m}(b_{ii}+b_{jj}-2b_{ij})=tr(B)_mb_{jj}\\
\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 = 2m tr(B)\\
tr(B)=\sum_{i=1}^{m}||z_i||^2</script><p>可得</p>
<script type="math/tex; mode=display">
b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i.}^2-dist_{.j}^2+dist{..}^2)</script><p>对矩阵B做特征值分解(eigenvalue decomposition)，$B = V \land V$,则</p>
<script type="math/tex; mode=display">
Z = \land_{*}^{1/2}V_{*}</script><p>欲获得低维子空间，最简单是对原始高维空间进行线性变换，$Z = W^TX$,特别的，$W$取正交变换，$W={w_1,w_2,…,w_{d’}}$W是d’个d维基向量，</p>
<h2><span id="zhu-cheng-fen-fen-xi">主成分分析</span><a href="#zhu-cheng-fen-fen-xi" class="header-anchor">#</a></h2><p>Principal Component Analysis ：PCA</p>
<p>在正交空间里面的样本，用一个超平面对样本进行恰当的表达，至少这个样本点满足</p>
<p>最近重构性： 样本点到这个超平面的距离足够近</p>
<p>最大可分性： 样本点在这超平面上的投影尽可能分开</p>
<p>对于最近重构性：</p>
<p>假设样本去中心化，再假设投影变换后得到欣的正交坐标系${w_1,w_2,…,w_d}$,d维空间里面的一组单位正交基，$||w_i||_2=0$,$||w_i^Tw_j||=0$,如果再新坐标系中丢掉一部分坐标，样本点在新坐标的投影是$z_i={w_1^Tx_{i1}},..,w_{d’}^Tx_{i}$,于是又$z_{ij} =w_{j}^Tx_i$,$\hat{x_i}=\sum_{j}^{d’}w_jx_i$</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{m}||\sum_{j=1}^{d'}z_{ij}w_j-x_i||_2^2=\sum_{i=1}^{m}z_i^Tz_i-2\sum_{i=1}^{m}z_i^TW^Tx_i+x_i^Tx_i\\
=\sum_{i=1}^{m}x_i^TWW^Tx_i-2\sum_{i=1}^{m}x_i^TWW^Tx_i+x_i^Tx_i\\
min -\sum_{i=1}^{m}z_i^Tz_i=-tr(Z^TZ)\\
min -tr(\sum_{i=1}^{m}W^Tx_ix_i^TW)=-tr(W^T(\sum_{i=1}^{m}x_i^Tx_i)W）=-tr(W^TXX^TW)\\
s.t W^TW = I</script><p>对于最大可分性$(W^T\hat{X}=0)$</p>
<script type="math/tex; mode=display">
max tr(W^TXX^TW)\\s.t W^TW = I</script><p>根据lagrange</p>
<script type="math/tex; mode=display">
L(W,\lambda)=-tr(W^TXX^TW)-\lambda(W^TW-I)\\
\frac{\partial L}{\partial w_i}=-2w_iXX^T-2\lambda_i w_i=0\\
XX^Tw_i = \lambda w_i</script><p>$XX^T$是协方差矩阵,$\lambda$是特征值，$w_i$是特征向量</p>
<p>特别提示，$x$需要中心化</p>
<p>对于线性PCA降维方法是从高维空间映射到低维空间，$Z= W^TX$,然而不少情况，则需要非线性映射才能找到恰当的低维嵌入， $\phi(x)$</p>
<script type="math/tex; mode=display">
\max tr(\phi(X)\phi(X)^T)=tr( W^T\varphi(x)\varphi(x)^TW)\\
W^TW = I</script><p>于是有</p>
<script type="math/tex; mode=display">
\varphi(x)^T\varphi(x)w_i=\lambda_iw_i\\
w_i=\frac{tr(\varphi(x)^T\varphi(x))}{\lambda_iw_i}</script><script type="math/tex; mode=display">
z_j = \frac{\sum_{i=1}^{m}\varphi(x)^T\varphi(x)}{\lambda_iw_i}\varphi(x_i)\
=\frac{\sum_{i=1}^{m}\varphi(x_i)K(x_i,x)}{\lambda_iw_i}</script><h2><span id="liu-xing-xue-xi-biao-shi-xue-xi-you-dian-kun-nan">流形学习（表示学习有点困难)</span><a href="#liu-xing-xue-xi-biao-shi-xue-xi-you-dian-kun-nan" class="header-anchor">#</a></h2><h1><span id="di-shi-yi-zhang-te-zheng-xuan-ze-yu-xi-shu-xue-xi">第十一章 特征选择与稀疏学习</span><a href="#di-shi-yi-zhang-te-zheng-xuan-ze-yu-xi-shu-xue-xi" class="header-anchor">#</a></h1><p>对于一个学习任务，对任务有用的特征,称为”relevant feature”，对于没有用的属性”irrelevant feature”,因此从给定特征集选择出相关特征子集的过程，特征选择（feature selection),原因一，降维；原因二：降低学习的任务。</p>
<p>无关特征，包括一类冗余特征（redundant feature），能够从其他特征里面推演出来。</p>
<h2><span id="te-zheng-sou-suo">特征搜索</span><a href="#te-zheng-sou-suo" class="header-anchor">#</a></h2><h3><span id="qian-xiang-forward-sou-suo">前向（forward)搜索</span><a href="#qian-xiang-forward-sou-suo" class="header-anchor">#</a></h3><p>对于特征集合$\{a_1,a_2,…,a_d \}$,每个特征看作一个候选集，对这$d$候选的单特征子集进行评价，可选出最优子集，然后，再下一轮子集中，构成了两个特征候选的子集，</p>
<h2><span id="hou-xiang-backward-sou-suo">后向 (backward) 搜索</span><a href="#hou-xiang-backward-sou-suo" class="header-anchor">#</a></h2><p>每次尝试去掉一个无关特征</p>
<h3><span id="shuang-xiang-bidirectional-sou-suo">双向(bidirectional)搜索</span><a href="#shuang-xiang-bidirectional-sou-suo" class="header-anchor">#</a></h3><p>上述操作只是贪心策略，仅仅考虑了本轮选定集合最优</p>
<p>​            </p>
<h2><span id="zi-ji-ping-jie-subset-evaluation">子集评价（subset evaluation)</span><a href="#zi-ji-ping-jie-subset-evaluation" class="header-anchor">#</a></h2><p>已知一个数据集$D$,假定第$i$类样本所占比例$p_i$,对于属性子集$A$,假设根据取值D分成V个子集$\{D^1,D^2,…,D^V\}$,则子集A的信心 增益</p>
<script type="math/tex; mode=display">
Gain(A) = Ent(D)-\sum_{i=1}^V\frac{|D^i|}{|D|}Ent(D^i)\\
Ent(D)=\sum_{i=1}^{|y|}p_ilog^{-p_i}</script><p>​    </p>
<p>信息增益Gain(A)越大，说明特征子集A包含的有助于分类的信息越多，特征子集A是对数据集D的一个划分，样本D的标记信息Y则对应着D的真实划分，就能对A进行评价，对Y对应的划分的差异越小，则说明A越好，</p>
<h2><span id="guo-lu-shi-xuan-ze">过滤式选择</span><a href="#guo-lu-shi-xuan-ze" class="header-anchor">#</a></h2><p>Relief （Relevant Feature） </p>
<p>设计一个“相关统计量”来描述度量特征的重要性，该统计量是一个向量，每个分量对应一个初式特征，而特征子集的重要性则是每个特征对应统计量分量之和来决定，最终只需指定一个阙值，根据阙值选择统计量分量对应的特征即可</p>
<p>如何确定相关统计量</p>
<p>给定训练集$(x_i,y_i)$,对于实例$x_i$,在其同类样本中找最近邻（near-hit),在从异类样本中寻找其最近邻$x_{x,nm}$称为“猜错近邻”，</p>
<script type="math/tex; mode=display">
\delta^j =\sum_i-diff(x_i^j,x_{i.nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2</script><p>分值越大，说明对应属性的分类能力越强</p>
<p>对于多分类问题</p>
<script type="math/tex; mode=display">
\delta^j = \sum_i-diff(x_i^j,x_{i,nh}^j)^2+\sum_{l \neq k}p_l\ diff(x_i^j,x_{i,l,nm}^j)</script><p>这种方法看一个属性（特征）重不重要，先计算出每个属性的统计分量，按照公式，子集的评价就是对于分量的和</p>
<h2><span id="bao-guo-shi-xuan-ze">包裹式选择</span><a href="#bao-guo-shi-xuan-ze" class="header-anchor">#</a></h2><p>直接把最终将要使用的学习器的性能作为特征子集的评价准则，特征选择的目的就是为给定学习期选择有利其性能的特征子集。</p>
<p>LVW（Las Vegas Wrapper）是典型的包裹式特征选择方法，拉斯维加斯方法（Las Vegas method）框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则</p>
<p>算法</p>
<h2><span id="qian-ru-shi-xuan-ze">嵌入式选择</span><a href="#qian-ru-shi-xuan-ze" class="header-anchor">#</a></h2><p>学习器自动地进行特征选择</p>
<p>L-P范数</p>
<script type="math/tex; mode=display">
L_P = ||X||_P = p\sqrt{\sum_{i=1}^{n}x_i^p}</script><p><img src="https://img-blog.csdn.net/20160623222921977" alt="这里写图片描述"></p>
<p>L0范数</p>
<script type="math/tex; mode=display">
||X||_0=向量中非零元素的个数</script><p>L1范数</p>
<script type="math/tex; mode=display">
||x||_1 = \sum|x_i|</script><p>L2范数，最常用</p>
<script type="math/tex; mode=display">
||X||_2=\sqrt{x_i^2}</script><p>无穷范数</p>
<script type="math/tex; mode=display">
||x||=max|x_i|</script><p>对于线性回归模型，防止过拟合，如果使用L2,称为岭回归(ridge regression),如果采取L1范数，则有称为LASSO，L1比L2更易于稀疏解，可以看得出L1范数正则化的过程得到了仅采用一部分初始化特征的模型。</p>
<p>L1正则化求解可使用近端梯度下降法(Proximal Gradient Descent)PGD</p>
<p><strong>L-Lipschitz条件</strong></p>
<p>设函数$Φ(x)$在有限 区间$[a,b]$上满足如下条件：</p>
<p>(1) 当$x∈[a,b]$时，$Φ(x)∈[a,b]$，即$a≤Φ(x)≤b$.</p>
<p>(2) 对任意的$x1，x2∈[a,b]$， 恒成立：$|Φ(x1)-Φ(x2)|≤L|x1-x2|$.</p>
<p>如果$f(x)$可导，并且$\nabla f$满足L-Lipschitz条件，</p>
<script type="math/tex; mode=display">
||\nabla f(x')-\nabla f(x)||_2^2<L||x'-x||_2^2</script><p>在$x_k$附近</p>
<script type="math/tex; mode=display">
\hat{f}(x)=f(x_k)+f^{'}(x_k)(x-x_k)+\frac{L}{2}||x-x_k||^2\\
=\frac{L}{2}||x-(x_k)-\frac{1}{L}\nabla f(x_k)||_2^2+const</script><p>可知</p>
<script type="math/tex; mode=display">
x_{k+1}=x_k-\frac{1}{L}\nabla f(x_k)</script><p>对于原始问题，可先计算$z=x_k-\frac{1}{L}\nabla f(x_k)$,</p>
<p>$x_{k+1}=arg \ \ min_{x} \frac{L}{2}||x-z||_2^2+\lambda||x||_1$</p>
<p>由于各个分量相互不影响</p>
<script type="math/tex; mode=display">
x_{k+1}^i=\begin{cases}z^i-\frac{\lambda}{L}, \frac{\lambda}{L}<z^i\\
0, |z^i| <= \frac{\lambda}{L} \\
z^i+\frac{\lambda}{L},z^i<-\frac{\lambda}{L}\end{cases}</script><h2><span id="xi-shu-xue-xi">稀疏学习</span><a href="#xi-shu-xue-xi" class="header-anchor">#</a></h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title>PageRank算法</title>
    <url>/2020/07/15/PageRank%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>PageRank是一种网页排序算法，基于页面的质量和数量。可应用于评估网页节点重要性。</p>
<a id="more"></a>
<h1><span id="pagerank-suan-fa">PageRank算法</span><a href="#pagerank-suan-fa" class="header-anchor">#</a></h1><p>PageRank,即<strong><em>\</em>网页排名**</strong>，又称<strong>网页级别</strong>、<strong>Google左侧排名</strong>或<strong>佩奇排名。</strong>PageRank是Google用于用来标识网页的等级/重要性的一种方法，是Google用来衡量一个网站的好坏的唯一标准。</p>
<p>假设</p>
<ol>
<li>数量假设: 如果一个页面节点入链数量越多，则这个页码越重要。</li>
<li>质量假设：指向页面A的入链质量不同，考虑权重的影响，则这个页面越是重要。</li>
</ol>
<h2><span id="suan-fa-qiu-jie">算法求解</span><a href="#suan-fa-qiu-jie" class="header-anchor">#</a></h2><ol>
<li>第一阶段：通过网页链接关系构建起Web图，初始每个页面相同的PageRank值，再通过若干轮得到每个页面的最终pagerank.</li>
<li>每一轮更新页面PageRank得分的计算方法</li>
</ol>
<h3><span id="quan-chong">权重</span><a href="#quan-chong" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
PR(T)/L(T)\\
where PR(T)的PageRank值，L(T)为T的出链数目</script><h3><span id="xiu-zheng">修正</span><a href="#xiu-zheng" class="header-anchor">#</a></h3><p>$L(T)$为0的情况，孤立网页，使得很多网页能被访问到。$q = 0.85$</p>
<script type="math/tex; mode=display">
PR(A) = (\frac{PR(B)}{L(B)}+\frac{PR(C)}{L(C)}+\dots)q+1-q</script><h2><span id="qi-ta-wang-luo-shu-xing-du-liang-fang-fa">其他网络属性度量方法</span><a href="#qi-ta-wang-luo-shu-xing-du-liang-fang-fa" class="header-anchor">#</a></h2><p>Centrality indices:  degree, betweenness, and closeness.</p>
<h2><span id="reference">reference</span><a href="#reference" class="header-anchor">#</a></h2><p>提出者： The anatomy of a large-scale hypertextual Web search engine  </p>
<p><a href="https://en.wikipedia.org/wiki/PageRank">https://en.wikipedia.org/wiki/PageRank</a></p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>R语言</title>
    <url>/2020/07/03/R%E8%AF%AD%E8%A8%80/</url>
    <content><![CDATA[<p><a href="https://bookdown.org/qiyuandong/intro_r/-r-basics-2.html#section-3.3">https://bookdown.org/qiyuandong/intro_r/-r-basics-2.html#section-3.3</a></p>
<p>入门： <a href="https://rc2e.com/">https://rc2e.com/</a></p>
<p><a href="http://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/intro.html">http://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/intro.html</a></p>
<p>全面：</p>
<p><a href="https://github.com/harryprince/R-Tutor">https://github.com/harryprince/R-Tutor</a></p>
<p>视频：</p>
<p>中文： <a href="https://www.youtube.com/watch?v=rPj5FsTRboE">https://www.youtube.com/watch?v=rPj5FsTRboE</a></p>
<p>英文：<a href="https://www.youtube.com/watch?v=32o0DnuRjfg">https://www.youtube.com/watch?v=32o0DnuRjfg</a></p>
<p>这个教程好： <a href="https://sites.google.com/site/econometricsacademy/econometrics-models/linear-regression">https://sites.google.com/site/econometricsacademy/econometrics-models/linear-regression</a></p>
<p><a href="https://www.youtube.com/watch?v=YMt5K68ZvjQ&amp;list=PLRW9kMvtNZOh7Xt1m5Mlhhz2wtr0tCUEE">https://www.youtube.com/watch?v=YMt5K68ZvjQ&amp;list=PLRW9kMvtNZOh7Xt1m5Mlhhz2wtr0tCUEE</a></p>
]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>Xgboost</title>
    <url>/2020/07/03/Xgboost/</url>
    <content><![CDATA[<h1><span id="li-lun-bu-fen">理论部分</span><a href="#li-lun-bu-fen" class="header-anchor">#</a></h1><p>该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次<strong>预测的残差</strong>。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。</p>
<a id="more"></a>
<p>boosting: <a href="https://zhuanlan.zhihu.com/p/38329631">https://zhuanlan.zhihu.com/p/38329631</a></p>
<p>Xgboost 就是回归树的集成</p>
<p><a href="https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/">https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/</a></p>
<p><a href="https://blog.csdn.net/github_38414650/article/details/76061893?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare">https://blog.csdn.net/github_38414650/article/details/76061893?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare</a></p>
<p><a href="https://blog.csdn.net/qq_24519677/article/details/81809157">https://blog.csdn.net/qq_24519677/article/details/81809157</a></p>
<p>有空再推导了</p>
<h1><span id="diao-yong-ku">调用库</span><a href="#diao-yong-ku" class="header-anchor">#</a></h1><p>Python 提供了两种库</p>
<ol>
<li>xgboost</li>
<li>xgboost sklearn接口</li>
</ol>
<p>搭建模型</p>
<ol>
<li><p>参数设置</p>
</li>
<li><p>GridSearchCV 调参(网格法)</p>
<ol>
<li>调参步骤，参数范围</li>
</ol>
</li>
</ol>
<p><a href="https://blog.csdn.net/han_xiaoyang/article/details/52665396">https://blog.csdn.net/han_xiaoyang/article/details/52665396</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error,make_scorer</span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> KFold, train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://blog.csdn.net/s09094031/article/details/94871596?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare">https://blog.csdn.net/s09094031/article/details/94871596?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare</a></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sklearn.model_selection.``train_test_split</span><br></pre></td></tr></tbody></table></figure>
<p>test_size train_size：</p>
<p>​    三种类型。float，int，None。</p>
<ul>
<li>float：0.0-1.0之间，代表训练数据集占总数据集的比例。</li>
<li>int：代表训练数据集具体的样本数量。</li>
<li>None：设置为test_size的补。</li>
<li>default：默认为None。</li>
</ul>
<p>random_state：三种类型。int，randomstate instance，None。</p>
<p>int：是随机数生成器的种子。每次分配的数据相同。</p>
<p>randomstate：random_state是随机数生成器的种子。（这里没太理解）</p>
<p>None：随机数生成器是使用了np.random的randomstate。</p>
<p>种子相同，产生的随机数就相同。种子不同，即使是不同的实例，产生的种子也不相同。</p>
<p>shuffle：布尔值，可选参数。默认是None。在划分数据之前先打乱数据。如果shuffle=FALSE，则stratify必须是None。</p>
<p>stratify：array-like或者None，默认是None。如果不是None，将会利用数据的标签将数据分层划分。</p>
<p>若为None时，划分出来的测试集或训练集中，其类标签的比例也是随机的。</p>
<p>若不为None时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集。</p>
<p>x_train, y_train, x_test, y_test = train_test_split(x, y,<br>                                                    test_size=0.23, random_state=2)</p>
<p><a href="https://blog.csdn.net/qq_43288098/article/details/105407204?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare">https://blog.csdn.net/qq_43288098/article/details/105407204?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare</a></p>
<p>参数：分开调</p>
<p><a href="https://blog.csdn.net/zc02051126/article/details/46711047">https://blog.csdn.net/zc02051126/article/details/46711047</a></p>
<p><a href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a></p>
<p><a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn">https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn</a></p>
<h2><span id="mo-xing-bao-cun">模型保存</span><a href="#mo-xing-bao-cun" class="header-anchor">#</a></h2><p><a href="https://www.fatrabbids.com/2018/10/19/xgboost的保存模型、加载模型、继续训练/#more-235">https://www.fatrabbids.com/2018/10/19/xgboost%e7%9a%84%e4%bf%9d%e5%ad%98%e6%a8%a1%e5%9e%8b%e3%80%81%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b%e3%80%81%e7%bb%a7%e7%bb%ad%e8%ae%ad%e7%bb%83/#more-235</a></p>
<h1><span id="xgboost-de-te-xing-chong-yao-xing-he-te-xing-xuan-ze">XGBoost的特性重要性和特性选择</span><a href="#xgboost-de-te-xing-chong-yao-xing-he-te-xing-xuan-ze" class="header-anchor">#</a></h1><ol>
<li><p>模型复杂度</p>
<p>特征数量衡量：特征重要性阙值的增加，选择特征数量减少，模型的准确率会下降。当然，特征数量的减少反而会是准确率升高，因为这些被剔除特征是噪声。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>English-Daily</title>
    <url>/2020/06/23/English-Daily/</url>
    <content><![CDATA[<a id="more"></a>
<h1><span id="2020-7-6">2020-7-6</span><a href="#2020-7-6" class="header-anchor">#</a></h1><p>coincide with v. 与…相符</p>
<p>stalk v. 潜近（猎物或人）；（非法）跟踪；怒冲冲地走；趾高气扬地走</p>
<p>n. 秆；柄；（叶）柄；（花）梗</p>
<p>verge</p>
<p>Bella was on the verge of tears when she heard the news.</p>
<p>听到这个消息时，贝拉差点就要哭了。</p>
<p>resistant adj. 抵制的，反抗的，抗拒的；有抵抗力的；抵抗…的；不受……损害的</p>
<p>People are usually resistant to change.</p>
<p>人们通常抗拒改变。</p>
<p>liar</p>
<p>The tall guy was a notorious liar.</p>
<p>那个高个子是个臭名昭著的骗子。</p>
<p>politics</p>
<p>n. 政治；政治事物（活动）；政见；权术</p>
<p>oblige (以法律、义务等)强迫, 迫使; 帮忙, 效劳; [常用被动]使感激; 使(行为等)成为必要</p>
<p>phrase. (feel obliged to do sth.)觉得有义务做；不得不做</p>
<p>I felt obliged to leave after such an unpleasant quarrel.</p>
<p>发生了这样不愉快的争吵之后，我觉得有必要离开。</p>
<h1><span id="2020-7-1">2020-7-1</span><a href="#2020-7-1" class="header-anchor">#</a></h1><p>jelly n. 果冻；肉冻；果酱；胶状物，胶凝物；轻便塑料鞋</p>
<p>oval adj. 椭圆形的；卵形的 n. 椭圆形；卵形</p>
<p>rigorous /‘rɪɡərəs/ adj. 谨慎的，细致的；严格的，严厉的</p>
<p>He makes a rigorous study of the plants in the area.</p>
<p>他对该地的植物进行了缜密的研究。</p>
<p>ultimately  UK/‘ʌltɪmətli/ adv. 最终, 最后, 归根结底, 终究</p>
<p>Everything will ultimately depend on what is said at the meeting.</p>
<p>一切将最终取决于会议的内容。</p>
<p><strong>sturdy</strong> UK/‘stɜːdi/</p>
<p>adj. 结实的，坚固的；强壮的；健壮的；坚决的，顽强的</p>
<p><strong>broaden</strong> UK/‘brɔːdn/</p>
<p>You should broaden your experience by travelling more.</p>
<p>你应该多到各地走走以增广见识.</p>
<p><strong>broaden</strong> the horizon 开拓视野</p>
<p>propel UK/prə’pel/ v. 推进，推动；驱使；迫使</p>
<p><strong>voyage</strong>  UK/‘vɔɪɪdʒ/</p>
<p>n. 航行, （尤指）航海</p>
<p>v. 航行, 远行, （尤指）远航</p>
<p>例句</p>
<p>The voyage from England to India used to take 3 weeks.</p>
<p>从英格兰到印度的航行曾经需要三周。</p>
<h1><span id="2020-6-28">2020-6-28</span><a href="#2020-6-28" class="header-anchor">#</a></h1><p>moist  UK/mɔɪst/ adj. 微湿的, 湿润的</p>
<p>insult UK/ɪn’sʌlt/v. 侮辱，辱骂 n. 侮辱，辱骂</p>
<p>spontaneous UK/spɒn’teɪniəs/</p>
<p>They greeted him with spontaneous applause.</p>
<p>他们自发地鼓起掌来欢迎他。</p>
<p>slender  UK/‘slendə(r)/</p>
<p>perimeter UK/pə’rɪmɪtə(r)/ n. 周长；外缘，边缘</p>
<p>blouse UK/blaʊz/</p>
<p>He pointed out a woman passing by who was wearing a skirt and blouse.</p>
<p>他指出了一个穿着裙子和衬衫的过路女子。</p>
<p>perfume UK/‘pɜːfjuːm/ n. 香水, 香料, 芳香 v. 使…发出香气, 洒香水</p>
<h1><span id="2020-6-27">2020-6-27</span><a href="#2020-6-27" class="header-anchor">#</a></h1><h1><span id="2020-6-26">2020-6-26</span><a href="#2020-6-26" class="header-anchor">#</a></h1><p>Functional foods are food products that have a potentially <strong>positive effect on</strong> health beyond basic nutritional benefits. Functional foods <strong>aim to solve not only</strong> all the needs that regular foods provide, <strong>but also to address</strong> functional needs, <strong>which can range from</strong> maintaining and improving physical or mental health <strong>to</strong> adjusting energy levels and moods.</p>
<p>Food has been historically used as preventive medicine in many cultures around the world, but the recent rise of functional foods can be directly linked to the rise of the wellness economy, which, in turn, is largely <strong>driven by</strong> influencer marketing and social media use.</p>
<h1><span id="2020-6-25">2020-6-25</span><a href="#2020-6-25" class="header-anchor">#</a></h1><p>IT IS A truth universally acknowledged that inequality（不平等）in the rich world（发达国家）is high and rising. Or, at least, it used to be. A growing band of economists are challenging the received（被公认的）wisdom, pointing out that trends in the distribution（分布，分配）of income and wealth may not be as bad as is often thought.</p>
<p>众所周知，富裕国家的不平等现象非常严重，而且还在加剧。或者说，至少曾经是这样的。越来越多的经济学家开始质疑既有的观点，他们指出收入和财富的分布趋势可能不是像通常被认为的那么糟糕。</p>
<h2><span id="2020-6-24">2020-6-24</span><a href="#2020-6-24" class="header-anchor">#</a></h2><p>imaginary</p>
<p>adj. 想象中的, 幻想的, 虚构的</p>
<p>carriage</p>
<p>n. 运输；运费，（旧时）马车；火车车厢；仪态，姿态，举止</p>
<p>message</p>
<p>messenger </p>
<p>n. 信使, 送信人, 通信员, 邮递员</p>
<p>pavement</p>
<p>n. 人行道</p>
<p>postpone</p>
<p>v. 延期, 延迟, 暂缓</p>
<p>We’ll have to postpone the meeting until next week.</p>
<p>我们将不得不把会议推迟到下周举行。</p>
<p>velocity</p>
<p>n. 速度，速率；高速</p>
<p>reconcile</p>
<p>v. 使和谐一致，调和；使和解；将就，妥协</p>
<p>It’s difficult to reconcile these two different points of view.</p>
<p>很难兼顾这两种不同的观点。</p>
<h1><span id="2020-6-23">2020-6-23</span><a href="#2020-6-23" class="header-anchor">#</a></h1><p>￼The success of the brand <strong>wasn’t</strong> built through big marketing campaigns, <strong>but</strong> through a savvy digital marketing strategy <strong>that</strong> <strong>increased</strong> brand awareness and <strong>generated</strong> high engagement, traffic, and conversions.  </p>
<p>该品牌的成功并不建立于大型营销活动，而是建立于精准的数字营销策略，该策略提高了品牌的知名度，获得了很高的参与度、流量和转化率。</p>
<p>traffic: 信息流量，通信量</p>
<p>With only 40 physical stores, which are mostly used to drive consumers to e-commerce portals, Perfect Diary maintains momentum primarily through its digital footprint. Currently, it has a powerful presence on Little Red Book, Bilibili, Weibo, WeChat, Tmall, and Douyin.  </p>
<p><strong>Thereafter</strong> she wrote articles for papers and magazines for a living.</p>
<p>此后她给报纸和杂志撰稿谋生。</p>
<p><strong>adv. 此后, 之后, 以后</strong></p>
<p><strong>spur</strong></p>
<p>n. 刺激, 激励, 鞭策; 踢马刺, 靴刺; 骨刺; 山嘴, 尖坡</p>
<p>v. 刺激, 激励, 促进, 鞭策</p>
<p><strong>stick</strong></p>
<p>adj. 黏（性）的, 一面带黏胶的, 闷热的, 感到热得难受的</p>
<p>n. 告事贴</p>
<p>I have to take a shower before going out because the sweat had made my skin sticky.</p>
<p>出门前我得冲个澡，因为汗水让我的皮肤黏乎乎的</p>
<p><strong>devotion</strong></p>
<p>n. 关爱，关照；奉献；忠诚；宗教礼拜</p>
<p>The career needs our devotion for all our lives.</p>
<p>这项事业需要我们毕生的奉献。</p>
<p><strong>reckless</strong></p>
<p>adj. 鲁莽的；不计后果的；无所顾忌的</p>
<p><strong>wag</strong></p>
<p>v. 摇动；摆（尾巴），（尾巴）摇，摆动</p>
<p>n. 摇摆，摆动；老开玩笑的人，爱闹着玩的人</p>
<p><strong>keen</strong></p>
<p>adj. 热衷的, 热情的; 渴望的; 敏捷的; 灵敏的; 锋利的; 强烈的</p>
<p>n. 恸哭; 挽歌</p>
<p>v. (为死者)恸哭</p>
<p>be keen on sth对 感兴趣 be keen to do 渴望做某事</p>
<p><strong>offspring</strong></p>
<p>n. 子女，后代；幼崽；幼苗</p>
<p><strong>receipt</strong></p>
<p>n. 收据，收入</p>
]]></content>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title>day</title>
    <url>/2020/06/22/time-series-01/</url>
    <content><![CDATA[<h1><span id="shi-jian-xu-lie-ji-qi-fen-jie">时间序列及其分解</span><a href="#shi-jian-xu-lie-ji-qi-fen-jie" class="header-anchor">#</a></h1><a id="more"></a>
<h2><span id="shi-jian-xu-lie-fen-lei">时间序列分类</span><a href="#shi-jian-xu-lie-fen-lei" class="header-anchor">#</a></h2><h3><span id="ping-wen-xu-lie-stationary-series">平稳序列（stationary series)</span><a href="#ping-wen-xu-lie-stationary-series" class="header-anchor">#</a></h3><p>序列中的各观察值基本上在某个固定的水平上波动，在不同时F间段波动程度不同，但不存在某种规律。平稳性时间序列的均值和方差都是常数。</p>
<p>方法：a) 看原图。是否在某个常数附近波动，且波动范围有界。如果有明显的趋势性或者周期性，则不是。b) ADF单位根检测。p值。</p>
<h3><span id="fei-ping-wen-xu-lie-non-stationary-series">非平稳序列（non-stationary series)</span><a href="#fei-ping-wen-xu-lie-non-stationary-series" class="header-anchor">#</a></h3><p>涉及趋势、季节性和周期三种特性，包含其中一种或者多种成分。</p>
<h4><span id="qu-shi-trend">趋势(trend)</span><a href="#qu-shi-trend" class="header-anchor">#</a></h4><p>时间序列在长时期内呈现出来的某种上升或者下降的趋势。分为线性和非线性。</p>
<h4><span id="ji-jie-xing-seasonality">季节性（seasonality)</span><a href="#ji-jie-xing-seasonality" class="header-anchor">#</a></h4><p>是指时间序列在一年内重复出现的周期波动。因季节不同而发生变化，如旅游旺季，旅游淡季。</p>
<h4><span id="zhou-qi-xing-cyclicity">周期性（cyclicity）</span><a href="#zhou-qi-xing-cyclicity" class="header-anchor">#</a></h4><p>是指时间序列呈现出的长期趋势。周期性不同于趋势变动，它是涨落相间的交替波动。不同意季节变动，它无固定规律，变动周期多在一年以上，且周期长短不一。周期性通常是由经济环境的变化引起的。</p>
<h4><span id="ou-ran-xing-yin-su">偶然性因素</span><a href="#ou-ran-xing-yin-su" class="header-anchor">#</a></h4><p>其导致时间序列呈现出某种随机波动。</p>
<p>时间序列的成分可分为：趋势（T),季节性（S),周期性（C),随机性（I)。</p>
<h2><span id="ping-wen-shi-jian-xu-lie-fen-xi">平稳时间序列分析</span><a href="#ping-wen-shi-jian-xu-lie-fen-xi" class="header-anchor">#</a></h2><h3><span id="ar-mo-xing">AR模型</span><a href="#ar-mo-xing" class="header-anchor">#</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">自回归模型AR</th>
<th style="text-align:left">自回归模型描述当前值与历史值之间的关系，用变量自身的历史时间数据对自身进行预测。自回归模型必须满足平稳性的要求。</th>
<th style="text-align:left"><img src="https://xiemaycherry.github.io/2020/06/22/time-series-01/F:%5C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A1%B9%E7%9B%AE%5C2020-5-19.assets%5Car.png" alt="img"></th>
<th style="text-align:left"></th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">移动平均模型MA</td>
<td style="text-align:left">移动平均模型关注的是自回归模型中的误差项的累加</td>
<td style="text-align:left"><img src="https://xiemaycherry.github.io/2020/06/22/time-series-01/F:%5C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A1%B9%E7%9B%AE%5C2020-5-19.assets%5Cma.png" alt="img"></td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">自回归移动平均模型ARMA</td>
<td style="text-align:left">自回归模型AR和移动平均模型MA模型相结合，我们就得到了自回归移动平均模型ARMA(p,q)</td>
<td style="text-align:left"><img src="https://xiemaycherry.github.io/2020/06/22/time-series-01/F:%5C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A1%B9%E7%9B%AE%5C2020-5-19.assets%5Cauma.png" alt="img"></td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">差分自回归移动平均模型ARIMA</td>
<td style="text-align:left">将自回归模型、移动平均模型和差分法结合，我们就得到了差分自回归移动平均模型ARIMA(p,d,q)</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
</div>
<h3><span id="can-shu-que-ding">参数确定</span><a href="#can-shu-que-ding" class="header-anchor">#</a></h3><p><strong>拖尾和截尾</strong><br>拖尾指序列以指数率单调递减或震荡衰减，而截尾指序列从某个时点变得非常小。</p>
<p><img src="https://xiemaycherry.github.io/2020/06/22/time-series-01/K:%5CMyBlog%5Chexo%5Csource_posts%5Ctime-series-01%5Ctuowei.png" alt="img"></p>
<h3><span id="arima-jian-mo-guo-cheng">ARIMA建模过程</span><a href="#arima-jian-mo-guo-cheng" class="header-anchor">#</a></h3><ol>
<li>将序列平稳（差分法确定d）</li>
<li>p和q阶数确定：ACF与PACF</li>
<li>ARIMA（p,d,q）</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">模型</th>
<th style="text-align:left">ACF</th>
<th style="text-align:left">PACF</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AR（p）</td>
<td style="text-align:left">衰减趋于零（几何型或振荡型）</td>
<td style="text-align:left">p阶后截尾</td>
</tr>
<tr>
<td style="text-align:left">MA（q）</td>
<td style="text-align:left">q阶后截尾</td>
<td style="text-align:left">衰减趋于零（几何型或振荡型）</td>
</tr>
<tr>
<td style="text-align:left">ARMA（p,q）</td>
<td style="text-align:left">q阶后衰减趋于零（几何型或振荡型）</td>
<td style="text-align:left">p阶后衰减趋于零（几何型或振荡型）</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="can-shu-p-q-de-zi-dong-que-ding-fang-shi">参数 p,q 的自动确定方式</span><a href="#can-shu-p-q-de-zi-dong-que-ding-fang-shi" class="header-anchor">#</a></h4><h5><span id="xin-xi-zhun-ze">信息准则</span><a href="#xin-xi-zhun-ze" class="header-anchor">#</a></h5><p>在参数估计的时候，我们可以采用似然函数作为目标函数。可以通过加入模型复杂度的惩罚项避免过拟合问题。比如赤池信息准则（AIC)和贝叶斯信息准则(BIC)</p>
<script type="math/tex; mode=display">
AIC=2k−2ln(L)</script><p>一方面引入惩罚项，使得模型参数尽快少，减少过拟合。另一方面，也希望提高模型的拟合度（极大似然）</p>
<script type="math/tex; mode=display">
BIC=kLn(n)−2ln(L)</script><p>k为模型参数个数，n为样本数量，L为似然函数。引入$Kln(n)$惩罚项在维度过大且样本数据相对较少的情况下，可以有效避免出现维度灾难。</p>
<h2><span id="shi-jian-xu-lie-de-fen-jie">时间序列的分解</span><a href="#shi-jian-xu-lie-de-fen-jie" class="header-anchor">#</a></h2><p>加法模型</p>
<script type="math/tex; mode=display">
X_t = T_t + C_t+S_t + I_t ,t = 1,2,..,n</script><p>每个时间序列看成是三个部分的叠加，分别是趋势项、循环项，季节项，随机项</p>
<p>乘法模型</p>
<script type="math/tex; mode=display">
X_t = T_t*C_t*S_t*I_t</script><h3><span id="qu-shi-fen-xi">趋势分析</span><a href="#qu-shi-fen-xi" class="header-anchor">#</a></h3><p>趋势拟合法就是把时间作为自变量，相应的序列观察值作为因变量，建立序列值随时间变化的回归模型。可分为线性拟合和曲线拟合。</p>
<h4><span id="xian-xing-ni-he">线性拟合</span><a href="#xian-xing-ni-he" class="header-anchor">#</a></h4><p>如果长期趋势呈现出线性特征，可用线性模型拟合，</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}
x_t = a+bt+I_t\\
E(I_t) = 0,Var(I_t) = \sigma^2
\end{array}
\right.</script><p>其中，$T_t = a+bt$就是消除随机波动影响后的该序列的长期趋势。</p>
<h4><span id="qu-xian-ni-he">曲线拟合</span><a href="#qu-xian-ni-he" class="header-anchor">#</a></h4><p>如果长期趋势呈现出线性特征，可用曲线模型来拟合</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{c|c|c}
二次型& T_t = a+bt+ct^2& 变换后，线性最小二乘法\\
指数型&T_t = ab^t& 对数变化 & 最小二乘法\\
修正指数型&T_t = a+bc^t&  &迭代法\\
Gompertz型& T_t = e^{a+bc^t}& & 迭代法\\
Logistic & T_t = \frac{1}{a+bc^t}& 迭代法
\end{array}
\right.</script><h4><span id="ping-hua-fa">平滑法</span><a href="#ping-hua-fa" class="header-anchor">#</a></h4><h4><span id="yi-dong-ping-jun-fa">移动平均法</span><a href="#yi-dong-ping-jun-fa" class="header-anchor">#</a></h4><p>假设在比较短的时间间隔里，序列的取值是较稳定的，这种差异是由随机波动造成的。由此，可用一定时间间隔内的平均值作为某一期的估计值。</p>
<p>n期中心移动平均</p>
<script type="math/tex; mode=display">
\widetilde{x_t} = \frac{1}{n}(\frac{1}{2}x_{t-\frac{n}{2}}+x_{t-\frac{n}{2}+1}+\dots+x_{t+\frac{n}{2}-1}+\frac{1}{2}x_{t+\frac{n}{2}})</script><p>n期移动平均</p>
<script type="math/tex; mode=display">
\widetilde{x_t} = \frac{1}{n}(x_t+x_{t-1}+\dots+x_{t-n+1})</script><h4><span id="zhi-shu-ping-hua-fa">指数平滑法</span><a href="#zhi-shu-ping-hua-fa" class="header-anchor">#</a></h4><p>简单指数平滑</p>
<script type="math/tex; mode=display">
\widetilde{x_t} = \alpha x_t+\alpha (1-\alpha )x_{t-1}+\dots)</script><h3><span id="ji-jie-xiao-ying">季节效应</span><a href="#ji-jie-xiao-ying" class="header-anchor">#</a></h3><p>季节性效应的存在，使得气温会在不同年份的相同月份呈现出相似的性质。</p>
<p>如果只是存在季节性和随机波动性</p>
<script type="math/tex; mode=display">
x_{ij} = \hat{x}S_j+I_{ij}</script><p>其中$S_j$表示第j个月的季节指数，$\hat{x}$为各月平均气温。</p>
<p>季节指数的计算:</p>
<p>Step1: 计算周期内各期的平均数</p>
<script type="math/tex; mode=display">
\hat{x}_k = \frac{\sum_{i= 1}^{n}x_{ik}}{n}（k = 1,2,...,m)</script><p>其中，m表示周期，n表示周期的数量</p>
<p>Step2: 计算总平均数</p>
<script type="math/tex; mode=display">
\hat{x} = \frac{\sum_{i = 1}^{n}\sum_{k = 1}^{m}x_{ik}}{nm}</script><p>Step3: 计算季节指数</p>
<script type="math/tex; mode=display">
S_k = \frac{\hat{x}_k}{\hat{x}}</script><h3><span id="hun-he-xiao-ying">混合效应</span><a href="#hun-he-xiao-ying" class="header-anchor">#</a></h3><p>加法模型</p>
<script type="math/tex; mode=display">
x_t = T_t + S_t + I_t</script><p>乘法模型</p>
<script type="math/tex; mode=display">
x_t = T_t*S_t*I_t</script><p>混合模型</p>
<script type="math/tex; mode=display">
x_t = S_t*T_t+I_t\\
x_t = S_t*(T_t+I_t)</script><p>如果季节波动的振幅不受趋势变动的影响，则说明季节性与趋势之间没有相互作用关系，可加。如果季节波动的振幅随趋势的变化而变化，是相互作用的关系，可尝试混合模型和乘法模型。</p>
<h1><span id="tool-in-python-xfresh">Tool in Python: xfresh</span><a href="#tool-in-python-xfresh" class="header-anchor">#</a></h1><h2><span id="te-zheng-ti-qu">特征提取</span><a href="#te-zheng-ti-qu" class="header-anchor">#</a></h2><p>官网： <a href="https://tsfresh.readthedocs.io/en/latest/text/quick_start.html">https://tsfresh.readthedocs.io/en/latest/text/quick_start.html</a></p>
<p>中文： <a href="https://github.com/SimaShanhe/tsfresh-feature-translation">https://github.com/SimaShanhe/tsfresh-feature-translation</a></p>
<h2><span id="data-formats">Data Formats</span><a href="#data-formats" class="header-anchor">#</a></h2><p>column_id: Features will be extracted individually for each entity(id); one row per id.</p>
<p>column_sort:  sorting the time series. </p>
<p>特征提取: 可以一次性提取完；也可以单独提取kind_to_parameters 设置参数；还可以提取</p>
<p><img src="https://tsfresh.readthedocs.io/en/latest/_images/feature_extraction_process_20160815_mc_1.png" alt="the time series"></p>
<p>可分布式计算</p>
<p>the rolling mechanism</p>
<p>首先确定滑动窗口</p>
<p>Step1 : 实现单变量特征的提取</p>
<p>Step2 : 实现多变量特征的提取</p>
<h2><span id="day-ox-01">Day Ox 01</span><a href="#day-ox-01" class="header-anchor">#</a></h2><p>知识清单:</p>
<ol>
<li><p>特征提取：大概上千种特征（几十种方法）</p>
<p>tsfresh.feature_extraction.extraction.extract_features(<strong>timeseries_container</strong>,default_fc_parameters=None<strong>,</strong> <em>kind_to_fc_parameters=None**</em>,<strong> <em>column_id=None</em></strong>,<strong> <em>column_sort=None</em></strong>,<strong> <em>column_kind=None</em></strong>,<strong> <em>column_value=None</em></strong>,<strong> <em>chunksize=None</em></strong>,<strong> <em>n_jobs=1</em></strong>,<strong> <em>show_warnings=False</em></strong>,<strong> <em>disable_progressbar=False</em></strong>,<strong> <em>impute_function=None</em></strong>,<strong> <em>profile=False</em></strong>,<strong> <em>profiling_filename=’profile.txt’</em></strong>,<strong> <em>profiling_sorting=’cumulative’</em></strong>,<strong> <em>distributor=None</em></strong>)**</p>
<p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame"><code>pandas.DataFrame</code></a> containing the different time series</p>
<p><strong>column_id</strong> (<a href="https://docs.python.org/3.7/library/stdtypes.html#str"><em>str</em></a>) – The name of the id column to group by.</p>
<p><strong>column_sort</strong> (<a href="https://docs.python.org/3.7/library/stdtypes.html#str"><em>str</em></a>) – The name of the sort column.</p>
<p><strong>n_jobs</strong> (<a href="https://docs.python.org/3.7/library/functions.html#int"><em>int</em></a>) – The number of processes to use for parallelization. </p>
</li>
<li><p>时间序列的滑动窗口（单序列划分成多序列）</p>
<p><code>tsfresh.utilities.dataframe_functions.``roll_time_series</code><strong>(*</strong>df_or_dict<strong>*,</strong> <em>column_id**</em>,<strong> <em>column_sort=None</em></strong>,<strong> <em>column_kind=None</em></strong>,<strong> <em>rolling_direction=1</em></strong>,<strong> <em>max_timeshift=None</em></strong>,<strong> <em>min_timeshift=0</em></strong>,<strong> <em>chunksize=None</em></strong>,<strong> <em>n_jobs=1</em></strong>,<strong> <em>show_warnings=False</em></strong>,<strong> <em>disable_progressbar=False</em></strong>,<strong> <em>distributor=None</em></strong>)**</p>
<ul>
<li><strong>max_timeshift</strong> (<a href="https://docs.python.org/3.7/library/functions.html#int"><em>int</em></a>) – If not None, the cut-out window is at maximum max_timeshift large. If none, it grows infinitely.</li>
<li><strong>min_timeshift</strong> (<a href="https://docs.python.org/3.7/library/functions.html#int"><em>int</em></a>) – Throw away all extracted forecast windows smaller or equal than this. Must be larger than or equal 0.</li>
<li><strong>n_jobs</strong> (<a href="https://docs.python.org/3.7/library/functions.html#int"><em>int</em></a>) – The number of processes to use for parallelization. If zero, no parallelization is used.</li>
<li><em>show_warnings=False</em></li>
</ul>
</li>
<li><p>（指定）特征提取</p>
</li>
<li><p>显著性检测</p>
<p><a href="https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_selection.html?highlight=select_features#tsfresh.feature_selection.selection.select_features">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_selection.html?highlight=select_features#tsfresh.feature_selection.selection.select_features</a></p>
<p>相关性检测</p>
<p><a href="https://tsfresh.readthedocs.io/en/latest/text/parallelization.html#parallelization-of-feature-selection">https://tsfresh.readthedocs.io/en/latest/text/parallelization.html#parallelization-of-feature-selection</a></p>
</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> extract_features, select_features,extract_relevant_features</span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> roll_time_series, make_forecasting_frame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tsfresh <span class="keyword">as</span> tsf </span><br><span class="line"></span><br><span class="line">fc_parameters_value1 = {<span class="string">"length"</span>: <span class="literal">None</span>, </span><br><span class="line">                          <span class="string">"sum_values"</span>: <span class="literal">None</span>}</span><br><span class="line"></span><br><span class="line">fc_parameters_value2 = {<span class="string">"maximum"</span>: <span class="literal">None</span>, </span><br><span class="line">                             <span class="string">"minimum"</span>: <span class="literal">None</span>}</span><br><span class="line"></span><br><span class="line">kind_to_fc_parameters = {</span><br><span class="line">    <span class="string">"value1"</span>: fc_parameters_value1,</span><br><span class="line">    <span class="string">"value2"</span>: fc_parameters_value2</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># ceate data</span></span><br><span class="line">    rawdata = {<span class="string">'id1'</span>: [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="string">'time'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],\</span><br><span class="line">        <span class="string">'value1'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>], <span class="string">'value2'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">        }</span><br><span class="line">    df = pd.DataFrame(rawdata)</span><br><span class="line"><span class="comment"># 设置长度+1 = 真实长度,是当前编号往上数.</span></span><br><span class="line">    df_rolled = roll_time_series(df, column_id=<span class="string">"id1"</span>, column_sort=<span class="string">"time"</span>,</span><br><span class="line">                                max_timeshift=<span class="number">1</span>, min_timeshift=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># roll_time_series的返回值</span></span><br><span class="line">    print(df_rolled)</span><br><span class="line">    df_rolled = df_rolled.drop(<span class="string">'id1'</span>,axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># column_id: 聚合列 column_sort:排序，一个column_id就对应一个特征</span></span><br><span class="line">    extracted_features = extract_features(df_rolled, column_id=<span class="string">'id'</span>, column_sort=<span class="string">'time'</span>, </span><br><span class="line">            kind_to_fc_parameters = kind_to_fc_parameters, show_warnings=<span class="literal">False</span>)</span><br><span class="line">    print(extracted_features)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="day-ox-02-cha-kan-ti-qu-te-zheng">Day Ox 02 查看提取特征</span><a href="#day-ox-02-cha-kan-ti-qu-te-zheng" class="header-anchor">#</a></h2><p>可根据此提取自动提取的特征，用于预测时候的提取特征</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(extracted_features)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 5. 特征抽取与过滤同时进行（一步到位，省去多余计算）</span></span><br><span class="line"><span class="comment"># column_id: group by </span></span><br><span class="line"><span class="comment">#features_filtered_direct = extract_relevant_features(timeseries, y, column_id='id', column_sort='time')</span></span><br><span class="line"><span class="comment">#print(features_filtered_direct.head())</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2020/06/22/time-series-01/image-20200705150137882.png" alt="image-20200705150137882" style="zoom: 50%;"></p>
<p>学习路径：</p>
<pre><code>1. 数据格式
 2. 滑动窗口设置
 3. 特征提取
 4. 特征选择
</code></pre><h1><span id="zhuan-ti-shi-jian-xu-lie-de-jing-sai-fang-an">专题 时间序列的竞赛方案</span><a href="#zhuan-ti-shi-jian-xu-lie-de-jing-sai-fang-an" class="header-anchor">#</a></h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU1Nzc1NjI0Nw==&amp;mid=2247485604&amp;idx=1&amp;sn=6283ec080344665bfad90570bf1504a4&amp;chksm=fc31b29ccb463b8acac7acf4d89494aaad0c76620becb2b07c370ccbfaff850edc3c1ad4e0fd&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1593390448780&amp;sharer_shareid=fb5716a8ad12ea6329433df53d4cbf64#rd">https://mp.weixin.qq.com/s?__biz=MzU1Nzc1NjI0Nw==&amp;mid=2247485604&amp;idx=1&amp;sn=6283ec080344665bfad90570bf1504a4&amp;chksm=fc31b29ccb463b8acac7acf4d89494aaad0c76620becb2b07c370ccbfaff850edc3c1ad4e0fd&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1593390448780&amp;sharer_shareid=fb5716a8ad12ea6329433df53d4cbf64#rd</a></p>
<p><a href="https://www.zhihu.com/question/21229371/answer/533770345">https://www.zhihu.com/question/21229371/answer/533770345</a></p>
<p>Prophet 工具</p>
]]></content>
  </entry>
  <entry>
    <title>回归分析</title>
    <url>/2020/06/20/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1><span id="hui-gui-fen-xi">回归分析</span><a href="#hui-gui-fen-xi" class="header-anchor">#</a></h1><p>最简单的线性回归，避免多重共线性，过拟合，引入正则项的线性回归模型。涉及到的数学知识：一范数，二范数，多元函数求极值。模型的含义，参数求解算法，目标函数，以及各种模型的优缺点。</p>
<a id="more"></a>
<h2><span id="ding-yi">定义</span><a href="#ding-yi" class="header-anchor">#</a></h2><p>回归分析是寻找自变量和因变量之间的数量关系，用于预测建模的方法。其一，它可以揭示自变量和因变量之间的显著性检测。其二，揭示多个自变量对一个因变量的影响程度大小。</p>
<h2><span id="hui-gui-lei-xing">回归类型</span><a href="#hui-gui-lei-xing" class="header-anchor">#</a></h2><p>1）独立变量的数量 2）度量变量的类型 3）回归线的形状</p>
<h2><span id="1-xian-xing-hui-gui-linear-regression">1. 线性回归（Linear Regression)</span><a href="#1-xian-xing-hui-gui-linear-regression" class="header-anchor">#</a></h2><p>因变量：连续； 自变量：连续或者离散</p>
<h3><span id="mo-xing-de-xing-shi">模型的形式</span><a href="#mo-xing-de-xing-shi" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
Y = a+bX+𝜀\\
\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{array}\right)=\left(\begin{array}{cccc}
1 & x_{11} & \cdots & x_{1(p-1)} \\
1 & x_{21} & \cdots & x_{2(p-1)} \\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n 1} & \cdots & x_{n(p-1)}
\end{array}\right) \beta+\left(\begin{array}{c}
e_{1} \\
e_{2} \\
\vdots \\
e_{n}
\end{array}\right)\\
Y_{n*1} = X_{n*p}\beta+𝜀</script><p>where $a$ and $b$ are the regression coefficients, and 𝜀 is the random error.</p>
<h3><span id="mu-biao-han-shu">目标函数</span><a href="#mu-biao-han-shu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
min SSR = \sum_{i}(y_i-f(x_i))^2\\
min_{w}||Xw-y||_2^2</script><h3><span id="can-shu-gu-ji">参数估计</span><a href="#can-shu-gu-ji" class="header-anchor">#</a></h3><p>最小二乘法（Lease Square Method)（OLS)</p>
<p>This approach is called the method of ordinary least squares.</p>
<h3><span id="mo-xing-ping-gu">模型评估</span><a href="#mo-xing-ping-gu" class="header-anchor">#</a></h3><h4><span id="ni-he-you-du-r-square-coefficient-of-determination">拟合优度 R-square , coefficient of determination</span><a href="#ni-he-you-du-r-square-coefficient-of-determination" class="header-anchor">#</a></h4><p>Larger $R^2$ indicates a better fit and means that the model can better explain the variation of the output with different inputs.</p>
<p><a href="https://realpython.com/linear-regression-in-python/">https://realpython.com/linear-regression-in-python/</a></p>
<h3><span id="yao-qiu">要求</span><a href="#yao-qiu" class="header-anchor">#</a></h3><ul>
<li>自变量和因变量之间必须满足线性关系。</li>
<li>多元回归存在多重共线性，自相关性和异方差性。</li>
<li>线性回归对异常值非常敏感。异常值会严重影响回归线和最终的预测值。</li>
<li>多重共线性会增加系数估计的方差，并且使得估计对模型中的微小变化非常敏感。结果是系数估计不稳定。</li>
<li>在多个自变量的情况下，我们可以采用正向选择、向后消除和逐步选择的方法来选择最重要的自变量。</li>
</ul>
<h2><span id="luo-ji-hui-gui-logistic-regression">逻辑回归（Logistic Regression)</span><a href="#luo-ji-hui-gui-logistic-regression" class="header-anchor">#</a></h2><p>Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。</p>
<h3><span id="logistic-fen-bu">Logistic 分布</span><a href="#logistic-fen-bu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
F(x) = P(X<=x) = \frac{1}{1+e^{-(x-u)/\gamma}}</script><script type="math/tex; mode=display">
f(x) = F'(X<=x) = \frac{e^{-(x-u)/\gamma}}{\gamma(1+e^{-1(x-u)/\gamma})^2}</script><p>where $u$ 表示位置参数，$\gamma$是形状参数</p>
<p><img src="https://pic2.zhimg.com/80/v2-b15289fd1162a807e11949e5396c7989_720w.jpg" alt="img"></p>
<h3><span id="mo-xing-xing-shi">模型形式</span><a href="#mo-xing-xing-shi" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
y = \frac{1}{1+e^{-(w^Tx+b)}}</script><script type="math/tex; mode=display">
P(Y=1|x) = \frac{1}{1+e^{-(w^Tx+b)}}</script><h3><span id="sun-shi-han-shu">损失函数</span><a href="#sun-shi-han-shu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
P(Y=1|x)=p(x)\\
p(Y=0|x) = 1-p(x)</script><p>似然函数</p>
<script type="math/tex; mode=display">
L(w)</script><h2><span id="duo-xiang-shi-hui-gui-polynomial-regression">多项式回归（Polynomial Regression）</span><a href="#duo-xiang-shi-hui-gui-polynomial-regression" class="header-anchor">#</a></h2><h2><span id="zhu-bu-hui-gui-stepwise-regrssion">逐步回归（Stepwise Regrssion)</span><a href="#zhu-bu-hui-gui-stepwise-regrssion" class="header-anchor">#</a></h2><h2><span id="ling-hui-gui-ridge-regression">岭回归（Ridge Regression)</span><a href="#ling-hui-gui-ridge-regression" class="header-anchor">#</a></h2><p>L2正则化(The ridge coefficients minimize a penalized residual sum of squares) 惩罚函数</p>
<h3><span id="sun-shi-han-shu">损失函数</span><a href="#sun-shi-han-shu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
argmin_{w}||y-X\beta||_2^2+\lambda||\beta||_2^2</script><p>岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。</p>
<h2><span id="tao-suo-hui-gui-lasso-regression">套索回归（ <strong>Lasso Regression</strong>）</span><a href="#tao-suo-hui-gui-lasso-regression" class="header-anchor">#</a></h2><h3><span id="l1-zheng-ze-hua-sun-shi-han-shu">L1正则化 损失函数</span><a href="#l1-zheng-ze-hua-sun-shi-han-shu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
argmin_{w}||y-X\beta||_2^2+\lambda||\beta||_1</script><p>The larger the value of $\lambda$ , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. </p>
<h2><span id="dan-xing-hui-gui-elasticnet-regression">弹性回归 ElasticNet Regression</span><a href="#dan-xing-hui-gui-elasticnet-regression" class="header-anchor">#</a></h2><h4><span id="sun-shi-han-shu">损失函数</span><a href="#sun-shi-han-shu" class="header-anchor">#</a></h4><script type="math/tex; mode=display">
argmin_{w}||y-X\beta||_2^2+\lambda_1||\beta||_2^2+\lambda_2||\beta||_1</script><h2><span id="bei-xie-si-hui-gui">贝叶斯回归</span><a href="#bei-xie-si-hui-gui" class="header-anchor">#</a></h2><p>频率派（优化问题）</p>
<p>贝叶斯派</p>
<p>在极大似然估计线性回归中我们把参数看成是一个未知的固定值，而贝叶斯学派则把看成是一个随机变量。 </p>
<p>Model</p>
<script type="math/tex; mode=display">
f(x) = w^Tx = x^Tw</script><p>Bayesian Method</p>
<p>Inference and Prediction</p>
<p><img src="https://img2018.cnblogs.com/blog/1252882/201902/1252882-20190224115011028-596537100.png" alt="img"></p>
<p><img src="https://img2018.cnblogs.com/blog/1252882/201902/1252882-20190224115035183-1841742133.png" alt="img"></p>
<p><img src="https://img2018.cnblogs.com/blog/1252882/201902/1252882-20190224115121150-29154011.png" alt="img"></p>
<p><img src="https://img2018.cnblogs.com/blog/1252882/201902/1252882-20190224130226736-1827960691.png" alt="img"></p>
<h2><span id="reference">reference</span><a href="#reference" class="header-anchor">#</a></h2><p><a href="https://courses.analyticsvidhya.com/courses/Fundamentals-of-Regression-Analysis?utm_source=blog&amp;utm_medium=introduction_to_regression">https://courses.analyticsvidhya.com/courses/Fundamentals-of-Regression-Analysis?utm_source=blog&amp;utm_medium=introduction_to_regression</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/">https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/</a></p>
<h1><span id="linear-regression-by-sklearn">Linear Regression by Sklearn</span><a href="#linear-regression-by-sklearn" class="header-anchor">#</a></h1><h2><span id="ols">OLS</span><a href="#ols" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score </span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the diabetes dataset</span></span><br><span class="line">diabetes_X, diabetes_Y = datasets.load_diabetes(return_X_y= <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># select one feature</span></span><br><span class="line">diabetes_X = diabetes_X[:, np.newaxis, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data set into training/testing sets</span></span><br><span class="line">X_train = diabetes_X[:<span class="number">-20</span>]</span><br><span class="line">X_test = diabetes_X[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line">Y_train = diabetes_Y[:<span class="number">-20</span>]</span><br><span class="line">Y_test = diabetes_Y[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train regression model</span></span><br><span class="line">regr.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict </span></span><br><span class="line">Y_pred = regr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate</span></span><br><span class="line">print(<span class="string">'Mean squared error: %.2f'</span>% mean_squared_error(Y_test, Y_pred))</span><br><span class="line">print(<span class="string">'R2_score:%.2f'</span>% r2_score(Y_test,Y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.scatter(X_test,Y_test, color = <span class="string">'black'</span>)</span><br><span class="line">plt.plot(X_test,Y_pred, color = <span class="string">'blue'</span>,linewidth = <span class="number">3</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="ridge-amp-lasso">Ridge &amp; Lasso</span><a href="#ridge-amp-lasso" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">reg = line_model.Ridge(alpha = <span class="number">.2</span>)</span><br><span class="line">reg.fit([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">8</span>]],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">reg.coef_</span><br><span class="line">reg.intercept_</span><br><span class="line"></span><br><span class="line">reg1 = line_model.RidgeCV(alphas = np.logspace(<span class="number">-6</span>,<span class="number">6</span>,<span class="number">13</span>))</span><br><span class="line">reg1.fit([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">8</span>]],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">reg.alpha_</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line">reg = line_model.Lasso(alpha = <span class="number">.2</span>)</span><br><span class="line">reg.fit([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">8</span>]],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">reg.coef_</span><br><span class="line">reg.intercept_</span><br><span class="line"></span><br><span class="line">reg1 = line_model.LassoCV(alphas = np.logspace(<span class="number">-6</span>,<span class="number">6</span>,<span class="number">13</span>))</span><br><span class="line">reg1.fit([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">8</span>]],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">reg.alpha_</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="zheng-ze-xi-shu-xuan-ze">正则系数选择</span><a href="#zheng-ze-xi-shu-xuan-ze" class="header-anchor">#</a></h3><p>交叉验证 LassoCV。 LassoLarsCV基于Least Angle Regression 算法</p>
<p>坐标下降法</p>
<h2><span id="dan-xing-hui-gui">弹性回归</span><a href="#dan-xing-hui-gui" class="header-anchor">#</a></h2><p>Elastic-Net</p>
<h3><span id="zui-xiao-jiao-hui-gui">最小角回归</span><a href="#zui-xiao-jiao-hui-gui" class="header-anchor">#</a></h3><h3><span id="lars-lasso">LARS Lasso</span><a href="#lars-lasso" class="header-anchor">#</a></h3><h3><span id="bei-xie-si-ling-hui-gui">贝叶斯岭回归</span><a href="#bei-xie-si-ling-hui-gui" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]]</span><br><span class="line">Y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">reg = linear_mode.BayesianRidge()</span><br><span class="line">reg.fit(X, Y)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="luo-ji-si-te-hui-gui">逻辑斯特回归</span><a href="#luo-ji-si-te-hui-gui" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from sklearn import linear_model</span><br><span class="line"></span><br><span class="line">reg = linear_model.LogisticRegression()</span><br></pre></td></tr></tbody></table></figure>
<p>参数</p>
<p><strong>penalty**</strong>{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’**</p>
<p><strong>tol*</strong>float, default=1e-4*</p>
<p>Tolerance for stopping criteria.</p>
<p><strong>C*</strong>float, default=1.0*</p>
<p>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</p>
<p><strong>solver*</strong>{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’*</p>
<p>Algorithm to use in the optimization problem.</p>
<p><strong>max_iter**</strong>int, default=100**</p>
<p><strong>n_jobs**</strong>int, default=None**</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归分析</tag>
      </tags>
  </entry>
  <entry>
    <title>知识清单</title>
    <url>/2020/06/19/%E7%9F%A5%E8%AF%86%E6%B8%85%E5%8D%95/</url>
    <content><![CDATA[<p>主要是列出关于日常中遇到的很好的资料，自己不清楚的文章和资料。</p>
<a id="more"></a>
<h2><span id="2020-8-24-5w2h">2020-8-24 5W2H</span><a href="#2020-8-24-5w2h" class="header-anchor">#</a></h2><p><strong>5W2H分别对应着7个关键问号：</strong></p>
<ul>
<li>What：何事？</li>
<li>Who：何人？</li>
<li>When：何时？</li>
<li>Where：何地？</li>
<li>Why：何因？</li>
<li>How：怎么做？</li>
<li>How much：多少钱？</li>
</ul>
<p><strong>5W2H梳理销售下降问题</strong></p>
<p>文章最开头，小P老板提了一个极其模糊的问题：</p>
<p><strong>“最近销售额为什么下降了？”</strong></p>
<p>如果用5W2H法，应该怎么理清头绪呢？</p>
<p>很简单，跟着问就完事儿了！</p>
<ul>
<li><strong>What</strong>（何事-问题是什么？）</li>
</ul>
<p>问题是老板抛出的销售额下降原因分析，但这个需求太过笼统，我们需要进一步询问来界定和解构问题。</p>
<ul>
<li><strong>When</strong>（什么时候？）</li>
</ul>
<p>是什么时间段销售开始下滑？下滑是环比还是同比，亦或是和平均相比？从趋势上看，是持续性下滑，还是某些时间节点的突然下跌？</p>
<ul>
<li><strong>Where</strong>（什么地方？）</li>
</ul>
<p>是所有渠道的普遍下跌还是某个重点渠道的折戟？是全国各地普遍销售下降，还是某个地区销售下降的厉害？</p>
<ul>
<li><strong>Who</strong>（是哪群人？）</li>
</ul>
<p>是新客户还是老客户的销售贡献乏力？是普通客户的减少，还是品牌忠诚客户的流失？</p>
<ul>
<li><strong>Why</strong>（为什么？）</li>
</ul>
<p>回答完上面4个W，综合起来基本能够回答为什么销售下跌这个问题，但是这样还不够，数据分析更重要的是指导该怎么做</p>
<ul>
<li><strong>How</strong>（怎么做？）</li>
</ul>
<p>如果是某个渠道老客流失严重，应该快速做客户原因定位，以及用CRM关怀来挽回客户。</p>
<p>如果是各渠道、全国性普遍销售下跌，市场份额被对手侵蚀，那应该紧密观察市场，紧盯竞品动作。</p>
<ul>
<li><strong>How much</strong>（量化做多少？）</li>
</ul>
<p>结合上一步的行动，具体衡量通过短信或者其他方式触达花费多少，需要投入多少折扣，预计唤回多少客户，提升多少销售额，这些都可以基于历史数据量化。</p>
<p><strong>怎么样？</strong></p>
<p>对于一个模糊的销售下跌问题，通过这7步的拆解，很快就打开了分析思路。不过，要完全精准的定位问题，找到本质解决办法，还<strong>需要进一步的定位、假设和验证</strong>。</p>
<h1><span id="2020-6-29-z-jian-ce-he-t-jian-ce">2020-6-29 Z检测和T检测</span><a href="#2020-6-29-z-jian-ce-he-t-jian-ce" class="header-anchor">#</a></h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&amp;mid=2247485455&amp;idx=1&amp;sn=857066158bf8c2de38939f3037416035&amp;chksm=eb9321b9dce4a8afd68d764c295f8bcc69c62f2b1d000f3e1c5e61a7d9b6e2ec3de8df068174&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;sharer_sharetime=1593403964973&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd">https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&amp;mid=2247485455&amp;idx=1&amp;sn=857066158bf8c2de38939f3037416035&amp;chksm=eb9321b9dce4a8afd68d764c295f8bcc69c62f2b1d000f3e1c5e61a7d9b6e2ec3de8df068174&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;sharer_sharetime=1593403964973&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd</a></p>
<h1><span id="2020-6-28">2020-6-28</span><a href="#2020-6-28" class="header-anchor">#</a></h1><p>视频： <a href="http://www.julyedu.com/video/play/58/405">http://www.julyedu.com/video/play/58/405</a></p>
<h1><span id="2020-6-19">2020-6-19</span><a href="#2020-6-19" class="header-anchor">#</a></h1><p>SQL</p>
<p>中文:</p>
<p><a href="https://www.liaoxuefeng.com/wiki/1177760294764384">https://www.liaoxuefeng.com/wiki/1177760294764384</a></p>
<p>英文：</p>
<p><a href="https://www.codecademy.com/courses/learn-sql/lessons/manipulation/exercises/sql">https://www.codecademy.com/courses/learn-sql/lessons/manipulation/exercises/sql</a></p>
<p>视频：</p>
<p><a href="https://www.jikexueyuan.com/course/sql/">https://www.jikexueyuan.com/course/sql/</a></p>
<p>基础  <a href="https://study.163.com/course/courseMain.htm?courseId=215012&amp;_trace_c_p_k2_=f68f3d2867a343789ac2d3cfa92dd308">https://study.163.com/course/courseMain.htm?courseId=215012&amp;_trace_c_p_k2_=f68f3d2867a343789ac2d3cfa92dd308</a></p>
<p><a href="https://www.nowcoder.com/discuss/95812?type=2">https://www.nowcoder.com/discuss/95812?type=2</a></p>
<p><a href="https://www.cnblogs.com/zsh-blogs/category/1413021.html">https://www.cnblogs.com/zsh-blogs/category/1413021.html</a></p>
]]></content>
      <categories>
        <category>规划</category>
      </categories>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Science</title>
    <url>/2020/06/11/Data-Science/</url>
    <content><![CDATA[<a id="more"></a>
<h1><span id="course">Course</span><a href="#course" class="header-anchor">#</a></h1><p>Tsinghua</p>
<p>Dr. Yuan</p>
<p>Data Mining: Theories and Algorithms for Tackling Big Data</p>
<h1><span id="tools">Tools</span><a href="#tools" class="header-anchor">#</a></h1><p>Stata: <a href="https://www.stata.com/why-use-stata/">https://www.stata.com/why-use-stata/</a></p>
<p><a href="https://www.youtube.com/watch?v=AyXeh7iojuA">https://www.youtube.com/watch?v=AyXeh7iojuA</a></p>
<h1><span id="booooook">BOOOOOOK</span><a href="#booooook" class="header-anchor">#</a></h1><p><a href="https://www-users.cs.umn.edu/~kumar001/dmbook/index.php">https://www-users.cs.umn.edu/~kumar001/dmbook/index.php</a></p>
]]></content>
      <categories>
        <category>数据科学(Data Science)</category>
      </categories>
      <tags>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title>or</title>
    <url>/2019/12/01/or/</url>
    <content><![CDATA[<h1><span id="yun-chou-xue">运筹学</span><a href="#yun-chou-xue" class="header-anchor">#</a></h1><p>目的是在决策时为管理人员提供科学依据。</p>
<p>利用统计学，数学模型和算法等方法，寻找复杂问题中的最佳或者近似最佳的解答。</p>
<p>解决问题的优化算法。</p>
<a id="more"></a>
<h2><span id="mo-xing-jian-li">模型建立</span><a href="#mo-xing-jian-li" class="header-anchor">#</a></h2><p>实际问题</p>
<ol>
<li><p>决策变量</p>
<p>影响所要到达目的的因素找到决策变量</p>
</li>
</ol>
<ol>
<li>目标函数</li>
</ol>
<ol>
<li>约束条件</li>
</ol>
<h2><span id="xian-xing-gui-hua">线性规划</span><a href="#xian-xing-gui-hua" class="header-anchor">#</a></h2><h2><span id="zheng-shu-gui-hua">整数规划</span><a href="#zheng-shu-gui-hua" class="header-anchor">#</a></h2><p>1、纯整数规划：所有决策变量均要求为整数的整数规划<br>2、混合整数规划：部分决策变量均要求为整数的整数规划<br>3、纯0－1整数规划：所有决策变量均要求为0－1的整数规划<br>4、混合0－1规划：部分决策变量均要求为0－1的整数规划</p>
<p>分支定界法 : 精确算法—分支定界法(Branch and Bound Algorithm, B&amp;B)</p>
<p> 这就意味着，要么花钱买以上求解器的使用权，要么就自己写B&amp;B算法的Code，然后忍受Cplex 1分钟可以求解的问题却要花1天时间的求解。（很多问题时间就是金钱，例如航班延误后剩余航班重新排班的问题，通常需要在10分钟内求解） </p>
<p>想法：</p>
<p>首先，可以确定的是这是个航班重新排班的问题，数学上，航班安排属于运筹学的问题之一，需要应用建立优化模型解决。建立最优化问题，最重要的两步是模型建立和模型求解。模型的建立：需要确定决策变量（整数规划，混合整数规划)，目标函数（多目标），约束条件。 模型的求解： 分层序列法 。</p>
<h2><span id="ke-tang-bei-jing">课堂背景</span><a href="#ke-tang-bei-jing" class="header-anchor">#</a></h2><p>航班的重排班问题，最优化问题，运筹学范畴的问题。</p>
<h3><span id="yue-shu">约束：</span><a href="#yue-shu" class="header-anchor">#</a></h3><p>算法能够在满足多种实际约束条件的前提下，可以对航班计划进行恢复，并快速给出最优的航班调整替换方案 ； </p>
<p> 航班运行与机组编排的各类约束条件 ； 根据航班计划对机组排班计划进行调整，使得机组的资质等与航班计划可以匹配，</p>
<p> 川航一个月内的全部航班计划与机组排班计划 </p>
<ol>
<li><p>备用飞机</p>
<p>有限</p>
</li>
<li><p>调整分机飞行顺序</p>
</li>
</ol>
<ol>
<li><p>航班延迟</p>
<p>不能提前起飞</p>
<p>不能超过延误时间</p>
</li>
<li><p>航班取消</p>
<p>如果超过了延误时间，目标函数增加调整成本</p>
</li>
<li><p>旅客转签</p>
<p>只能一次转，还有座位限制</p>
</li>
<li><p>航班直飞</p>
<p> 航班由于天气或流控等原因无法顺畅运行时，将联程航班中段取消直飞最终目的地，并妥善处置旅客是航班调整方法之一。（联程航班定义为，前后段衔接并且航班号相同的多个航班） </p>
</li>
<li><h5><span id="ji-zu-diao-zheng">机组调整</span><a href="#ji-zu-diao-zheng" class="header-anchor">#</a></h5><p>（一） 备份机组<br>在航班计划出现机组实力缺口时，可以在航班的出发地寻找空闲机组，安排其执行该航班。<br>（二） 调换机组<br>将多个机组的航班计划进行调换。<br>（三） 机组摆渡<br>当遇到机组计划不衔接（机组的上个航班的目的地与下个航班的出发地不一致）时，可以通过摆渡的方式，采用飞机或是其他交通工具到达下个航班的出发地。</p>
</li>
</ol>
<h3><span id="mu-biao-jiang-hang-ban-yun-xing-qing-kuang-shou-dao-de-ying-xiang-jiang-dao-zui-di">目标：  将航班运行情况受到的影响降到最低</span><a href="#mu-biao-jiang-hang-ban-yun-xing-qing-kuang-shou-dao-de-ying-xiang-jiang-dao-zui-di" class="header-anchor">#</a></h3><p>从而使得航班与机组计划得到快速恢复、减少航班延误、提高航班正常率，使旅客有更好的出行体验，并提升公司的运行效率与经济效益。 </p>
<ol>
<li>航公公司： 损失最小</li>
<li>游客:  航班延时短</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>linux</title>
    <url>/2019/11/27/linux/</url>
    <content><![CDATA[<p>linux内核，linux发行版（带桌面环境），服务器的访问方式（三种），linux操作系统的相关使用，通过命令行和键盘输入搞定（用户权限，用户分组，用户之间的关系，用户操作；文件结构，文件使用，文件权限，文件编辑，文件压缩和解压，这系列操作类比操作系统，只是linux系统里面，都是命令行完成，不是可视化界面罢了；帮助，终止操作，root，sudo,超级管理员，管理员组，普通用户）windows操作系统可以干的事情，在linux服务器里面，都可以干，通过命令行配置，安装，完成！</p>
<a id="more"></a>
<h1><span id="linux">Linux</span><a href="#linux" class="header-anchor">#</a></h1><h2><span id="linux-de-ke-pu">Linux 的科普</span><a href="#linux-de-ke-pu" class="header-anchor">#</a></h2><p> Linux 是一套免费使用和自由传播的类 Unix 操作系统 ，支持多用户，多任务，支持多线程和对CPU的操作系统。，就像你多少已经了解的 Windows（xp，7，8）和 Mac OS 。</p>
<p><img src="https://doc.shiyanlou.com/linux_base/1-1.png/wm" alt="图1-1"></p>
<p> 或许你之前不知道 Linux ，要知道，你之前在 Windows 使用百度、谷歌，上淘宝，聊 QQ 时，支撑这些软件和服务的，是后台成千上万的 Linux 服务器主机，它们时时刻刻都在忙碌地进行着数据处理和运算，可以说世界上大部分软件和服务都是运行在 Linux 之上的。 </p>
<p> 明确目的：你是要用 Linux 来干什么，搭建服务器、做程序开发、日常办公，还是娱乐游戏； </p>
<p> 兼具图形界面操作（需要使用带有桌面环境的发行版）和完全的命令行操作，可以只用键盘完成一切操作，新手入门较困难，需要一些学习和指导（这正是我们要做的事情），一旦熟练之后效率极高。 </p>
<p>一般命令行操作，通过键盘完成</p>
<p> 因为linux的哲学就是：没有结果就是最好的结果 </p>
<p> 如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为linux的哲学就是：没有结果就是最好的结果 </p>
<h2><span id="linux-de-fa-xing-ban">Linux的发行版</span><a href="#linux-de-fa-xing-ban" class="header-anchor">#</a></h2><p>Linux发行版 = linux内核+应用软件的打包</p>
<p>知名的发行版： ubuntu，redhat,centos</p>
<p>Linux系统</p>
<ol>
<li><p>用户登录系统</p>
<p>（1）命令行</p>
<p>（2）ssh登录</p>
<p> SSH 为 <a href="https://baike.baidu.com/item/Secure Shell">Secure Shell</a> 的缩写 ，用于远程登陆的协议</p>
<p>远程连接工具客户端：xshell, putty,</p>
<p>  (3)   图形界面登录</p>
</li>
</ol>
<p><img src="https://img-blog.csdn.net/20180719122550321?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTAzNjcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<ol>
<li><p>文件目录以及权限</p>
<ol>
<li>Linux 中创建、删除用户，及用户组等操作。</li>
<li>Linux 中的文件权限设置。</li>
</ol>
<p>1.2 实验知识点</p>
<ul>
<li>Linux <strong>用户管理</strong></li>
<li>Linux <strong>权限管理</strong></li>
</ul>
<ol>
<li><p>用户管理</p>
<p>通过第一节课程的学习，你应该已经知道，Linux 是一个可以实现多用户登录的操作系统，比如“李雷”和“韩梅梅”都可以同时登录同一台主机，他们共享一些主机的资源，但他们也分别有自己的用户空间，用于存放各自的文件。但实际上他们的文件都是放在同一个物理磁盘上的甚至同一个逻辑分区或者目录里，但是由于 Linux 的 <strong>用户管理</strong>和 <strong>权限机制</strong>，不同用户不可以轻易地查看、修改彼此的文件。 </p>
</li>
<li><p>在 Linux 系统里， <code>root</code> 账户拥有整个系统至高无上的权利，比如 新建/添加 用户。</p>
<p>sudo adduser lilei</p>
</li>
<li><p>创建用户（sudo 组）</p>
<p>我们一般登录系统时都是以普通账户的身份登录的，要创建用户需要 root 权限，这里就要用到 <code>sudo</code> 这个命令了。不过使用这个命令有两个大前提，一是你要知道当前登录用户的密码，二是当前用户必须在 <code>sudo</code> 用户组。 sudo命令：获得root权限</p>
</li>
<li><p>用户组</p>
<p>查看：</p>
<p>在 Linux 里面每个用户都有一个归属（用户组），用户组简单地理解就是一组用户的集合，它们共享一些资源和权限，同时拥有私有资源 。</p>
<p> groups shiyanlou </p>
<p>加入sudo用户组</p>
<p>su <user>：切换用户user，需要输入目标用户和密码</user></p>
<p>sudo usermod -G sudo lilei</p>
</li>
<li><p>文件所以者</p>
<p>su -l lilei</p>
<p>su chown 修改权限</p>
<p><code>sudo</code> 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码。 </p>
</li>
</ol>
</li>
<li><p>文档编辑</p>
<p>vim编辑器</p>
<p>i esc :wq</p>
</li>
<li><p>linux文件系统与磁盘管理</p>
</li>
<li><p><img src="https://doc.shiyanlou.com/linux_base/4-1.png/wm" alt="img"></p>
<p>  $ tree /  </p>
<p>pwd</p>
<p>cd ..: 上一级目录</p>
<p>../</p>
<p>/:根目录：绝对路径</p>
<p> cd /home/shiyanlou</p>
<p>touch test</p>
<p> mkdir mydir </p>
<p>  <code>cp</code>（copy）命令复制一个文件到指定目录 </p>
<p> 要成功复制目录需要加上 <code>-r</code> 或者 <code>-R</code> 参数，表示递归复制 </p>
<p>cd /home/shiyanlou<br> mkdir family<br>​ cp -r father family</p>
</li>
</ol>
<pre><code>rm test 

跟复制目录一样，要删除一个目录，也需要加上 `-r` 或 `-R` 参数 

mv 源目录文件 目的目录 ，可以用来重命名文件
</code></pre><p>   $ cd /home/shiyanlou/</p>
<p>   使用通配符批量创建 5 个文件:</p>
<p>   $ touch file{1..5}.txt</p>
<p>   批量将这 5 个后缀为 .txt 的文本文件重命名为以 .c 为后缀的文件:</p>
<p>   $ rename ‘s/.txt/.c/‘ *.txt</p>
<p>   批量将这 5 个文件，文件名和后缀改为大写:</p>
<p>   $ rename ‘y/a-z/A-Z/‘ *.c</p>
<p>   文件打包和解压缩</p>
<ul>
<li>zip：<ul>
<li>打包 ：zip something.zip something （目录请加 -r 参数）</li>
<li>解包：unzip something.zip</li>
<li>指定路径：-d 参数</li>
</ul>
</li>
<li><p>tar：</p>
<ul>
<li>打包：tar -cf something.tar something</li>
<li>解包：tar -xf something.tar</li>
<li>指定路径：-C 参数、</li>
</ul>
<p>tar -cf shiyanlou.tar /home/shiyanlou/Desktop </p>
<p><code>-c</code> 表示创建一个 tar 包文件，<code>-f</code> 用于指定创建的文件名，注意文件名必须紧跟在 <code>-f</code> 参数之后 </p>
<p>tar -xf shiyanlou.tar -C tardir </p>
<p>解包一个文件（<code>-x</code> 参数）到指定路径的<strong>已存在</strong>目录（<code>-C</code> 参数） </p>
<p>tar -xzf shiyanlou.tar.gz </p>
</li>
</ul>
<ol>
<li><p>| 压缩文件格式 | 参数 |<br>| —————— | —— |<br>| <code>*.tar.gz</code>   | <code>-z</code> |<br>| <code>*.tar.xz</code>   | <code>-J</code> |<br>| <code>*tar.bz2</code>   | <code>-j</code> |</p>
</li>
<li><p>linux安装软件</p>
</li>
</ol>
<h3><span id="shu-ru-shu-chu">输入，输出</span><a href="#shu-ru-shu-chu" class="header-anchor">#</a></h3><p>   输入：输入当然就是打开终端，然后按键盘输入，然后按回车，输入格式一般就是这类的</p>
<p>输出：</p>
<p>输出会返回你想要的结果，比如你要看什么文件，就会返回文件的内容。</p>
<p>如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为linux的哲学就是：没有结果就是最好的结果</p>
<p>Tab: 命令补全</p>
<p>Ctrl+C:强制终止</p>
<p>学会使用通配符：通配符：*,?</p>
<p>学会在命令行中获取帮助：man命令调用手册页，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>区段</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>一般命令</td>
</tr>
<tr>
<td>2</td>
<td>系统调用</td>
</tr>
<tr>
<td>3</td>
<td>库函数，涵盖了C标准函数库</td>
</tr>
<tr>
<td>4</td>
<td>特殊文件（通常是/dev中的设备）和驱动程序</td>
</tr>
<tr>
<td>5</td>
<td>文件格式和约定</td>
</tr>
<tr>
<td>6</td>
<td>游戏和屏保</td>
</tr>
<tr>
<td>7</td>
<td>杂项</td>
</tr>
<tr>
<td>8</td>
<td>系统管理命令和守护进程</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">man 1 ls</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">ls --help</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>清晰有效的数据分析思路</title>
    <url>/2019/11/26/%E6%B8%85%E6%99%B0%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E8%B7%AF/</url>
    <content><![CDATA[<p>数据分析</p>
<a id="more"></a>
<h2><span id="ru-he-zuo-shu-ju-fen-xi-hui-bao">如何做数据分析汇报</span><a href="#ru-he-zuo-shu-ju-fen-xi-hui-bao" class="header-anchor">#</a></h2><h3><span id="1-miao-shu-shu-ju-de-biao-zheng">1. 描述数据的表征</span><a href="#1-miao-shu-shu-ju-de-biao-zheng" class="header-anchor">#</a></h3><p>描述性统计：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>平均数</th>
<th>中位数</th>
<th>众数</th>
<th>几何平均数</th>
<th>调和平均数</th>
</tr>
</thead>
<tbody>
<tr>
<td>平均值</td>
<td>中间位置的数</td>
<td>出现次数最多</td>
<td></td>
<td></td>
</tr>
<tr>
<td>方差</td>
<td>标准差</td>
<td>分布</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>得到第一份数据结论</p>
<h3><span id="2-xun-zhao-bian-hua-shen-ru-guan-cha">2. 寻找变化，深入观察</span><a href="#2-xun-zhao-bian-hua-shen-ru-guan-cha" class="header-anchor">#</a></h3><p>发生变化的指标一般就是指标关联的业务环境发生了某种变化。通过观察变化量，寻找可能的业务问题点。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>同比</th>
<th>对比同期的变化</th>
<th>如：上周五和去年过年</th>
</tr>
</thead>
<tbody>
<tr>
<td>环比</td>
<td>对比连续周期</td>
<td>如：今天和昨天；本月和上月；</td>
</tr>
<tr>
<td>增长率</td>
<td>评估累计型指标的有力工具</td>
<td>如：收入</td>
</tr>
</tbody>
</table>
</div>
<p> 时间上的对比，也称为纵比  ：环比，同比</p>
<p>同级单位之间的比较，简称横比 ： 不同省份之间的分析</p>
<p>得到第二份数据结论，可以分析到问题所在。</p>
<h3><span id="3-quan-mian-ping-gu-duo-wei-fen-xi">3.全面评估，多维分析</span><a href="#3-quan-mian-ping-gu-duo-wei-fen-xi" class="header-anchor">#</a></h3><p>多维分析：</p>
<p>维度是描述指标 不同角度，通过多维分析，来寻求指标的变化的可以的原因。</p>
<p>广义的多维分析，不仅仅包括从指标的不同维度进行分析，也包含拆分为多个子指标进行分析。</p>
<p>指标体系+维度体系</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基础/通用</th>
<th>年龄、性别、学历、地域、手机型号、操作系统</th>
</tr>
</thead>
<tbody>
<tr>
<td>产品</td>
<td>产品类型、归属业务</td>
</tr>
<tr>
<td>运营</td>
<td>归属渠道、投放周期、活动类型</td>
</tr>
<tr>
<td>营销</td>
<td>市场推广、营销方式、营销目的</td>
</tr>
</tbody>
</table>
</div>
<p>如粽子的维度</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>产品维度</th>
<th>肉粽，大枣棕，糖总</th>
</tr>
</thead>
<tbody>
<tr>
<td>渠道维度</td>
<td>线上：自有APP,电商渠道，合作渠道；线下：合作门店，大屏广告</td>
</tr>
<tr>
<td>时间维度</td>
<td>投放周期；投放时段</td>
</tr>
<tr>
<td>地域维度</td>
<td>直辖市；省会；二三线城市</td>
</tr>
<tr>
<td>年龄维度</td>
<td>年龄段</td>
</tr>
<tr>
<td>线上入口</td>
<td>splash,banner,弹窗，角标</td>
</tr>
</tbody>
</table>
</div>
<p>输入：第三份结论；单指标分析：得到分析到上升，下降的原因。</p>
<h3><span id="4-duo-zhi-biao-jiao-cha-fen-xi">4. 多指标交叉分析</span><a href="#4-duo-zhi-biao-jiao-cha-fen-xi" class="header-anchor">#</a></h3><p>维度偏差： 大数据涉及的维度很多单一维度分析会出现偏差，多个维度组合起来的时候可能得到相反的结论。</p>
<p>幸存者偏差：样本的丢失问题</p>
<p>第四份分析结论：分析得到出现的问题？</p>
<h3><span id="5-liang-hua-ping-gu-xun-zhao-gui-yin">5. 量化评估，寻找归因</span><a href="#5-liang-hua-ping-gu-xun-zhao-gui-yin" class="header-anchor">#</a></h3><p>相关性分析： 在业务中，通过是为了量化评估各种因素对于核心指标的影响程度，寻找对业务影响的原因。</p>
<p>相关性分析：</p>
<ol>
<li>单因素相关性分析</li>
<li>多因素的相关性分析</li>
</ol>
<p>第五份分析结论： 找到了核心影响因素了</p>
<h3><span id="6-hui-dao-wei-lai-qu-shi-yu-ce">6. 回到未来、趋势预测</span><a href="#6-hui-dao-wei-lai-qu-shi-yu-ce" class="header-anchor">#</a></h3><p>趋势预测： 预测分析是一种统计或数据挖掘解决方案</p>
<p>时间序列预测：一般时间序列预测；季节性时间序列预测；复合时间序列预测</p>
<ol>
<li><p>数学层面是严谨的</p>
<p>用一些数据预测方法和算法：指数平滑模型</p>
</li>
<li><p>业务层面是易变的</p>
<p>实际业务环境中，影响未来发展的还会有行业环境的突变，资源的突变，产品客群的突变等，人为的干扰较大。业务层面的趋势预测是不稳定的，且易变的</p>
</li>
</ol>
<p>得到第五份结论：未来效果</p>
<h3><span id="7-fen-xi-de-shi-xiang-luo-di-ye-wu">7. 分析的实相，落地业务</span><a href="#7-fen-xi-de-shi-xiang-luo-di-ye-wu" class="header-anchor">#</a></h3><p>分析的结论和数据逻辑与业务方—-确认，数据分析一定要闭环，即从业务中来，到业务中去。</p>
<h3><span id="zhi-biao-he-wei-de-gai-nian">指标和维的概念</span><a href="#zhi-biao-he-wei-de-gai-nian" class="header-anchor">#</a></h3><ol>
<li>指标</li>
</ol>
<p>​       指标:衡量事物发展程度的单位和方法，也叫度量。如：人口数，GDP, 收入，用户数，利润利，留存率，覆盖率等。</p>
<p>​       指标分为：绝对数指标和相对数指标。绝对指标：反映了规模大小；相对指标：反映了质量好坏的指标。</p>
<ol>
<li><p>维度</p>
<p>事物或者现象的某种特征，如性别，地区，时间。</p>
</li>
</ol>
<p>​       分为定量维度和定性维度。定性：字符型数据；定量：数值型。</p>
<p> 只有通过事物发展的数量、质量两大方面，从横比、纵比角度进行全方位的比较，我们才能够全面的了解事物发展的好坏 </p>
<p>通俗举个例子：2019年各个省级的经济发展状况：GDP总量：指标；省份，二三线城市：维度；</p>
<p>​    </p>
<p>总结： 数据分析的典型过程；指标拆分，维度对比；</p>
<p>产品（Product），是用来满足人们需求和欲望的物体或无形的载体。产品的实体称为一般产品。产品包含了产品的核心利益（向消费者提供的基本效用和利益）</p>
<pre><code>1. 软件，通讯，手机，科技产品
</code></pre><p>市场是指一种货物或劳务的潜在购买者的集合需求。</p>
<p>在市场营销组合中， 4P 分别是产品( product) 、价格( price) 、地点( place) 、促销( promotion) </p>
<p>营销是创造、沟通与传送价值给顾客，及经营顾客关系以便让组织与其利益关系人（stakeholder）受益的一种组织功能与程序。</p>
<p>通俗地讲，就是通过宣传、推广，进而促进产品或服务的销售。</p>
<p> 互联网产品公司三个业务部分：产品，技术，运营</p>
<p>产品：把东西想出来</p>
<p>技术：把东西做出来</p>
<p>运营：把东西用起来</p>
<p>从字面上看，运，是让产品维持运转；营，是让产品运转得更好，就是要对用户群体进行有目的地组织和管理，增加用户数量、用户粘性、用户贡献和用户忠诚度，这也就涉及到运营工作的三个重要方面：拉新、留存、促活。</p>
<p>理解问题—&gt; 设计解决方案—&gt; 迭代方案，直到问题解决</p>
<h2><span id="shu-ju-fen-xi-shi-de-ji-neng-zhi-lu">数据分析师的技能之路</span><a href="#shu-ju-fen-xi-shi-de-ji-neng-zhi-lu" class="header-anchor">#</a></h2><ol>
<li>week 01:  <strong>Excel学习掌握</strong> </li>
<li><p>week 02: 数据可视化</p>
</li>
<li><p>week 03： 分析思维的训练</p>
</li>
<li>week 04:  数据库学习</li>
<li>week 05: 统计知识学习</li>
<li>week 06: 业务学习（用户行为，产品，运营）</li>
<li>week 07： Python/R学习</li>
</ol>
<h2><span id="shu-ju-fen-xi-ying-you-de-luo-ji-si-wei-ji-fen-xi-fang-fa">数据分析应有的逻辑思维及分析方法</span><a href="#shu-ju-fen-xi-ying-you-de-luo-ji-si-wei-ji-fen-xi-fang-fa" class="header-anchor">#</a></h2><p><strong>提出问题➟分析问题➟提出假设➟验证假设➟输出结论</strong></p>
<h3><span id="01-mu-biao-si-wei">01 目标思维</span><a href="#01-mu-biao-si-wei" class="header-anchor">#</a></h3><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8jiavA4aJqXtHRH1SXglgV3uP6ORRu1BIoqJswicZicnb1zsQ7ibns8MWibHCKEEHgm57sR90oykQx8vQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>在陈述问题时所使用的KWIC方法，其实也是逻辑要素的延伸：</p>
<p>1）K（KEY）：核心观点</p>
<p>2）W（Widen）：扩展核心观点包含的内容</p>
<p>3）I（Illustrate）：举例说明佐证观点</p>
<p>4）C（Conclude）：总结</p>
<h3><span id="02-jie-gou-hua-si-wei">02 结构化思维</span><a href="#02-jie-gou-hua-si-wei" class="header-anchor">#</a></h3><p>结构化思维能够帮助我们<strong>将无序、散乱的信息进行聚焦、归纳、分类。</strong></p>
<p><strong><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8nz3To4RXmOMqV4SKpD4TxOL5ysfw6Q8R7b4ovapHViaF3bf5qFcib7nyjQUXZa8QL9rQRs8U7TPMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></strong></p>
<h3><span id="03-tui-li-si-wei"><strong>03 推理思维</strong></span><a href="#03-tui-li-si-wei" class="header-anchor">#</a></h3><p>确认论点，结构化论据，下一步是论证。在论证中运用推理思维能够帮助我们迅速找到问题的异同点，从而发现它们的规律。</p>
<p>归纳法，指从特殊（部分样本）到一般（全量样本）的过程，通俗的说是从个别的经验归纳出普遍规律的方法。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8lgsYibjmVic1HHc5nqxSKyJWHDb9qNwPdrdsodEEwttZ931nVv6faBiaCKVOqbpe9XebNiarhde3UHA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>这实质上是以偏概全的方法，一旦有一个用户不满足这个前提，这个结论就无法成立。</p>
<p>在输出结论之前需要判断<strong>样本是否足够有代表性，判断是必然事件还是随机事件。</strong></p>
<p><strong>3-2、演绎法</strong></p>
<p>演绎法则与归纳法相反,是从既有经证实的普遍性结论，推导出个别性结论的一种方法，常见的表现形式是逻辑三段论。</p>
<p>逻辑三段论的格式为：大前提、小前提、结论。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8lgsYibjmVic1HHc5nqxSKyJgjXf7nJqD4cWCfRHpjqvyPYqNCHFg0g40HN2ia8zXXy4tXej0M3eyUg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>3-3、因果关系分析法</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8lgsYibjmVic1HHc5nqxSKyJpdZ0ialiaNyXsvnJQkOkFqmy8WaMXiaWdMTUrHbzJOprobx3Y1jIAMllg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>枚举完毕后，辩证时提问3个问题：</p>
<p>1）原因是否真实？</p>
<p>2）结果是否真实</p>
<p>3）这个原因一定会引起这个结果吗？是否有其他的原因？</p>
<h2><span id="shu-ju-fen-xi-de-fang-fa">数据分析的方法</span><a href="#shu-ju-fen-xi-de-fang-fa" class="header-anchor">#</a></h2><h3><span id="01-shu-ju-fen-xi-qian-de-zhun-bei"><strong>01  数据分析前的准备</strong></span><a href="#01-shu-ju-fen-xi-qian-de-zhun-bei" class="header-anchor">#</a></h3><p><strong>1-1、分清楚目标和指标</strong></p>
<p>数据分析，能帮助我们了解<strong>业务运行状况</strong>，并从中<strong>发现问题、优化问题</strong>。其次，还能够帮助<strong>洞察</strong>下一个增长点。</p>
<p><strong>数据分析的意义，往往在数据产生之前。</strong>我们应围绕产品目标，进行产品设计以及运营策划。</p>
<p><strong>目标是结果，而指标是对结果分拆的具体要求，是对目标的衡量。</strong></p>
<p>假设我们的目标是提升年度成交金额，那衡量这个目标的方法是什么呢？</p>
<p>根据衡量的方法我们才能定向的设置调整产品设计及运营策略。如果缺少<strong>可衡量目标的单位和方法</strong>，目标会难以达成。</p>
<p>而围绕目标设置数据的采集方案，可以大大节省数据过滤和清洗的时间。</p>
<p>甚至于在明确指标后再最开始就设置好分析模型，通过监测模型中的数据情况更及时的发现问题，做出更高质、高效的决策。</p>
<p><strong>1-2、辨别指标的目的</strong><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l9BEvJAaDibbu191EOBOPbPic0uCNrrlhfiam1CHLIHnDia6eWUf0lAa7RcQF0rfHhDggCLZWziaaXXOIg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>结果指标用于衡量目标，过程指标用于体现如何完成。观察指标则指的受影响指标，其是否会受到自变量（结果指标）的影响，导致上升或下降。</p>
<p>在上图中，基于成交订单数，设置过程指标为订单平均金额及商品分布能帮助我们了解完成的方式。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4libVa40fIVA6bicpQPicicqMTZjkVI206GqoaZ3gMu8dPUiclcxBtW7atN7P2ZHRzExop13iby2Q9MyANPQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>1-3、确认分析类型</strong></p>
<p>在完成目标和指标后，下一步就是应用结构化思维进行拆解和延伸。</p>
<p>拆解出的指标目的是什么？根据目的我们才能有倾向性的分析。</p>
<p><strong>1）描述性分析</strong></p>
<p>表现形式：数据报表</p>
<p>数据报表能够帮助我们描述事件发展的情况，但很难解释某种结果发生的原因和未来可能的趋势。</p>
<p>它更偏向结果性的描述，此前的结果对此后是不具备太多参考意义的。</p>
<p><strong>2）测性分析</strong></p>
<p>表现形式：用户相似度及物品相似度计算、用户购买饱和度、用户成交影响因子</p>
<p>预测性分析可以理解为对结果和变量的关系进行预测的过程，包含相似度、相关性分析、回归分析等。</p>
<p>相似度多用于推荐算法，通过计算用户的相似度和商品相似度从而推荐给用户。而相关分析用于预测变量的关联性，如用户的成交会受什么因素影响。</p>
<p><strong>3）实证性分析及规范性分析</strong></p>
<p>表现形式：A/B实验</p>
<p>实证性分析，指是什么，偏向于客观；规范性分析指应当做什么，偏向于主观。</p>
<p>在实际使用过程，上述的4种分析类型常常会被混合使用，混合使用时应明确不同类型我们应采取的分析维度。</p>
<p>数据分析是有顺承关系的，先采集事实，再根据事实或者预测，提出我们的假设。逐步灰度地验证假设，最终才输出我们的结论。</p>
<p>不能将主观猜测强加于事实之上，<strong>已经发生的结果并不一定是未来的结果</strong></p>
<p><strong>02 数据分析如何带来长期价值</strong></p>
<p>为了使有用功更多，下文将从<strong>用户</strong>和<strong>收益</strong>2个维度分享数据如何为我们沉淀长期价值。</p>
<p><strong>2-1、了解我们的用户</strong></p>
<p><strong>1）基础信息</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8MwDpLTOtyiaLCECkfK0mbQjK5axB94KNVB08VlbaM92685Uf6ms9mEkSXEFiaEHEzYTWqiakXOhmrg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>基础信息，指用户本身的属性。</p>
<p>身份特征，可以从自然属性、社会属性向下细分，包含用户的性别、年龄、职业、教育等。</p>
<p>渠道属性，指用户的注册时间、注册平台、注册来源等。</p>
<p><strong>2）决策类型</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8MwDpLTOtyiaLCECkfK0mbQXKm0NUBtLk9Dh5NUGCgoicSf4H4bDukPZ5XUlLEY2Ie8wO8viaiaEU8yg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>决策类型，主要分为决策周期、品类偏好、促销偏好、对象偏好，</strong>这是用户分析中常常被忽略的一方面。</p>
<p>决策周期中的首次访问，指的首次触及该商品的时间。结合次数、时长以及成交时间，从而了解用户的决策周期。</p>
<p>品类偏好，结合品牌和历史成交单数，能够帮助我们获悉品牌、价格综合对用户的影响。</p>
<p>而成交品类、商品、单数则是帮助我们理解其品类购买深度及路径，用于进行关联推荐和评判用户的价值。</p>
<p>促销偏好，结合品类和折扣金额了解用户的敏感度，能更好的提高其转化率。对象偏好，同样是了解购买深度及路径，不过维度不同。</p>
<p>在用户层面的分析，此前接触的一些朋友都非常热衷于使用RFM模型，在使用过程中也应“因地制宜”。</p>
<p><strong>
</strong></p>
<p><strong>3）购买路径</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8MwDpLTOtyiaLCECkfK0mbQ3ZACcSicicEroxkWZTVVicpHYFnGC6LhpHjwvaClE2rVV9K4NmrBL6TSQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>品类深度、对象深度是影响决策类型的因子，当它们在购买路径时则聚焦于次序。</p>
<p>根据次序，制定运营的发力点，再遵循用户的购买路径制定转化路径。</p>
<p>在用户分布相对稳定的前提下，应<strong>顺从用户的购买规律而非倾力于另一条主线。</strong></p>
<p>一专多强的前提是专，只有聚焦优势品类或主题建立了优势，才能为其他的方向供应炮弹。</p>
<p><strong>4）增长观察</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8MwDpLTOtyiaLCECkfK0mbQcA4bCRYXYqRgbl0uGWVltwQZpoKpdHxjVG8fkavAaZhzo9Nz9apjfw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>前面解决的问题是：他是谁，买什么以及怎么买。最后一点，则是增长观察。</p>
<p>购买路径聚焦于次序，<strong>增长观察聚焦于深度</strong>。购买的次序是运营的主线，购买的深度用于精细化运营。</p>
<p>了解用户在品类和对象的购买深度，再辅以ARPU与LTV的比对，从用户的剩余潜力寻找平台增长点的方式。</p>
<p><strong>2-2、建立你的用户模型</strong></p>
<p>当时我把平台用户的地域年龄、性别等分布介绍了一番。紧接着他提问：“<strong>根据这样的画像你能够做什么呢？</strong>”<img src="http://mmbiz.qpic.cn/mmbiz_png/Pk2wLZrp4l8MwDpLTOtyiaLCECkfK0mbQp6ibPM1mLleO2Zu2Q96v0iaicPMVz52WwIHmA4ia3QwD4ttlJrfzAVKczA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>基于对用户的认识建立模型，以上一小节的决策模型为例。</p>
<p>将决策类型、品类偏好、对象偏好、促销偏好4个因子的关联，并辅以用户的基础信息进行组合。</p>
<p>如：“精打细算、专注大牌、疼爱孩子的母亲”。</p>
<p>这样一来冰冷的数据也被赋予了情感化的表达，无论是产品设计、交互设计、产品运营都会变得容易的多。</p>
<p>建立起用户模型，才能够更好地进行情感化设计、精细化运营。</p>
<p><a href="https://mp.weixin.qq.com/s/eWYiHNJ57aXtqygitnwVqw">https://mp.weixin.qq.com/s/eWYiHNJ57aXtqygitnwVqw</a></p>
<h1><span id="shu-ju-fen-xi-de-liu-cheng">数据分析的流程</span><a href="#shu-ju-fen-xi-de-liu-cheng" class="header-anchor">#</a></h1><p><img src="https://picb.zhimg.com/v2-78b75b190181c603402346bd1194c495_r.jpg" alt="img"></p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/</url>
    <content><![CDATA[<h1><span id="principal-component-analysis-zhu-cheng-fen-fen-xi">Principal Component Analysis**  : 主成分分析</span><a href="#principal-component-analysis-zhu-cheng-fen-fen-xi" class="header-anchor">#</a></h1><h2><span id="xing-xiang-li-jie">形象理解</span><a href="#xing-xiang-li-jie" class="header-anchor">#</a></h2><p>如图，下面是一张3d的图片，从不同的方向投影出来的二维图，可以看出右往左投影的含有更多信息。</p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133219347.png" alt="image-20191102133219347"></p>
<p>如图，下面是一个高斯分布，二维点往两个正交的方向投影，长轴含有的信息更多。</p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133339237.png" alt="image-20191102133339237"></p>
<h2><span id="pca-de-guo-cheng-shi-yi">PCA的过程示意</span><a href="#pca-de-guo-cheng-shi-yi" class="header-anchor">#</a></h2><p><strong>Step 1 </strong> : 去中心。中心在坐标轴在（0,0)，均值在坐标轴原点</p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133416190.png" alt="image-20191102133416190"></p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133422942.png" alt="image-20191102133422942"></p>
<p><strong>Step 2</strong>  : Remove correlation(去除相关性)</p>
<p>通过坐标变化，坐标旋转，矩阵作用</p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133442399.png" alt="image-20191102133442399"></p>
<h3><span id="shu-xue-tui-dao">数学推导</span><a href="#shu-xue-tui-dao" class="header-anchor">#</a></h3><p>目标：变换后的矩阵，对角非零，非对角线全为零。S(Y)有非零的对角元素，所有非对角元素都是零 </p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133714679.png" alt="image-20191102133714679"></p>
<h3><span id="li-lun-tui-dao">理论推导</span><a href="#li-lun-tui-dao" class="header-anchor">#</a></h3><p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133819074.png" alt="image-20191102133819074"></p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133929665.png" alt="image-20191102133929665"></p>
<h3><span id="pca-examples">PCA Examples</span><a href="#pca-examples" class="header-anchor">#</a></h3><p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102133956360.png" alt="image-20191102133956360"></p>
<h3><span id="pca-bias">PCA bias</span><a href="#pca-bias" class="header-anchor">#</a></h3><p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102134003219.png" alt="image-20191102134003219"></p>
<p><img src="/2019/11/25/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/Users\ADMIN\AppData\Roaming\Typora\typora-user-images\image-20191102134023435.png" alt="image-20191102134023435"></p>
]]></content>
  </entry>
  <entry>
    <title>Apriori</title>
    <url>/2019/11/25/Apriori/</url>
    <content><![CDATA[<p>Apriori 算法是一种挖掘关联规则的频繁项集的算法。</p>
<a id="more"></a>
<h2><span id="yin-yan">引言</span><a href="#yin-yan" class="header-anchor">#</a></h2><p>对于特征构成的集合$A$, 如果列出非空集合有$a^{|A|}-1$种，太恐怖了。</p>
<p>Aprior算法：核心想法是</p>
<ol>
<li>L_1是频繁的，则其子集也是频繁的。</li>
<li>L_1是非频繁的，则其超集是非频繁的</li>
</ol>
<p>这样的化，就大大减小了搜索空间了。</p>
<p>Aprior算法的过程：</p>
<p>$C_i$：表示数据集生成候选项集</p>
<p>$L_i$:表示生成的频繁项集</p>
<p>$C_{k-1}$产生$L_k$</p>
<h3><span id="zhi-chi-du">支持度</span><a href="#zhi-chi-du" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
support(\{A,B\}) = num\{AUB\}/W = P(A \ bing \ B)</script><p>W:总的记录，</p>
<h3><span id="zhi-xin-du">置信度</span><a href="#zhi-xin-du" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
Confidence(A->B) = support(\{A,B\})/support(B) = P(B/A)</script><p>注意：support(B)和Confidence(A-&gt;B)的影响，</p>
<h2><span id="xu-lie-mo-xing">序列模型</span><a href="#xu-lie-mo-xing" class="header-anchor">#</a></h2><p>考虑时间，如周一买一堆对象，周二买一堆东西</p>
<script type="math/tex; mode=display">
t= {t_1,t_2,..,t_n}\\
s = {s_1,s_2,..,s_n}</script><p>&lt;{s1},{s_2}&gt;是正确的</p>
<p>&lt;{s1,s2}}&gt;是错误的表达</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>关联规则</tag>
      </tags>
  </entry>
  <entry>
    <title>简单的数据探索</title>
    <url>/2019/11/20/%E7%AE%80%E5%8D%95%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/</url>
    <content><![CDATA[<h1><span id="jian-dan-de-tan-suo-shu-ju-de-fang-fa">简单的探索数据的方法</span><a href="#jian-dan-de-tan-suo-shu-ju-de-fang-fa" class="header-anchor">#</a></h1><p>总结一些简单的数据分析方法，以及常用的python 库 Pandas里面相应的函数。</p>
<a id="more"></a>
<h2><span id="tong-ji-hui-zong">统计汇总</span><a href="#tong-ji-hui-zong" class="header-anchor">#</a></h2><h3><span id="dan-ge-te-zheng">单个特征</span><a href="#dan-ge-te-zheng" class="header-anchor">#</a></h3><figure class="highlight txt"><table><tbody><tr><td class="code"><pre><span class="line">decrible() # 给出样本的基本统计量</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li><p>频率</p>
</li>
<li><p>众数</p>
</li>
<li><p>百分位数</p>
</li>
</ul>
<ul>
<li>位置度量</li>
</ul>
<ul>
<li>均值和方差</li>
</ul>
<ul>
<li>散布度量： 极差和方差</li>
</ul>
]]></content>
      <categories>
        <category>数据挖掘</category>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据探索</tag>
      </tags>
  </entry>
  <entry>
    <title>英语Daily</title>
    <url>/2019/08/25/%E8%8B%B1%E8%AF%ADDaily/</url>
    <content><![CDATA[<h1><span id="2020">2020</span><a href="#2020" class="header-anchor">#</a></h1><h2><span id="09">09</span><a href="#09" class="header-anchor">#</a></h2><h3><span id="05">05</span><a href="#05" class="header-anchor">#</a></h3><p>上星期我去<strong>看戏</strong>。我的<strong>座位</strong>很好，戏很有意思，但我却无法欣赏。一青年男子与一青年女子<strong>坐在我的身后</strong>，大声地说着话。我非常生气，因为我听不见演员在说什么。我<strong>回过头</strong>去<strong>怒视</strong>着那一男一女，他们却毫不理会。最后，我忍不住了，又一次回过头去，<strong>生气地说</strong>：”我一个字也听不见了！”</p>
<p>“<strong>不关你的事</strong>，”那男的毫不客气地说，”这是私人间的谈话！”</p>
]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>新概念</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Basics</title>
    <url>/2019/05/28/Python-basic/</url>
    <content><![CDATA[<h1><span id><!-- more  --></span><a href="#" class="header-anchor">#</a></h1><h1><span id="chong-xin-xue-xi">重新学习</span><a href="#chong-xin-xue-xi" class="header-anchor">#</a></h1><p>开始很乱的学习Python，现在想系统学习基础，真正了解pythonic,</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Networks</title>
    <url>/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/</url>
    <content><![CDATA[<h1><span id="c4-convolutional-neural-networks-juan-ji-shen-jing-wang-luo">C4 : Convolutional Neural Networks(卷积神经网络)</span><a href="#c4-convolutional-neural-networks-juan-ji-shen-jing-wang-luo" class="header-anchor">#</a></h1><h2><span id="w1-convolutional-neural-networks-juan-ji-shen-jing-wang-luo">W1 :Convolutional Neural Networks(卷积神经网络)</span><a href="#w1-convolutional-neural-networks-juan-ji-shen-jing-wang-luo" class="header-anchor">#</a></h2><h3><span id="l1-computer-vision">L1: Computer Vision</span><a href="#l1-computer-vision" class="header-anchor">#</a></h3><ol>
<li>Image classification</li>
<li>Object detection</li>
<li>Neural Style Transfer</li>
</ol>
<p>Problem : input big</p>
<ol>
<li>神经网络结构复杂，数据量相对较少，容易出现过拟合；</li>
<li>所需内存和计算量巨大。</li>
</ol>
<h3><span id="l2-edge-detection-example">L2: Edge detection example</span><a href="#l2-edge-detection-example" class="header-anchor">#</a></h3><p>我们之前提到过，神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等），到最后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。</p>
<p><strong>卷积运算（Convolutional Operation）</strong>是卷积神经网络最基本的组成部分。我们以边缘检测为例，来解释卷积是怎样运算的。</p>
<ol>
<li><p>常见的边缘检测</p>
<p>垂直边缘（Vertical Edges) 和 水平边缘（horizontal Edges)</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Different-edges.png" alt></p>
</li>
</ol>
<p>这张图的栏杆就对应垂直线，栏杆的水平线是水平边缘。</p>
<p>那么图片是怎么检测边缘的呢？</p>
<p>过滤器：filter</p>
<p>在数学中“”就是卷积的标准标志，但是在<strong>Python</strong>中，这个标识常常被用来表示乘法或者元素乘法。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1.png" alt></p>
<p>Output; 4 by 4</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1——2.png" alt></p>
<p>具体运算：</p>
<p>1）</p>
<p>为了计算第一个元素，在4×4左上角的那个元素，使用3×3的过滤器，将其覆盖在输入图像，如下图所示。然后进行元素乘法（<strong>element-wise products</strong>）运算</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_3png.png" alt></p>
<p>2）为了弄明白第二个元素是什么，你要把蓝色的方块，向右移动一步，像这样，把这些绿色的标记去掉：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_4png.png" alt></p>
<p>6×6矩阵和3×3矩阵进行卷积运算得到4×4矩阵。这些图片和过滤器是不同维度的矩阵，但左边矩阵容易被理解为一张图片，中间的这个被理解为过滤器，右边的图片我们可以理解为另一张图片。这个就是垂直边缘检测器。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutional-operation.jpg" alt></p>
<p>举例说明： Vertical edge detection</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_5.png" alt></p>
<p>这里在结果可能有点不对头，检测到的边缘太粗了，主要是图片太小了，</p>
<p>卷积操作API</p>
<ul>
<li>在 Python 中，卷积用<code>conv_forward()</code>表示；</li>
<li>在 Tensorflow 中，卷积用<code>tf.nn.conv2d()</code>表示；</li>
<li>在 keras 中，卷积用<code>Conv2D()</code>表示。</li>
</ul>
<h3><span id="l3-edge-detection-example">L3: Edge Detection Example</span><a href="#l3-edge-detection-example" class="header-anchor">#</a></h3><ol>
<li><p>颜色由暗到亮，还是亮到暗</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_1.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_2.png" alt></p>
</li>
</ol>
<p>这种滤波器可以区分明暗变化，取绝对值没有区别了</p>
<ol>
<li><p>水平边缘</p>
<p>上边相对较亮，而下方相对较暗</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_3.png" alt></p>
<ol>
<li>复杂栗子</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_4.png" alt></p>
</li>
</ol>
<p>这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这个一个非常大的1000×1000的类似这样棋盘风格的大图，就不会出现这些亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小。</p>
<ol>
<li>filter</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_5.png" alt></p>
<p>sobel过滤器，优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。</p>
<p>charr过滤器，它有着和之前完全不同的特性，实际上也是一种垂直边缘检测，如果你将其翻转90度，你就能得到对应水平边缘检测。</p>
<p>学习的其中一件事就是当你真正想去检测出复杂图像的边缘，你不一定要去使用那些研究者们所选择的这九个数字，但你可以从中获益匪浅。把这矩阵中的9个数字当成9个参数，并且在之后你可以学习使用反向传播算法，其目标就是去理解这9个参数。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png" alt></p>
<p>这样可能得到一个出色的边缘检测</p>
<p>相比这种单纯的垂直边缘和水平边缘，它可以检测出45°或70°或73°，甚至是任何角度的边缘。所以将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，我们会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。</p>
<p>不管是垂直的边缘，水平的边缘，还有其他奇怪角度的边缘，甚至是其它的连名字都没有的过滤器。</p>
<h3><span id="padding">Padding</span><a href="#padding" class="header-anchor">#</a></h3><p>按照我们上面讲的图片卷积，如果原始图片尺寸为$n x n$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n-f+1) x (n-f+1)$，注意f一般为奇数。这样会带来两个问题：</p>
<ul>
<li><p><strong>卷积运算后，输出图片尺寸缩小</strong></p>
</li>
<li><p><strong>原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息</strong></p>
<p>边缘像素点只被一个输出所触碰或者使用，</p>
</li>
</ul>
<p>为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行<strong>填充（Padding）</strong>，以增加矩阵的大小。通常将 0 作为填充值。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Padding.jpg" alt></p>
<p>经过padding之后，填充p,原始图片尺寸为$(n+2p) x (n+2p)$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n+2p-f+1) x (n+2p-f+1)$。若要保证卷积前后图片尺寸不变，则p应满足：$ p=(f-1)/2$,f通常是奇数，如果是偶数，造成不对称填充，第二个原因是当你有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点。有时在计算机视觉里，如果有一个中心像素点会更方便，便于指出过滤器的位置</p>
<ol>
<li>p=0,Valid convolution</li>
<li>p=((f-1))/2,Same convolution</li>
</ol>
<h3><span id="l05-strided-convolution-juan-ji-bu-chang">L05: Strided convolution（卷积步长）</span><a href="#l05-strided-convolution-juan-ji-bu-chang" class="header-anchor">#</a></h3><p>Stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前我们默认stride=1。若stride=2，则表示filter每次步进长度为2，即隔一点移动一次。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Stride.jpg" alt></p>
<p>我们用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为：</p>
<script type="math/tex; mode=display">
\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor X\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor</script><p>向下取整</p>
<p>目前为止我们学习的“卷积”实际上被称为<strong>互相关（cross-correlation）</strong>，而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png" alt></p>
<p>互相关：过滤器沿水平和垂直轴翻转，元素相乘来计算，这些视频中定义卷积运算时，我们跳过了这个镜像操作。（不进行翻转操作）叫做卷积操作</p>
<h3><span id="l06-convolution-over-volumes-san-wei-juan-ji">L06: Convolution over volumes(三维卷积)</span><a href="#l06-convolution-over-volumes-san-wei-juan-ji" class="header-anchor">#</a></h3><ol>
<li><p>卷积运算</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutions-on-RGB-image.png" alt></p>
</li>
</ol>
<p>过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值。</p>
<p>不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。</p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_8.png" alt></p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_9.png" alt></p>
<p>若输入图片的尺寸为n x n x nc，nc: 通道数目，filter尺寸为f x f x nc，则卷积后的图片尺寸为(n-f+1) x (n-f+1) x nc′。其中，nc为图片通道数目，nc′为滤波器组个数。</p>
<h3><span id="l7-one-layer-of-a-convolution-network-dan-ceng-shen-jing-wang-luo">L7 : One layer of a convolution network (单层神经网络)</span><a href="#l7-one-layer-of-a-convolution-network-dan-ceng-shen-jing-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_10.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_11.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_12.png" alt></p>
<p>CNN单层的所以标记符号，设层数$l$,</p>
<script type="math/tex; mode=display">
\begin{array}{l}{f^{[l]}=\text { filter size }} \\ {p^{[l]}=\text { padding }} \\ {g^{[l]}=\text { stride }} \\ {n_{c}^{[l]}=\text { number of filters }}\end{array}</script><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_13.png" alt></p>
<script type="math/tex; mode=display">
\begin{array}{c}{n_{H}^{[l]}=\left\lfloor\frac{n_{H}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor} \\ { n_{W}^{[l]}=\left\lfloor\frac{n_{W}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor}\end{array}</script><p>如果$m$个样本，进行向量化运算，相应的输出维度，为</p>
<script type="math/tex; mode=display">
\mathrm{m} \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}</script><h3><span id="l8-a-simple-convolution-network-example-jian-dan-juan-ji-wang-luo-shi-li">L8 : A simple convolution network example（简单卷积网络示例）</span><a href="#l8-a-simple-convolution-network-example-jian-dan-juan-ji-wang-luo-shi-li" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-33.jpg" alt></p>
<ul>
<li>一般而言，<strong>图片的height $n^{[l]}_{H}$和width $n^{[l]}_W$随着层数的增加逐渐降低，但channel $n^{[l]}_C$逐渐增加</strong>。</li>
</ul>
<p>CNN有三种类型的layer：</p>
<ul>
<li>Convolution层（CONV）</li>
<li>Pooling层（POOL）</li>
<li>Fully connected层（FC）</li>
</ul>
<h3><span id="l9-pooling-layers-chi-hua-ceng">L9: Pooling layers(池化层)</span><a href="#l9-pooling-layers-chi-hua-ceng" class="header-anchor">#</a></h3><p>卷积神经网络除了卷积层，还有池化层来缩减模型的大小，提高运算速度和鲁棒性</p>
<ol>
<li>池的类型有max pooling(最大池化)</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_14.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_15.png" alt></p>
<p>这里步幅是s=2，filter = 2*2是最大池化的超参数,如果是三维，则单独在每个通道执行最大池化操作</p>
<p>关于max pooling的直觉解释： 元素较大的值，可能是卷积过程中提取到的某些特征（比如边界），而max pooling则在压缩了矩阵大小的情况下，保留每个分区内最大的输出，即保留了提取的特征。但理论上还没有证明max pooling的原理，max pooling应用的原因是在实践中效果很好。</p>
<ol>
<li><p>Pooling layer: Average pooling</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_16.png" alt></p>
<p>但是最大池化更好用</p>
</li>
</ol>
<p>summary : 输入$n_H<em>n_W</em>n_C$,如果没有padding,输出$(n_h-f)/s+1<em>(n_w-f)/s+1</em>n_c$</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_17.png" alt></p>
<h3><span id="l10-convolutional-neural-network-example-juan-ji-shen-jing-wang-luo-shi-li">L10: Convolutional neural network example (卷积神经网络实例)</span><a href="#l10-convolutional-neural-network-example-juan-ji-shen-jing-wang-luo-shi-li" class="header-anchor">#</a></h3><p>做一个识别数字的CNN网络</p>
<ol>
<li><p>LeNet-5架构如下：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/CNN.jpg" alt></p>
<ul>
<li>通常Conv Layer和Pooling Layer合在一起算一个layer，因为pooling layer并没有参数训练</li>
</ul>
</li>
</ol>
<ul>
<li>常见的结构：Conv ==&gt; Pool ==&gt; Conv ==&gt; Pool ==&gt; FC ==&gt; FC ==&gt; softmax</li>
<li>最终还会用FC层（全连接层），与一般NN的处理一样；并在输出层，应用softmax得到10个数字的概率。</li>
<li>在整个网络中，Height和Width是逐渐递减的，但channel和filter是递增的。</li>
<li>关于CNN如何选择超参：可以参考论文的经验。</li>
<li><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_18.png" alt></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Activation shape</th>
<th style="text-align:center">Activation Size</th>
<th>#parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Input:</strong></td>
<td style="text-align:center">(32, 32, 3)</td>
<td style="text-align:center">3072</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV1(f=5, s=1)</strong></td>
<td style="text-align:center">(28, 28, 6)</td>
<td style="text-align:center">4704</td>
<td>156 (=5<em>5</em>6+6)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL1</strong></td>
<td style="text-align:center">(14, 14, 6)</td>
<td style="text-align:center">1176</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV2(f=5, s=1)</strong></td>
<td style="text-align:center">(10, 10, 16)</td>
<td style="text-align:center">1600</td>
<td>416 (=5<em>5</em>16+16)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL2</strong></td>
<td style="text-align:center">(5, 5, 16)</td>
<td style="text-align:center">400</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC3</strong></td>
<td style="text-align:center">(120, 1)</td>
<td style="text-align:center">120</td>
<td>48120 (=120*400+120)</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC4</strong></td>
<td style="text-align:center">(84, 1)</td>
<td style="text-align:center">84</td>
<td>10164 (=84*120+84)</td>
</tr>
<tr>
<td style="text-align:center"><strong>Softmax</strong></td>
<td style="text-align:center">(10, 1)</td>
<td style="text-align:center">10</td>
<td>850 (=10*84+10)</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="l11-why-convolution">L11 Why convolution</span><a href="#l11-why-convolution" class="header-anchor">#</a></h3><ul>
<li><p>参数共享（parameter sharing)</p>
<p> 如果用FC的话，参数爆炸啊！如果conv layer 就需要filter检测器，这个参数就少了，还参数共享</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_19.png" alt></p>
</li>
<li><p>稀疏连接(sparsity of connection)</p>
<p>输出中的每个单元仅和输入的一个小分区相关，比如输出的左上角的像素仅仅由输入左上角的9个像素决定（假设filter大小是3*3），而其他输入都不会影响。</p>
</li>
</ul>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_20.png" alt></p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="read">1. 卷积神经网络的基本构造和计算过程 2. 如何整合这些模型 3.  哪些超参数 4. 为什么使用卷积 </font>

<h2><span id="w2-deep-convolutional-models-case-studies-shen-du-juan-ji-wang-luo-shi-li-tan-jiu">W2 : Deep convolutional models: case studies(深度卷积网络：实例探究)</span><a href="#w2-deep-convolutional-models-case-studies-shen-du-juan-ji-wang-luo-shi-li-tan-jiu" class="header-anchor">#</a></h2><h3><span id="l1-why-look-at-case-studies-wei-shi-me-yao-jin-xing-shi-li-tan-jiu">L1 : Why look at case studies?(为什么要进行实例探究？)</span><a href="#l1-why-look-at-case-studies-wei-shi-me-yao-jin-xing-shi-li-tan-jiu" class="header-anchor">#</a></h3><p>本文将主要介绍几个典型的CNN案例。通过对具体CNN模型及案例的研究，来帮助我们理解知识并训练实际的模型。</p>
<p>典型的CNN模型包括：</p>
<ul>
<li><strong>LeNet-5</strong></li>
<li><strong>AlexNet</strong></li>
<li><strong>VGG</strong></li>
</ul>
<p>还会介绍Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。还会介绍Inception Neural Network</p>
<h3><span id="l2-classic-networks-jing-dian-wang-luo">L2 : Classic networks(经典网络)</span><a href="#l2-classic-networks-jing-dian-wang-luo" class="header-anchor">#</a></h3><h4><span id="1-lenet-5">1. LeNet-5</span><a href="#1-lenet-5" class="header-anchor">#</a></h4><p><strong>LeNet-5</strong>是针对灰度图片训练的，使用6个5×5的过滤器，步幅为1。由于使用了6个过滤器，步幅为1，<strong>padding</strong>为0，输出结果为28×28×6，图像尺寸从32×32缩小到28×28。然后进行池化操作，在这篇论文写成的那个年代，人们更喜欢使用平均池化，而现在我们可能用最大池化更多一些。在这个例子中，我们进行平均池化，过滤器的宽度为2，步幅为2，图像的尺寸，高度和宽度都缩小了2倍，输出结果是一个14×14×6的图像。我觉得这张图片应该不是完全按照比例绘制的，如果严格按照比例绘制，新图像的尺寸应该刚好是原图像的一半。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-34.jpg" alt></p>
<p>该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。</p>
<h4><span id="1-alexnet">1. AlexNet</span><a href="#1-alexnet" class="header-anchor">#</a></h4><p>AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-35.jpg" alt></p>
<p><strong>AlexNet</strong>首先用一张227×227×3的图片作为输入，实际上原文中使用的图像是224×224×3，但是如果你尝试去推导一下，你会发现227×227这个尺寸更好一些。第一层我们使用96个11×11的过滤器，步幅为4，由于步幅是4，因此尺寸缩小到55×55，缩小了4倍左右。然后用一个3×3的过滤器构建最大池化层,f=3，步幅为2，卷积层尺寸缩小为27×27×96。接着再执行一个5×5的卷积，<strong>padding</strong>之后，输出是27×27×276。然后再次进行最大池化，尺寸缩小到13×13。再执行一次<strong>same</strong>卷积，相同的<strong>padding</strong>，得到的结果是13×13×384，384个过滤器。再做一次<strong>same</strong>卷积，就像这样。再做一次同样的操作，最后再进行一次最大池化，尺寸缩小到6×6×256。6×6×256等于9216，将其展开为9216个单元，然后是一些全连接层。最后使用<strong>softmax</strong>函数输出识别的结果，看它究竟是1000个可能的对象中的哪一个。</p>
<p>实际上，这种神经网络与<strong>LeNet</strong>有很多相似之处，不过<strong>AlexNet</strong>要大得多。正如前面讲到的<strong>LeNet</strong>或<strong>LeNet-5</strong>大约有6万个参数，而<strong>AlexNet</strong>包含约6000万个参数。当用于训练图像和数据集时，<strong>AlexNet</strong>能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，这一点<strong>AlexNet</strong>表现出色。<strong>AlexNet</strong>比<strong>LeNet</strong>表现更为出色的另一个原因是它使用了<strong>ReLu</strong>激活函数。原作者还提到了一种优化技巧，叫做Local Response Normalization(LRN)。 而在实际应用中，LRN的效果并不突出。</p>
<h4><span id="3-vgg-16">3. VGG-16</span><a href="#3-vgg-16" class="header-anchor">#</a></h4><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-36.jpg" alt></p>
<p>首先用3×3，步幅为1的过滤器构建卷积层，<strong>padding</strong>参数为<strong>same</strong>卷积中的参数。然后用一个2×2，步幅为2的过滤器构建最大池化层。因此<strong>VGG</strong>网络的一大优点是它确实简化了神经网络结构，下面我们具体讲讲这种网络结构。</p>
<p>数字16，就是指在这个网络中包含16个卷积层和全连接层。总共包含约1.38亿个参数</p>
<h3><span id="l3-residual-networks-resnets-can-chai-wang-luo-resnets">L3 : Residual Networks (ResNets)(残差网络(ResNets))</span><a href="#l3-residual-networks-resnets-can-chai-wang-luo-resnets" class="header-anchor">#</a></h3><p>我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为Residual Networks(ResNets)。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_21.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Residual-Network.jpg" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/ResNet-Training-Error.jpg" alt></p>
<h3><span id="l4-why-resnets-work-can-chai-wang-luo-wei-shi-me-you-yong">L4: Why ResNets work?(残差网络为什么有用？)</span><a href="#l4-why-resnets-work-can-chai-wang-luo-wei-shi-me-you-yong" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_22.png" alt></p>
<p>因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。</p>
<p>注意，如果$ a[l]$与 $a[l+2]$的维度不同，需要引入矩阵 $W_s$与 $a_{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a[l]$截断或者补零。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-37.jpg" alt></p>
<h3><span id="l5-network-in-network-and-1x1-convolutions-wang-luo-zhong-de-wang-luo-yi-ji-1x1-juan-ji">L5 : Network in Network and 1×1 convolutions(网络中的网络以及 1×1 卷积)</span><a href="#l5-network-in-network-and-1x1-convolutions-wang-luo-zhong-de-wang-luo-yi-ji-1x1-juan-ji" class="header-anchor">#</a></h3><ol>
<li>作用 </li>
</ol>
<p>假设这是一个28×28×192的输入层，你可以使用池化层压缩它的高度和宽度，这个过程我们很清楚。但如果通道数量很大，该如何把它压缩为28×28×32维度的层呢？你可以用32个大小为1×1的过滤器，严格来讲每个过滤器大小都是1×1×192维，因为过滤器中通道数量必须与输入层中通道的数量保持一致。但是你使用了32个过滤器，输出层为28×28×32，这就是压缩通道数（$n_c$）的方法，对于池化层我只是压缩了这些层的高度和宽度</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_23.png" alt></p>
<ol>
<li><strong>doing something pretty non-trivial</strong></li>
</ol>
<p>它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，当然如果你愿意，也可以增加通道数量。</p>
<h3><span id="l6-inception-network-motivation-gu-ge-inception-wang-luo-jian-jie">L6 : Inception network motivation(谷歌 Inception 网络简介)</span><a href="#l6-inception-network-motivation-gu-ge-inception-wang-luo-jian-jie" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/99f8fc7dbe7cd0726f5271aae11b9872.png" alt></p>
<p>有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。</p>
<p> 1x1 的卷积层通常被称作<strong>瓶颈层（Bottleneck layer）</strong></p>
<p>计算量为 28x28x32x5x5x192 = 1.2亿</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/The-problem-of-computational-cost.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Using-1x1-convolution.png" alt></p>
<p>28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。</p>
<h3><span id="l7-inception-network-inception-wang-luo">L7 : Inception network(Inception 网络)</span><a href="#l7-inception-network-inception-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_24.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_25.png" alt></p>
<h3><span id="l8-using-open-source-implementations-shi-yong-kai-yuan-de-shi-xian-fang-an">L8 : Using open-source implementations( 使用开源的实现方案)</span><a href="#l8-using-open-source-implementations-shi-yong-kai-yuan-de-shi-xian-fang-an" class="header-anchor">#</a></h3><p>开源项目</p>
<h3><span id="l9-transfer-learning-qian-yi-xue-xi">L9 ： Transfer Learning（迁移学习）</span><a href="#l9-transfer-learning-qian-yi-xue-xi" class="header-anchor">#</a></h3><p>如果你下载别人已经训练好网络结构的权重，你通常能够进展的相当快，用这个作为预训练，然后转换到你感兴趣的任务上。</p>
<ol>
<li>只有很小数据集： 可以你只需要训练<strong>softmax</strong>层的权重，把前面这些层的权重都冻结。</li>
<li>稍微更大的数据集： 你应该冻结更少的层，比如只把这些层冻结，然后训练后面的层。如果你的输出层的类别不同，那么你需要构建自己的输出单元；或者你可以直接去掉这几层，换成你自己的隐藏单元和你自己的<strong>softmax</strong>输出层，这些方法值得一试。</li>
<li>大量数据： 你可以用下载的权重只作为初始化，用它们来代替随机初始化，接着你可以用梯度下降训练，更新网络所有层的所有权重。</li>
</ol>
<h3><span id="l10-data-augmentation-shu-ju-zeng-qiang">L10 ： Data augmentation（数据增强）</span><a href="#l10-data-augmentation-shu-ju-zeng-qiang" class="header-anchor">#</a></h3><p>数据量远远不够</p>
<ol>
<li>Mirroring</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring.png" alt></p>
<ol>
<li>Random Cropping</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_1.png" alt></p>
<ol>
<li><p>彩色转换color shifting</p>
<p>r,g,b数据改变</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_2.png" alt></p>
<p>除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_3.png" alt></p>
<p>常用的实现数据扩充的方法是使用一个线程或者是多线程，这些可以用来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练这个（编号2）和这个（编号1），可以并行实现。</p>
<h3><span id="l11-the-state-of-computer-vision-ji-suan-ji-shi-jue-xian-zhuang">L11：The state of computer vision(计算机视觉现状)</span><a href="#l11-the-state-of-computer-vision-ji-suan-ji-shi-jue-xian-zhuang" class="header-anchor">#</a></h3><ol>
<li>神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理。</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_4.png" alt></p>
<p>hand-engineering是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响很大，特别是在数据量不多的情况下。</p>
<p>当你有少量的数据时，有一件事对你很有帮助，那就是迁移学习。在别人做好的基础上研究</p>
<ol>
<li><p>提升性能</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_5.png" alt>*</p>
</li>
</ol>
<p>由于计算机视觉问题建立在小数据集之上，其他人已经完成了大量的网络架构的手工工程。一个神经网络在某个计算机视觉问题上很有效，但令人惊讶的是它通常也会解决其他计算机视觉问题。</p>
<p>所以，要想建立一个实用的系统，你最好先从其他人的神经网络架构入手。如果可能的话，你可以使用开源的一些应用，因为开放的源码实现可能已经找到了所有繁琐的细节，比如学习率衰减方式或者超参数。</p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">1. CNN的常见网络结构 重点说了一些残差网络 2.数据增加的方法 3. 多用开源框架，不用从头开始训练 </font>

<h1><span id="w3-object-detection-mu-biao-jian-ce">W3 Object detection(目标检测)</span><a href="#w3-object-detection-mu-biao-jian-ce" class="header-anchor">#</a></h1><h3><span id="l1-object-localization-mu-biao-ding-wei">L1 :Object localization(目标定位)</span><a href="#l1-object-localization-mu-biao-ding-wei" class="header-anchor">#</a></h3><p>目标定位和目标检测</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_1.png" alt></p>
<p>模型</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_2.png" alt></p>
<p>输入还包括位置信息</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_3.png" alt></p>
<p>损失函数</p>
<p>情况一：检测到了</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_4.png" alt></p>
<p>情况二：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_5.png" alt></p>
<h3><span id="l2-landmark-detection-te-zheng-dian-jian-ce">L2: Landmark detection(特征点检测)</span><a href="#l2-landmark-detection-te-zheng-dian-jian-ce" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_6.png" alt></p>
<p>该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值。通过检测人脸特征点可以进行情绪分类与判断，或者应用于AR领域等等。</p>
<p>除了人脸特征点检测之外，还可以检测人体姿势动作，如下图所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_7.png" alt></p>
<h3><span id="l3-object-detection-mu-biao-jian-ce">L3 :Object detection(目标检测)</span><a href="#l3-object-detection-mu-biao-jian-ce" class="header-anchor">#</a></h3><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。这节课，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_8.png" alt></p>
<p>训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p>
<p>选定特定大小的窗口，窗口圈定输入卷积神经网络，卷积神经网络开始预测。</p>
<p>重复上述操作，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或<img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_10.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_11.png" alt></p>
<p>如果你这样做，不论汽车在图片的什么位置，总有一个窗口可以检测到它。</p>
<p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p>
<p>滑动窗算法的优点是原理简单，且不需要人为选定目标区域（检测出目标的滑动窗即为目标区域）。但是其缺点也很明显，首先滑动窗的大小和步进长度都需要人为直观设定。滑动窗过小或过大，步进长度过大均会降低目标检测正确率。而且，每次滑动窗区域都要进行一次CNN网络计算，如果滑动窗和步进长度较小，整个目标检测的算法运行时间会很长。所以，滑动窗算法虽然简单，但是性能不佳，不够快，不够灵活。</p>
<h3><span id="l-4-convolutional-implementation-of-sliding-windows-hua-dong-chuang-kou-de-juan-ji-shi-xian">L 4 : Convolutional implementation of sliding windows(滑动窗口的卷积实现)</span><a href="#l-4-convolutional-implementation-of-sliding-windows-hua-dong-chuang-kou-de-juan-ji-shi-xian" class="header-anchor">#</a></h3><ol>
<li><p>全连接层转化为卷积层</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_12.png" alt></p>
</li>
</ol>
<p>单个窗口区域卷积网络结构建立完毕之后，对于待检测图片，即可使用该网络参数和结构进行运算。例如16 x 16 x 3的图片，步进长度为2，CNN网络得到的输出层为2 x 2 x 4。其中，2 x 2表示共有4个窗口结果。对于更复杂的28 x 28 x3的图片，CNN网络得到的输出层为8 x 8 x 4，共64个窗口结果。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_13.png" alt></p>
<p>之前的滑动窗算法需要反复进行CNN正向计算，例如16 x 16 x 3的图片需进行4次，28 x 28 x3的图片需进行64次。而利用卷积操作代替滑动窗算法，则不管原始图片有多大，只需要进行一次CNN正向计算，因为其中共享了很多重复计算部分，这大大节约了运算成本。值得一提的是，窗口步进长度与选择的MAX POOL大小有关。如果需要步进长度为4，只需设置MAX POOL为4 x 4即可。</p>
<h3><span id="l5-bounding-box-predictions-bounding-box-yu-ce">L5 ： Bounding box predictions（Bounding Box预测）</span><a href="#l5-bounding-box-predictions-bounding-box-yu-ce" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_14.png" alt></p>
<ol>
<li><p>YOLO（You Only Look Once）算法可以解决这类问题，生成更加准确的目标区域（如上图红色窗口）。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_16.png" alt></p>
</li>
<li><p>如果目标中心坐标(bx,by)不在当前网格内，则当前网格Pc=0；相反，则当前网格Pc=1（即只看中心坐标是否在当前网格内）。判断有目标的网格中，bx,by,bh,bw限定了目标区域。值得注意的是，当前网格左上角坐标设定为(0, 0)，右下角坐标设定为(1, 1)，(bx,by)范围限定在[0,1]之间，但是bh,bw可以大于1。因为目标可能超出该网格，横跨多个区域，如上图所示。目标占几个网格没有关系，目标中心坐标必然在一个网格之内。</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_15.png" alt></p>
<h3><span id="l6-intersection-over-union-jiao-bing-bi">L6 ：Intersection over union（交并比)</span><a href="#l6-intersection-over-union-jiao-bing-bi" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_17.png" alt></p>
<p>一般约定，在计算机检测任务中，如果lou&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，<strong>loU</strong>就是1，因为交集就等于并集。但一般来说只要lou&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将<strong>loU</strong>定得更高，比如说大于0.6或者更大的数字，但<strong>loU</strong>越高，边界框越精确。</p>
<h3><span id="l7-non-max-suppression-fei-ji-da-zhi-yi-zhi">L7: Non-max suppression(非极大值抑制)</span><a href="#l7-non-max-suppression-fei-ji-da-zhi-yi-zhi" class="header-anchor">#</a></h3><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次，我们讲一个例子。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_18.png" alt></p>
<p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_19.png" alt></p>
<p>实际情况是格子1，2，3，4，5，6都认为里面有车。因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的pc,我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。</p>
<p>非最大值抑制（Non-max Suppression）做法很简单，图示每个网格的Pc值可以求出，Pc值反映了该网格包含目标中心坐标的可信度。首先选取Pc最大值对应的网格和区域，然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值（例如0.5）的所有网格及区域。这样就能保证同一目标只有一个网格与之对应，且该网格Pc最大，最可信。接着，再从剩下的网格中选取Pc最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。如下图所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_20.png" alt></p>
<p>总结一下非最大值抑制算法的流程：</p>
<ol>
<li><strong>剔除Pc值小于某阈值（例如0.6）的所有网格；</strong></li>
<li><strong>选取Pc值最大的网格，利用IoU，摒弃与该网格交叠较大的网格；</strong></li>
<li><strong>对剩下的网格，重复步骤2。</strong></li>
</ol>
<p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念，我们从一个例子开始讲吧。方法是使用不同形状的Anchor Boxes。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_21.png" alt></p>
<p>这就是<strong>anchor box</strong>的概念，我们建立<strong>anchor box</strong>这个概念，是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生</p>
<h3><span id="l9-yolo-suan-fa-putting-it-together-yolo-algorithm">L9 :  YOLO 算法（Putting it together: YOLO algorithm）</span><a href="#l9-yolo-suan-fa-putting-it-together-yolo-algorithm" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_22.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_23.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_24.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_25.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_26.png" alt></p>
<p>这就是<strong>YOLO</strong>对象检测算法，这实际上是最有效的对象检测算法之一，包含了整个计算机视觉对象检测领域文献中很多最精妙的思路</p>
<h3><span id="region-proposals-optional-hou-xuan-qu-yu-xuan-xiu">Region proposals (Optional)（候选区域（选修））</span><a href="#region-proposals-optional-hou-xuan-qu-yu-xuan-xiu" class="header-anchor">#</a></h3><p>之前介绍的滑动窗算法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，例如下图所示。这样会降低算法运行效率，耗费时间。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_27.png" alt></p>
<p>为了解决这一问题，尽量避免对无用区域的扫描，可以使用Region Proposals的方法。具体做法是先对原始图片进行分割算法处理，然后支队分割后的图片中的块进行目标检测。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_28.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_29.png" alt></p>
<p>Region Proposals共有三种方法：</p>
<ul>
<li><strong>R-CNN: 滑动窗的形式，一次只对单个区域块进行目标检测，运算速度慢。</strong></li>
<li><strong>Fast R-CNN: 利用卷积实现滑动窗算法，类似第4节做法。</strong></li>
<li><strong>Faster R-CNN: 利用卷积对图片进行分割，进一步提高运行速度。</strong></li>
</ul>
<h2><span id="w4-special-applications-face-recognition-amp-neural-style-transfer-te-shu-ying-yong-ren-lian-shi-bie-he-shen-jing-feng-ge-zhuan-huan">W4：Special applications: Face recognition &amp;Neural style transfer( 特殊应用：人脸识别和神经风格转换)</span><a href="#w4-special-applications-face-recognition-amp-neural-style-transfer-te-shu-ying-yong-ren-lian-shi-bie-he-shen-jing-feng-ge-zhuan-huan" class="header-anchor">#</a></h2><h3><span id="c1-what-is-face-recognition">C1 ： What is face recognition?</span><a href="#c1-what-is-face-recognition" class="header-anchor">#</a></h3><p>首先简单介绍一下人脸验证（face verification）和人脸识别（face recognition）的区别。</p>
<ul>
<li><strong>人脸验证：输入一张人脸图片，验证输出与模板是否为同一人，即一对一问题。</strong></li>
<li><strong>人脸识别：输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题。</strong></li>
</ul>
<h3><span id="l2-one-shot-learning">L2 ： One-shot learning</span><a href="#l2-one-shot-learning" class="header-anchor">#</a></h3><p>One-shot learning就是说数据库中每个人的训练样本只包含一张照片，然后训练一个CNN模型来进行人脸识别。若数据库有K个人，则CNN模型输出softmax层就是K维的。</p>
<p>但是One-shot learning的性能并不好，其包含了两个缺点：</p>
<ul>
<li><strong>每个人只有一张图片，训练样本少，构建的CNN网络不够健壮。</strong></li>
<li><strong>若数据库增加另一个人，输出层softmax的维度就要发生变化，相当于要重新构建CNN网络，使模型计算量大大增加，不够灵活。</strong></li>
</ul>
<p>为了解决One-shot learning的问题，我们先来介绍相似函数（similarity function）。相似函数表示两张图片的相似程度，用d(img1,img2)来表示。若d(img1,img2)较小，则表示两张图片相似；若d(img1,img2)较大，则表示两张图片不是同一个人。相似函数可以在人脸验证中使用：</p>
<ul>
<li><strong>d(img1,img2)≤τ : 一样</strong></li>
<li><strong>d(img1,img2)&gt;τ : 不一样</strong></li>
</ul>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_1.png" alt></p>
<p>现在你已经知道函数d是如何工作的，通过输入两张照片，它将让你能够解决一次学习问题。那么，下节视频中，我们将会学习如何训练你的神经网络学会这个函数。</p>
<h3><span id="l3-siamese-network">L3: Siamese network</span><a href="#l3-siamese-network" class="header-anchor">#</a></h3><p>最后一层去掉softmax单元做分类</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_2.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_3.png" alt></p>
<p>如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我要把第二张图片的编码叫做$f(x^{(2)})$。这里我用$x^{(1)}$和$x^{(2)}$仅仅代表两个输入图片,</p>
<script type="math/tex; mode=display">
d(x^{(1)},x^{(2)})=||f(x^{(1)}-f(x^{(2)}||^2</script><p>不同的图片的CNN网络结构和参数都是一样的，目标就是利用梯度下降算法，调整网络参数</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>The Deep Learning Specialization</title>
    <url>/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/</url>
    <content><![CDATA[<h1><span id="c3-improving-model-performance">C3 Improving Model Performance</span><a href="#c3-improving-model-performance" class="header-anchor">#</a></h1><h2><span id="w1-ml-strategy-1">W1 ML Strategy(1)</span><a href="#w1-ml-strategy-1" class="header-anchor">#</a></h2><h3><span id="l01-improving-model-performance">L01 Improving Model Performance</span><a href="#l01-improving-model-performance" class="header-anchor">#</a></h3><p>需要提高训练结果的表现，表现得更好的措施 Machine Learning Strategy</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-24_21-03-13.jpg" alt></p>
<h3><span id="l2-orthogonalization-zheng-jiao-hua">L2 : Orthogonalization(正交化)</span><a href="#l2-orthogonalization-zheng-jiao-hua" class="header-anchor">#</a></h3><p>所谓正交，<strong>就是你的操控效果尽量只影响一个方面</strong>。比如以老式电视机为例，调节图像的大小、左右偏移、上下偏移。而不是一个按钮可以同时调节图像大小和左右偏移，那样会很难操作。</p>
<p>具体到supervised learning，有以下4个假设是正交的？</p>
<ol>
<li>Fit <strong>training set</strong> well in cost function If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</li>
<li>Fit <strong>development set</strong> well on cost function If it doesn’t fit well, regularization or using bigger training set might help.</li>
<li>Fit <strong>test set</strong> well on cost function If it doesn’t fit well, the use of a bigger development set might help</li>
<li>Performs well in <strong>real world</strong> If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.</li>
</ol>
<p>在训练集上表现欠佳，需要切换到好的优化算法</p>
<p>在验证集上表现不好，一组正则化按钮</p>
<p>在测试集表现不好，需要更好的验证集</p>
<p>在用户体验不好，需要改变测试集大小或者成本函数</p>
<h3><span id="l3-single-number-evaluation-metric-dan-yi-shu-zi-ping-gu-zhi-biao">L3 Single number evaluation metric(单一数字评估指标)</span><a href="#l3-single-number-evaluation-metric-dan-yi-shu-zi-ping-gu-zhi-biao" class="header-anchor">#</a></h3><h4><span id="classification">classification</span><a href="#classification" class="header-anchor">#</a></h4><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-25_20-42-10.jpg" alt></p>
<h4><span id="precesion-cha-zhun-lu">Precesion （查准率）</span><a href="#precesion-cha-zhun-lu" class="header-anchor">#</a></h4><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-25_20-47-46.jpg" alt></p>
<h4><span id="recall-cha-quan-lu">recall（查全率）</span><a href="#recall-cha-quan-lu" class="header-anchor">#</a></h4><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-25_20-48-11.jpg" alt></p>
<script type="math/tex; mode=display">
F 1=\frac{2}{\frac{1}{P}+\frac{1}{R}}=\frac{2 P R}{P+R}</script><h3><span id="l4-satisficing-and-optimizing-metrics-man-zu-he-you-hua-zhi-biao">L4  Satisficing and optimizing metrics(满足和优化指标)</span><a href="#l4-satisficing-and-optimizing-metrics-man-zu-he-you-hua-zhi-biao" class="header-anchor">#</a></h3><p>如果我们还想要将分类器的运行时间也纳入考虑范围，将其和精确率、召回率组合成一个单值评价指标显然不那么合适。这时，我们可以将某些指标作为<strong>优化指标（Optimizing Matric）</strong>，寻求它们的最优值；而将某些指标作为<strong>满足指标（Satisficing Matric）</strong>，只要在一定阈值以内即可。</p>
<p>在这个例子中，准确率就是一个优化指标，因为我们想要分类器尽可能做到正确分类；而运行时间就是一个满足指标，如果你想要分类器的运行时间不多于某个阈值，那最终选择的分类器就应该是以这个阈值为界里面准确率最高的那个。</p>
<p>如此，accuracy就变成了<strong>optimizing metric</strong>，而running time则是<strong>satisfying metric</strong>，statisfying metric只要达到标准即可，而optimizing metric则追求更好。一般的，选择一项metric作为optimizing metric，其他的则设置为satisfying metric： </p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-09-36.jpg" alt></p>
<h3><span id="l-5-train-dev-test-distributions-xun-lian-kai-fa-ce-shi-ji-hua-fen">L 5: Train/dev/test distributions(训练/开发/测试集划分)</span><a href="#l-5-train-dev-test-distributions-xun-lian-kai-fa-ce-shi-ji-hua-fen" class="header-anchor">#</a></h3><p>开发（<strong>dev</strong>）集也叫做开发集（<strong>development set</strong>），有时称为保留交叉验证集（<strong>hold out cross validation set</strong>）。</p>
<p>如何设置Train/dev/test集，很大程度上影响了机器学习的速度。</p>
<p>Train/dev/test的区别 Workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep innovating to improve dev set performance until, finally, you have one class that you’re happy with that you then evaluate on your test set.</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-09-37.jpg" alt></p>
<p>开发集合和开发集合来自同一分布，如果是不同分布，相当于靶心移动了</p>
<h3><span id="l-6-size-of-dev-and-test-sets-kai-fa-ji-he-ce-shi-ji-de-da-xiao">L 6: Size of dev and test sets(开发集和测试集的大小)</span><a href="#l-6-size-of-dev-and-test-sets-kai-fa-ji-he-ce-shi-ji-de-da-xiao" class="header-anchor">#</a></h3><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-29-51.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-25.jpg" alt></p>
<h3><span id="l7-when-to-change-dev-test-sets-and-metrics-shi-me-shi-hou-gai-gai-bian-kai-fa-ce-shi-ji-he-zhi-biao">L7 : When to change dev/test sets and metrics(什么时候该改变开发/测试集和指标)</span><a href="#l7-when-to-change-dev-test-sets-and-metrics-shi-me-shi-hou-gai-gai-bian-kai-fa-ce-shi-ji-he-zhi-biao" class="header-anchor">#</a></h3><p>如果发现设定目标和实际期望不符，那就调整目标。</p>
<ol>
<li><p>举个例子</p>
<p>A可能把一些色情照片也分类成猫了，因此改变优化指标</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-26.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-27.jpg" alt></p>
</li>
</ol>
<p>我想你处理机器学习问题时，应该把它切分成独立的步骤。一步是弄清楚如何定义一个指标来衡量你想做的事情的表现，然后我们可以分开考虑如何改善系统在这个指标上的表现。你们要把机器学习任务看成两个独立的步骤，用目标这个比喻，第一步就是设定目标。所以要定义你要瞄准的目标，这是完全独立的一步，这是你可以调节的一个旋钮。如何设立目标是一个完全独立的问题，把它看成是一个单独的旋钮，可以调试算法表现的旋钮，如何精确瞄准，如何命中目标，定义指标是第一步。</p>
<p>后第二步要做别的事情，在逼近目标的时候，也许你的学习算法针对某个长这样的成本函数优化，$J=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)$你要最小化训练集上的损失。你可以做的其中一件事是，修改这个，为了引入这些权重，也许最后需要修改这个归一化常数，$J=\frac{1}{\sum w^{(i)}} \sum_{i=1}^{m} w^{(i)} L\left(\hat{y}^{(i)}, y^{(i)}\right)$</p>
<p>再次，如何定义J并不重要，关键在于正交化的思路，把设立目标定为第一步，然后瞄准和射击目标是独立的第二步。换种说法，我鼓励你们将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数J。</p>
<h3><span id="l8-why-human-level-performance-wei-shi-me-shi-ren-de-biao-xian">L8 : Why human-level performance?(为什么是人的表现？)</span><a href="#l8-why-human-level-performance-wei-shi-me-shi-ren-de-biao-xian" class="header-anchor">#</a></h3><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Bayes-Optimal-Error.png" alt></p>
<p>上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为<strong>贝叶斯最优误差（Bayes Optimal Error）</strong>。</p>
<p>也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。</p>
<p>当模型的表现超过人类后，这些手段起的作用就微乎其微了。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/e1ef954731399bb4fbf18f2fb99b863a.png" alt></p>
<h3><span id="l9-avoidable-bias-ke-bi-mian-pian-chai">L9 : Avoidable bias(可避免偏差)</span><a href="#l9-avoidable-bias-ke-bi-mian-pian-chai" class="header-anchor">#</a></h3><ol>
<li><p>training error</p>
<p>我们经常使用猫分类器来做例子，比如人类具有近乎完美的准确度，所以人类水平的错误是1%。在这种情况下，如果您的学习算法达到8%的训练错误率和10%的开发错误率，那么你也许想在训练集上得到更好的结果。所以事实上，你的算法在训练集上的表现和人类水平的表现有很大差距的话，说明你的算法对训练集的拟合并不好。所以从减少偏差和方差的工具这个角度看，在这种情况下，我会把重点放在减少偏差上。你需要做的是，比如说训练更大的神经网络，或者跑久一点梯度下降，就试试能不能在训练集上做得更好。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-28.jpg" alt></p>
</li>
</ol>
<ol>
<li><p>dev error</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-29.jpg" alt></p>
</li>
</ol>
<p><strong>贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差</strong></p>
<h3><span id="l-10-understanding-human-level-performance-li-jie-ren-de-biao-xian">L 10: Understanding human-level performance(理解人的表现)</span><a href="#l-10-understanding-human-level-performance-li-jie-ren-de-biao-xian" class="header-anchor">#</a></h3><p>还记得上个视频中，我们用过这个词“人类水平错误率”用来估计贝叶斯误差，那就是理论最低的错误率，任何函数不管是现在还是将来，能够到达的最低值</p>
<h3><span id="l11-surpassing-human-level-performance-chao-guo-ren-de-biao-xian">L11 : Surpassing human- level performance(超过人的表现)</span><a href="#l11-surpassing-human-level-performance-chao-guo-ren-de-biao-xian" class="header-anchor">#</a></h3><p>现在，机器学习有很多问题已经可以大大超越人类水平了。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/de2eb0ddc7918f6e9213871e07b8fa56.png" alt></p>
<h3><span id="l12-improving-your-model-performance-gai-shan-ni-de-mo-xing-de-biao-xian">L12 : Improving your model performance(改善你的模型的表现)</span><a href="#l12-improving-your-model-performance-gai-shan-ni-de-mo-xing-de-biao-xian" class="header-anchor">#</a></h3><p>你们学过正交化，如何设立开发集和测试集，用人类水平错误率来估计贝叶斯错误率以及如何估计可避免偏差和方差。我们现在把它们全部组合起来写成一套指导方针，如何提高学习算法性能的指导方针。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-39.jpg" alt></p>
<p>method</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-49.jpg" alt></p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">这一周的内容主要是改善模型的表现，主要是按照正交化，使得更好的满足 1. 评价指标 2. 数据集的划分 3. 人的表现的重要性 4. 当出现表现不好的时候，如何改善呢，有哪些方法呢？ </font>

<h2><span id="w2-ml-strategy-2">W2 ML Strategy(2)</span><a href="#w2-ml-strategy-2" class="header-anchor">#</a></h2><h3><span id="c-1-carrying-out-error-analysis-jin-xing-wu-chai-fen-xi">C 1: Carrying out error analysis(进行误差分析)</span><a href="#c-1-carrying-out-error-analysis-jin-xing-wu-chai-fen-xi" class="header-anchor">#</a></h3><h4><span id="1-simple-analysis">1. simple analysis</span><a href="#1-simple-analysis" class="header-anchor">#</a></h4><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/e1ef954731399bb4fbf18f2fb99b863.png" alt></p>
<p>通过观察发现算法分类出错的例子，是把狗分成猫，提高准确率的方法就是如何针对狗的图片优化算法。你可以针对狗，收集更多的狗图，或者设计一些只处理狗的算法功能之类的，为了让你的猫分类器在狗图上做的更好，让算法不再将狗分类成猫。现在考虑的是应该不应该这么去做呢？统计一下dev set里面多少是错误标记是狗的个数，分析出可以改善的算法的上限。</p>
<h4><span id="mutiply-analysis">mutiply analysis</span><a href="#mutiply-analysis" class="header-anchor">#</a></h4><p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-51.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-50.jpg" alt></p>
<h3><span id="c2-cleaning-up-incorrectly-labeled-data-qing-chu-biao-zhu-cuo-wu-de-shu-ju">C2 : Cleaning up Incorrectly labeled data(清除标注错误的数据)</span><a href="#c2-cleaning-up-incorrectly-labeled-data-qing-chu-biao-zhu-cuo-wu-de-shu-ju" class="header-anchor">#</a></h3><h4><span id="incorrct-label">incorrct label</span><a href="#incorrct-label" class="header-anchor">#</a></h4><h4><span id="traning-set">traning set</span><a href="#traning-set" class="header-anchor">#</a></h4><p>DL algorithms are quite robust to random errors in the traning set so long as your errors or your labeled example to once those errors are not too far from random .</p>
<h4><span id="distribution">distribution</span><a href="#distribution" class="header-anchor">#</a></h4><p>首先，我鼓励你不管用什么修正手段，都要同时作用到开发集和测试集上，我们之前讨论过为什么，开发和测试集必须来自相同的分布。开发集确定了你的目标，当你击中目标后，你希望算法能够推广到测试集上，这样你的团队能够更高效的在来自同一分布的开发集和测试集上迭代。如果你打算修正开发集上的部分数据，那么最好也对测试集做同样的修正以确保它们继续来自相同的分布。所以我们雇佣了一个人来仔细检查这些标签，但必须同时检查开发集和测试集。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-52.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-53.jpg" alt></p>
<h4><span id="suggestion">suggestion</span><a href="#suggestion" class="header-anchor">#</a></h4><p>最后我讲几个建议：</p>
<p>首先，深度学习研究人员有时会喜欢这样说：“我只是把数据提供给算法，我训练过了，效果拔群”。这话说出了很多深度学习错误的真相，更多时候，我们把数据喂给算法，然后训练它，并减少人工干预，减少使用人类的见解。但我认为，在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统，尽管深度学习的研究人员不愿意承认这点。</p>
<p>其次，不知道为什么，我看一些工程师和研究人员不愿意亲自去看这些样本，也许做这些事情很无聊，坐下来看100或几百个样本来统计错误数量，但我经常亲自这么做。当我带领一个机器学习团队时，我想知道它所犯的错误，我会亲自去看看这些数据，尝试和一部分错误作斗争。我想就因为花了这几分钟，或者几个小时去亲自统计数据，真的可以帮你找到需要优先处理的任务，我发现花时间亲自检查数据非常值得，所以我强烈建议你们这样做，如果你在搭建你的机器学习系统的话，然后你想确定应该优先尝试哪些想法，或者哪些方向。</p>
<p>这就是错误分析过程，在下一个视频中，我想分享一下错误分析是如何在启动新的机器学习项目中发挥作用的。</p>
<h3><span id="c3-build-your-first-system-quickly-then-iterate-kuai-su-da-jian-ni-de-di-yi-ge-xi-tong-bing-jin-xing-die-dai">C3: Build your first system quickly, then iterate(快速搭建你的第一个系统，并进行迭代)</span><a href="#c3-build-your-first-system-quickly-then-iterate-kuai-su-da-jian-ni-de-di-yi-ge-xi-tong-bing-jin-xing-die-dai" class="header-anchor">#</a></h3><h4><span id="1-iteration">1. iteration</span><a href="#1-iteration" class="header-anchor">#</a></h4><p>I recommend that you first quickly set up a definition and metrics so this is really you know  deciding where to place your target and you get it wrong you can always move it later we just set up a target somewhere and then I recommend you build an inital machine learning system quickly find the traning set train it and see start to see and understand how well your are doing against your Devon chess setting evaluation metric when you build your initial system you then be able to use bias variance analysis we should talk about earlier as well as error analysis whick we talked about just in last several videos to prioritize the next step in particular if error analysis causes you to realize that a lot of the errors are from the spearker being very far from the mirophone which causes special challenges speech recognitin then that would give you a good reason to focus on techniques to address this it called fast used speech recognition which basically means handling when the speaker is very far from microphone along the value of building this inital  system  it can be a quick  and diry implementation you know do not overthink it but all the value of the inital system is having some learning system having some tranin system allows you lok at bias and variance to do error analysis look at some mistakes to figure out all the different directins you could go in.</p>
<p>我鼓励你们搭建快速而粗糙的实现，然后用它做偏差/方差分析，用它做错误分析，然后用分析结果确定下一步优先要做的方向。</p>
<h3><span id="c4-training-and-testing-on-different-distributions-shi-yong-lai-zi-bu-tong-fen-bu-de-shu-ju-jin-xing-xun-lian-he-ce-shi">C4 : Training and testing on different distributions(使用来自不同分布的数据，进行训练和测试)</span><a href="#c4-training-and-testing-on-different-distributions-shi-yong-lai-zi-bu-tong-fen-bu-de-shu-ju-jin-xing-xun-lian-he-ce-shi" class="header-anchor">#</a></h3><p>this is resulted in many teams sometimes taking one of the days you can find and just shoving it into the training set .</p>
<ol>
<li><p>Cat app example </p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/Xnip2018-06-26_08-32-54.jpg" alt></p>
<p>假设你在开发一个手机应用，用户会上传他们用手机拍摄的照片，你想识别用户从应用中上传的图片是不是猫。现在你有两个数据来源，一个是你真正关心的数据分布，来自应用上传的数据，比如右边的应用，这些照片一般更业余，取景不太好，有些甚至很模糊，因为它们都是业余用户拍的。另一个数据来源就是你可以用爬虫程序挖掘网页直接下载，就这个样本而言，可以下载很多取景专业、高分辨率、拍摄专业的猫图片。如果你的应用用户数还不多，也许你只收集到10,000张用户上传的照片，但通过爬虫挖掘网页，你可以下载到海量猫图，也许你从互联网上下载了超过20万张猫图。而你真正关心的算法表现是你的最终系统处理来自应用程序的这个图片分布时效果好不好，因为最后你的用户会上传类似右边这些图片，你的分类器必须在这个任务中表现良好。现在你就陷入困境了，因为你有一个相对小的数据集，只有10,000个样本来自那个分布，而你还有一个大得多的数据集来自另一个分布，图片的外观和你真正想要处理的并不一样。但你又不想直接用这10,000张图片，因为这样你的训练集就太小了，使用这20万张图片似乎有帮助。但是，困境在于，这20万张图片并不完全来自你想要的分布，那么你可以怎么做呢？</p>
<p>我们真正关心的是来自手机手机收集的数据，而不是来自网页。方法一，随机分配训练集、验证集、测试集，这样的后果就是花了大量时间在实际不关心的数据分布去优化。</p>
<p>训练集20万张网络，5000手机，验证集和测试集各2500，这样可以保证验证集和测试集更接近实际应用场景，我们试试搭建一个学习系统，让系统在处理手机上传图片分布时效果良好。缺点在于，当然了，现在你的训练集分布和你的开发集、测试集分布并不一样。但事实证明，这样把数据分成训练、开发和测试集，在长期能给你带来更好的系统性能。我们以后会讨论一些特殊的技巧，可以处理 训练集的分布和开发集和测试集分布不一样的情况。</p>
</li>
</ol>
<h2><span id="c5-bias-and-variance-with-mismatched-data-distributions-shu-ju-fen-bu-bu-pi-pei-shi-pian-chai-yu-fang-chai-de-fen-xi">C5: Bias and Variance with mismatched data distributions（数据分布不匹配时，偏差与方差的分析）</span><a href="#c5-bias-and-variance-with-mismatched-data-distributions-shu-ju-fen-bu-bu-pi-pei-shi-pian-chai-yu-fang-chai-de-fen-xi" class="header-anchor">#</a></h2><p>首先算法只看过训练集数据，没看过开发集数据。第二，开发集数据来自不同的分布。很难确认这增加的9%误差率有多少是因为算法没看到开发集中的数据导致的，这么评估呢？到底哪个影响元素更大，</p>
<p>评估方法，训练集的分布挖出，traning-dev set : Same distributation as traning set ,but not used for training.</p>
<p>现在，我们有了<em>训练集</em>错误率、<em>训练-验证集</em>错误率，以及<em>验证集</em>错误率。其中，<em>训练集</em>错误率和<em>训练-验证集</em>错误率的差值反映了方差；而<em>训练-验证集</em>错误率和<em>验证集</em>错误率的差值反映了样本分布不一致的问题，从而说明<strong>模型擅长处理的数据和我们关心的数据来自不同的分布</strong>，我们称之为<strong>数据不匹配（Data Mismatch）</strong>问题。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Analysis-With-Data-Mismatch.png" alt></p>
<h3><span id="c6-addressing-data-mismatch-chu-li-shu-ju-bu-pi-pei-wen-ti">C6: Addressing data mismatch（处理数据不匹配问题）</span><a href="#c6-addressing-data-mismatch-chu-li-shu-ju-bu-pi-pei-wen-ti" class="header-anchor">#</a></h3><p>I<img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-55.jpg" alt></p>
<p>Data: Artifical data synthesis</p>
<p>所以，总而言之，如果你认为存在数据不匹配问题，我建议你做错误分析，或者看看训练集，或者看看开发集，试图找出，试图了解这两个数据分布到底有什么不同，然后看看是否有办法收集更多看起来像开发集的数据作训练。</p>
<h3><span id="c7-transfer-learning-qian-yi-xue-xi">C7: Transfer learning（迁移学习）</span><a href="#c7-transfer-learning-qian-yi-xue-xi" class="header-anchor">#</a></h3><p><strong>迁移学习（Tranfer Learning）</strong>是通过将已训练好的神经网络模型的一部分网络结构应用到另一模型，将一个神经网络从某个任务中学到的知识和经验运用到另一个任务中，以显著提高学习任务的性能。</p>
<p>例如，我们将为猫识别器构建的神经网络迁移应用到放射科诊断中。因为猫识别器的神经网络已经学习到了有关图像的结构和性质等方面的知识，所以只要先删除神经网络中原有的输出层，加入新的输出层并随机初始化权重系数（$W[L]$、$b[L]$），随后用新的训练集进行训练，就完成了以上的迁移学习。</p>
<p>如果新的数据集很小，可能只需要重新训练输出层前的最后一层的权重，即$W[L]$$、b[L]$，并保持其他参数不变；而如果有足够多的数据，可以只保留网络结构，重新训练神经网络中所有层的系数。这时初始权重由之前的模型训练得到，这个过程称为<strong>预训练（Pre-Training）</strong>，之后的权重更新过程称为<strong>微调（Fine-Tuning）</strong>。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-56.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-57.jpg" alt></p>
<p>在下述场合进行迁移学习是有意义的：</p>
<p>两个任务有同样的输入（比如都是图像或者都是音频）；<br>拥有更多数据的任务迁移到数据较少的任务；<br>某一任务的低层次特征（底层神经网络的某些功能）对另一个任务的学习有帮助。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-58.jpg" alt></p>
<h3><span id="c8-multi-task-learning-duo-ren-wu-xue-xi">C8; Multi-task learning （多任务学习）</span><a href="#c8-multi-task-learning-duo-ren-wu-xue-xi" class="header-anchor">#</a></h3><p>For example, autonomous driving example,check cars,stop signs,trfffic lights ,输出也是一个向量，</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-59.jpg" alt></p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-60.jpg" alt></p>
<h3><span id="c9-what-is-end-to-end-deep-learning-shi-me-shi-duan-dao-duan-de-shen-du-xue-xi">C9 :  What is end-to-end deep learning?(什么是端到端的深度学习)</span><a href="#c9-what-is-end-to-end-deep-learning-shi-me-shi-duan-dao-duan-de-shen-du-xue-xi" class="header-anchor">#</a></h3><p>在传统的机器学习分块模型中，每一个模块处理一种输入，然后其输出作为下一个模块的输入，构成一条流水线。而<strong>端到端深度学习（End-to-end Deep Learning）</strong>只用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。</p>
<p><img src="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\End-to-end-Deep-Learning.png" alt></p>
<h3><span id="you-dian-yu-que-dian">优点与缺点</span><a href="#you-dian-yu-que-dian" class="header-anchor">#</a></h3><p>应用端到端学习的优点：</p>
<ul>
<li>只要有足够多的数据，剩下的全部交给一个足够大的神经网络。比起传统的机器学习分块模型，可能更能捕获数据中的任何统计信息，而不需要用人类固有的认知（或者说，成见）来进行分析；</li>
<li>所需手工设计的组件更少，简化设计工作流程；</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要大量的数据；</li>
<li>排除了可能有用的人工设计组件；</li>
</ul>
<p>根据以上分析，决定一个问题是否应用端到端学习的<strong>关键点</strong>是：是否有足够的数据，支持能够直接学习从 x 映射到 y 并且足够复杂的函数？</p>
<h3><span id="whether-to-use-end-to-end-learning-shi-fou-yao-shi-yong-duan-dao-duan-de-shen-du-xue-xi">Whether to use end-to-end learning?(是否要使用端到端的深度学习?)</span><a href="#whether-to-use-end-to-end-learning-shi-fou-yao-shi-yong-duan-dao-duan-de-shen-du-xue-xi" class="header-anchor">#</a></h3><p>Pros:</p>
<p>​    let the data speak : x-&gt;y</p>
<p>​    less hand-designing of components needed</p>
<p>Cons:</p>
<p>​    May need large amount of data</p>
<p>​    excludes potentially useful hand-designed components</p>
<p>Key question: Do you hava sufficient data to learn a function of the complexity needed to map x to y?</p>
<p>如果你想使用机器学习或者深度学习来学习某些单独的组件，那么当你应用监督学习时，你应该仔细选择要学习的x到y映射类型，这取决于那些任务你可以收集数据。相比之下，谈论纯端到端深度学习方法是很激动人心的，你输入图像，直接得出方向盘转角，但是就目前能收集到的数据而言，还有我们今天能够用神经网络学习的数据类型而言，这实际上不是最有希望的方法，或者说这个方法并不是团队想出的最好用的方法。而我认为这种纯粹的端到端深度学习方法，其实前景不如这样更复杂的多步方法。因为目前能收集到的数据，还有我们现在训练神经网络的能力是有局限的。</p>
<h1><span id="summary"><font color="green">Summary</font></span><a href="#summary" class="header-anchor">#</a></h1><font color="red">学习如何通过一些手段提高模型的表现，首先了解模型的性能的体现，bias、variance、贝叶斯误差。以及如何一步步的改善性能。具体解决了如下问题，1. 数据的划分 2. 人的表现与机器性能的关系、偏差、方差 3. 训练集和验证集的分布问题，当数据样本对于解决问题不足的时候的解决办法，4. 迁移学习 5. 端到端的学习 6. 多任务学习。6. 在性能不好的情况下，可能需要手动的分析误差，对测试集错误样例做统计等等， </font>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>aiai_</title>
    <url>/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/</url>
    <content><![CDATA[<h1><span id="c2w1">C2W1</span><a href="#c2w1" class="header-anchor">#</a></h1><h2><span id="l01-train-dev-test-sets">L01 : Train/Dev/Test Sets</span><a href="#l01-train-dev-test-sets" class="header-anchor">#</a></h2><h3><span id="1-process">1. process</span><a href="#1-process" class="header-anchor">#</a></h3><p>应用型机器学习是一个高度迭代的过程，通常在项目启动时，我们会先有一个初步想法，比如构建一个含有特定层数，隐藏单元数量或数据集个数等等的神经网络，然后编码，并尝试运行这些代码，通过运行和测试得到该神经网络或这些配置信息的运行结果，你可能会根据输出结果重新完善自己的想法，改变策略，或者为了找到更好的神经网络不断迭代更新自己的方案。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_2.png" alt></p>
<h3><span id="2-data-split">2. data split</span><a href="#2-data-split" class="header-anchor">#</a></h3><ul>
<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>
<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>或者验证不同算法的有效性。</li>
<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>
</ul>
<p>假设这是训练数据，我用一个长方形表示，我们通常会将这些数据划分成几部分，一部分作为训练集，一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念，最后一部分则作为测试集。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_3.png" alt></p>
<ol>
<li><p>在机器学习发展的小数据量时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>
<ul>
<li>无验证集的情况：70% / 30%；</li>
<li>有验证集的情况：60% / 20% / 20%；</li>
</ul>
</li>
<li><p>在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>
<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>
<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>
</li>
</ol>
<ul>
<li>100 万数据量：98% / 1% / 1%；</li>
<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>
</ul>
<h3><span id="3-jian-yi">3. 建议</span><a href="#3-jian-yi" class="header-anchor">#</a></h3><p><strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>
<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然，如果你不需要无偏估计，那就再好不过了。</p>
<h2><span id="l02-bias-variance">L02 : Bias/Variance</span><a href="#l02-bias-variance" class="header-anchor">#</a></h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>
<p>泛化误差可分解为偏差、方差与噪声之和：</p>
<ul>
<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>
<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>
<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>
</ul>
<p>high bias ,underfitting</p>
<p>high variance, overfitting</p>
<p>just right</p>
<h3><span id="1-example">1. example</span><a href="#1-example" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_5.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_6.png" alt></p>
<p>Your algorithms ever on the training set and dev set you can try to diganose whether has problems high barriers or high variances or both or neither.</p>
<h2><span id="l03-basic-recipe-for-machine-learning">L03 Basic Recipe for Machine learning</span><a href="#l03-basic-recipe-for-machine-learning" class="header-anchor">#</a></h2><h3><span id="1-method">1. METHOD</span><a href="#1-method" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_8.png" alt></p>
<p>Training a bigger network almost never hurts. And the main cost of training a neural network that’s too big is just computational time, so long as you’re regularizing.</p>
<p>今天我们讲了如何通过组织机器学习来诊断偏差和方差的基本方法，然后选择解决问题的正确操作，希望大家有所了解和认识。我在课上不止一次提到了正则化，它是一种非常实用的减少方差的方法，正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高，我们下节课再细讲，以便大家更好理解如何实现神经网络的正则化。</p>
<p>第一点，高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同</p>
<p>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</p>
<h2><span id="l04">L04</span><a href="#l04" class="header-anchor">#</a></h2><h3><span id="1-over-fitting">1. over fitting</span><a href="#1-over-fitting" class="header-anchor">#</a></h3><h3><span id="regularization">regularization</span><a href="#regularization" class="header-anchor">#</a></h3><p>L2 regularization</p>
<p>L1 regularizaion: w will be sparse  L1 正则化最后得到 w 向量中将存在大量的 0</p>
<p>为什么只正则化参数w？为什么不再加上参数b 呢？你可以这么做，只是我习惯省略不写，因为通常w是一个高维参数矢量，w已经可以表达高偏差问题，可能w包含有很多参数，我们不可能拟合所有参数，而只是b单个数字，所以w几乎涵盖所有参数，而不是，如果加了参数b，其实也没太大影响，因为b只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。</p>
<ol>
<li><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_9.png" alt></li>
</ol>
<p>2.<img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_10.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_11.png" alt>矩阵范数被称作“弗罗贝尼乌斯范数”，用下标标注F</p>
<ol>
<li><p>反向传播时，填上正则化的一项</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_12.png" alt></p>
<p>因此L2正则化也被称为“权重衰减”。</p>
</li>
</ol>
<p>to get more training data</p>
<h2><span id="l05-why-regularization-reduces-overfitting">L05 :Why Regularization Reduces Overfitting</span><a href="#l05-why-regularization-reduces-overfitting" class="header-anchor">#</a></h2><p>我们添加正则项，它可以避免数据权值矩阵过大，这就是弗罗贝尼乌斯范数，为什么压缩范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单.Regularization其实是让函数变得<strong>简化</strong>。</p>
<p>直观上理解就是如果正则化设置得足够大，权重矩阵被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。</p>
<p>总结一下，如果正则化参数变得很大，w参数很小，z也会相对变小，此时忽略的b影响，z会相对变小，实际上，z的取值范围很小，这个激活函数tanh，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p>
<p><strong>L2 regularization的不足</strong>：要通过不断的选用不同的λ进行测试，计算量加大了。</p>
<h2><span id="l06-dropout-regularization">L06 : Dropout Regularization</span><a href="#l06-dropout-regularization" class="header-anchor">#</a></h2><h3><span id="1-gong-zuo-yuan-li">1. 工作原理</span><a href="#1-gong-zuo-yuan-li" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_15.png" alt></p>
<p>如果上面这幅图存在over fitting。复制这个神经网络，dropout会遍历网络的每一层。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_13.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_14.png" alt></p>
<p>我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p>
<h3><span id="2-inverted-dropout-fan-xiang-sui-ji-shi-huo">2. <strong>inverted dropout</strong>（反向随机失活）</span><a href="#2-inverted-dropout-fan-xiang-sui-ji-shi-huo" class="header-anchor">#</a></h3><p>对第L</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>    <span class="comment"># 设置神经元保留概率</span></span><br><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>], al.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">al = np.multiply(al, dl)</span><br><span class="line">al /= keep_prob</span><br></pre></td></tr></tbody></table></figure>
<p>最后一步<code>al /= keep_prob</code>是因为 a[l]a[l]中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z[l+1]=W[l+1]a[l]+b[l+1]$的期望值，因此除以一个<code>keep_prob</code>。举例解释我们假设第三隐藏层上有50个单元或50个神经元，在一维上是50，我们通过因子分解将它拆分成维的，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%=10）个，现在我们看下$z^{[4]}$，，我们的预期是$z^{[4]}=w^{[4]}a^{[3]}$，$a^{[3]}$减少20%，也就是说中有$a^{[3]}$20%的元素被归零，为了不影响的$a^{[4]}$期望值，我们需要用$w^{[4]}a^{[3]}/keep_prob$，它将会修正或弥补我们所需的那20%，$a^{[3]}$的期望值不会变，划线部分就是所谓的<strong>dropout</strong>方法。</p>
<h2><span id="l07-understanding-dropout">L07 : Understanding Dropout</span><a href="#l07-understanding-dropout" class="header-anchor">#</a></h2><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，<strong>dropout</strong>将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施<strong>dropout</strong>的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_16.png" alt></p>
<p>计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以<strong>dropout</strong>在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但要牢记一点，<strong>dropout</strong>是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用<strong>dropout</strong>的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于<strong>dropout</strong>函数的原因。直观上我认为不能概括其它学科。<strong>dropout</strong>将产生收缩权重的平方范数的效果。当然，不同的层，值可以设置成不同，如果你觉得某一层容易过拟合，把值设置小一点。</p>
<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w,b)$函数单调递减，再打开 dropout。</p>
<h2><span id="l08-other-regularization-methods">L08 :  Other Regularization Methods</span><a href="#l08-other-regularization-methods" class="header-anchor">#</a></h2><ul>
<li><p>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_17.png" alt></p>
</li>
<li><p>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</p>
</li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_18.png" alt></p>
<p>但对我来说<strong>early stopping</strong>的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数，因为现在你不再尝试降低代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p>
<p><strong>Early stopping</strong>的优点是，只运行一次梯度下降，你可以找出的w较小值，中间值和较大值，而无需尝试正则化超级参数的很多值。</p>
<h2><span id="l09-normalizing-inputs">L09 ： Normalizing inputs</span><a href="#l09-normalizing-inputs" class="header-anchor">#</a></h2><ol>
<li><p>零均值</p>
<p>$u=\frac{1}{m}\sum x^{(i)}$,$x-u$</p>
</li>
<li><p>归一化方差；</p>
<p>$\delta^2=\frac{1}{m}(x^{(i)})^2$,每个特征的方差，每个特征数据除以它，就归一化方差了</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_19.png" alt></p>
<h3><span id="why">why</span><a href="#why" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_20.png" alt></p>
<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>
<h2><span id="l10-vanishing-exploding-gradients">L10 : Vanishing /Exploding Gradients</span><a href="#l10-vanishing-exploding-gradients" class="header-anchor">#</a></h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p>
<p>在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与相关的指数级数增长或下降，它也适用于与层数相关的导数或梯度函数，也是呈指数级增长或呈指数递减。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_21.png" alt></p>
<p>假定 g(z)=z,b[l]=0g(z)=z,b[l]=0，对于目标输出有：</p>
<p>$y^=W[L]W[L−1]…W[2]W[1]X$</p>
<ul>
<li>对于$ W[l]$的值大于 1 的情况，激活函数的值将以指数级递增；</li>
<li>对于 $W[l]$的值小于 1 的情况，激活函数的值将以指数级递减。</li>
</ul>
<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>
<h2><span id="l11-weight-initialization-in-a-deep-network">L11 : Weight initialization in a deep network</span><a href="#l11-weight-initialization-in-a-deep-network" class="header-anchor">#</a></h2><p>为了预防值z过大或过小，你可以看到n越大，你希望w越小，因为z是wx+b的和,最合理的方法$w_i=1/n$</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_22.png" alt></p>
<p>因此，实际上，你要做的就是设置某层权重矩阵</p>
<p>$w^{[l]}=n p . random. randn (shape) * np.sqrt \left(\frac{1}{n^{[l-1]}}\right)$</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_23.png" alt></p>
<p>当多个节点时，也一样的看，使得这个节点$z^{<a href="i">L</a>}$不要太大，单独看每个节点既可以</p>
<p>relu : var(w(i)) = 2/n or $\frac{2}{n^{[l-1]}*n^{[l]}}$</p>
<p>tanh: var(w(i)) = 1/n</p>
<p>通过设置初始化化权重矩阵，使得不会增长太快或者太慢</p>
<h2><span id="l12-numerical-approximations-of-gradients">L12 ： Numerical Approximations of Gradients</span><a href="#l12-numerical-approximations-of-gradients" class="header-anchor">#</a></h2><p>单边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$</p>
<p>误差$O(\varepsilon)$</p>
<p>双边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0}=\frac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2 \varepsilon}$</p>
<p>$O\left(\varepsilon^{2}\right)$</p>
<h2><span id="l-13-gradient-checking">L 13 Gradient Checking</span><a href="#l-13-gradient-checking" class="header-anchor">#</a></h2><p>梯度检验帮我们节省了很多时间，也多次帮我发现<strong>backprop</strong>实施过程中的bug，接下来，我们看看如何利用它来调试或检验<strong>backprop</strong>的实施是否正确。</p>
<p>首先要做的就是，把所有参数转换成一个巨大的向量数据，你要做的就是把矩阵w转换成一个向量，把所有矩阵w转换成向量之后，做连接运算，得到一个巨型向量$\theta$，该向量表示为参数$\theta$，代价函数J是所有W和b的函数，现在你得到了一个的代价函数（即）。接着，你得到与和顺序相同的数据，你同样可以把$dW^{[l]}$,和$db^{[l]}$ 转换成一个新的向量，用它们来初始化大向量$d\theta$，它与$\theta$具有相同维度。</p>
<p>梯度的逼近值</p>
<script type="math/tex; mode=display">
d \theta_{\text { approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon}</script><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_24.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_25.png" alt></p>
<p>现在你已经了解了梯度检验的工作原理，它帮助我在神经网络实施中发现了很多<strong>bug</strong>，希望它对你也有所帮助。</p>
<h1><span id="l-14-gradient-checking-implementation-notes">L 14 : Gradient Checking Implementation notes</span><a href="#l-14-gradient-checking-implementation-notes" class="header-anchor">#</a></h1><ol>
<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；太慢了</li>
<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>
<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>
<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week1_25.png" alt></p>
<h2><span id="summary">Summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">回顾这一周，我们讲了如何配置训练集，验证集和测试集，如何分析偏差和方差，如何处理高偏差或高方差以及高偏差和高方差并存的问题，如何在神经网络中应用不同形式的正则化，如正则化和**dropout**，还有加快神经网络训练速度的技巧，最后是梯度检验。</font>



<h1><span id="c2w2-optimization-algorithm">C2W2 :Optimization Algorithm</span><a href="#c2w2-optimization-algorithm" class="header-anchor">#</a></h1><h2><span id="l-01-mini-batch-gradient-descent">L 01 : Mini Batch Gradient Descent</span><a href="#l-01-mini-batch-gradient-descent" class="header-anchor">#</a></h2><ol>
<li><p>Vectorization</p>
</li>
<li><p>Mini batch</p>
<p>not entire training set </p>
<p>bady training set i，$x^{\{i\}}$</p>
<p>mini batch training set</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_1.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_2.png" alt></p>
</li>
</ol>
<p>mini batch gradient descent</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_3.png" alt></p>
<h2><span id="l-02-understanding-mini-batch-gradient-decent">L 02 : Understanding Mini-Batch Gradient Decent</span><a href="#l-02-understanding-mini-batch-gradient-decent" class="header-anchor">#</a></h2><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_4.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_6.png" alt></p>
<p>左图，随着iterations increased, it should decrease .if it ever goes up on iteration,something is wrong.</p>
<p>右图 : it’s as if on every iteration you’re training on a different training set or really training on a different mini batch. It should trend downwards, but it’s also going to be a little bit noisier.So if you plot J{t}, as you’re training mini batch in descent it may be over multiple epochs,you might expect to see a curve like this.</p>
<h3><span id="choosing-your-mini-batch-size">Choosing your mini-batch size</span><a href="#choosing-your-mini-batch-size" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_5.png" alt></p>
<h3><span id="1-you-que-dian">1. 优缺点</span><a href="#1-you-que-dian" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_7.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_8.png" alt></p>
<p>通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的<strong>mini-batch</strong>尺寸，实际上学习率达到最快。你会发现两个好处，一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果<strong>mini-batch</strong>大小为1000个样本，你就可以对1000个样本向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每次训练集允许我们采取5000个梯度下降步骤，所以实际上一些位于中间的<strong>mini-batch</strong>大小效果最好。</p>
<p>使用<strong>batch</strong>梯度下降法时，每次迭代你都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许你的学习率太大。</p>
<p>在随机梯度下降法中，从某一点开始，我们重新选取一个起始点，每次迭代，你只对一个样本进行梯度下降，大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动，但它并不会在达到最小值并停留在此。</p>
<p>用<strong>mini-batch</strong>梯度下降法，我们从这里开始，一次迭代这样做，两次，三次，四次，它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率，我们在下个视频会讲到学习率衰减，也就是如何减小学习率。</p>
<p>batch : too long,too time</p>
<p>随机： lose speeding ,噪声大</p>
<p>mini-batch和stochastic都存在噪声问题，且在local optima附近会徘徊。但设置合适大小的mini-batch size，噪声和徘徊问题可接受的范围内。</p>
<p>size=1,又叫随机梯度下降法 stochastic gradient descent </p>
<h3><span id="how">how</span><a href="#how" class="header-anchor">#</a></h3><p>如何选择mini-batch size（这是一个hyperparameter）：</p>
<ul>
<li>小数据量，比如总的样本只有几千个，完全可以直接用batch gradient descent</li>
<li>大数量，mini-batch size倾向于选择2^n个，比如64, 128, 256等</li>
<li><p>mini-batch 与CPU/GPU memory的内存容量。</p>
<p>In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. 按照上面的方法</p>
</li>
</ul>
<h2><span id="l-03-exponentially-weighted-averages">L 03: Exponentially Weighted Averages</span><a href="#l-03-exponentially-weighted-averages" class="header-anchor">#</a></h2><p>In order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics.</p>
<h3><span id="1-zhi-shu-jia-quan-ping-jun-shu-exponentially-weighted-averages">1. 指数加权平均数（Exponentially weighted averages）</span><a href="#1-zhi-shu-jia-quan-ping-jun-shu-exponentially-weighted-averages" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_9.png" alt></p>
<p>$\theta _i$表示每一日的温度值，蓝色的点，$v_t$表示加权平均后的,红色</p>
<p>权平均方法是：每天的温度值加权值$vt$设置为前一天的温度加权值$vt−1$和当天的温度实际值$θt$做加权平均：</p>
<script type="math/tex; mode=display">
v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}</script><p>由于以后我们要考虑的原因，在计算时可视$v_T$大概是$\frac{1}{(1-\beta)}$的每日温度的加权平均，</p>
<p>如果是$\beta$=0.9，这是十天的平均值，红色</p>
<p>如果$\beta$=0.98,是50天的结果，绿色</p>
<p>如果$beta$=0.5,是2day的结果，黄色</p>
<p>由于仅平均了两天的温度，平均的数据太少，所以得到的曲线有更多的噪声，有可能出现异常值，但是这个曲线能够更快适应温度变化。</p>
<p>当 $\beta$较大时，指数加权平均值适应地更缓慢一些。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_10.png" alt></p>
<p>$</p>
<h2><span id="l-04-understanding-exponentially-weighted-averages">L 04 : Understanding Exponentially Weighted Averages</span><a href="#l-04-understanding-exponentially-weighted-averages" class="header-anchor">#</a></h2><p><strong>假如β=0.9，每个v的计算如下：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} v_{100} &=0.9 v_{99}+0.1 \theta_{100} \\ v_{99} &=0.9 v_{98}+0.1 \theta_{99} \\ v_{98} &=0.9 v_{97}+0.1 \theta_{98} \end{aligned}</script><p>递推可得：</p>
<script type="math/tex; mode=display">
v_{100}=0.1 \theta_{100}+0.1 * 0.9 \theta_{99}+0.1 *(0.9)^{2} \theta_{98}+\ldots</script><p>指数的衰减规律</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_11.png" alt></p>
<p>一般的</p>
<script type="math/tex; mode=display">
v_{t}=\sum_{i=1}^{t}(1-\beta) \beta^{t-i} \theta_{i}</script><p>无穷级数求和：</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{n}(1-\beta) \beta^{t}=1</script><p>因此可以近似的认为所有项的系数之和正好为100%。</p>
<p>即，$vt$是对t日之前<strong>所有的实际温度的加权平均</strong>,权重是指数递减的。</p>
<p>十天后，曲线高度下降到了1/3,赋予权重$\beta^{t-i}$</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_12.png" alt></p>
<script type="math/tex; mode=display">
0.9^{10}~=0.35~=1/e</script><p>一般认为，$v_t$近似前$\frac{1}{1-\beta}$的加权平均值</p>
<h2><span id="l05-bias-correction-in-exponentially-weighted-averages">L05 : Bias correction in exponentially weighted averages</span><a href="#l05-bias-correction-in-exponentially-weighted-averages" class="header-anchor">#</a></h2><p>指数加权平均的偏差修正</p>
<p>由于计算$v1$的时候，并没有历史值做加权，这个时候令其前一个加权值$v0=0$，则会导致$v_1$远小于$\theta_1$,依次类推，<strong>在靠近前面的值会出现显著的小于实际值的情况</strong></p>
<p>因此做一个修正</p>
<script type="math/tex; mode=display">
v_{t}=\frac{\beta v_{t-1}+(1-\beta) \theta_{t}}{1-\beta^{t}}</script><p>你会发现随着$\beta^t$增加，接近于0，所以当t很大的时候，偏差修正几乎没有作用，因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_13.png" alt></p>
<p>因为在Machine Learning中看重的是很多次迭代后的结果，初期的偏差影响并不大。</p>
<h2><span id="l-06-gradient-descent-with-momentum">L 06 : Gradient Descent With Momentum</span><a href="#l-06-gradient-descent-with-momentum" class="header-anchor">#</a></h2><p>动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_14.png" alt></p>
<p>当慢慢下降到最小值，上下波动的梯度下降法的速度减缓，无法使用更大的学习率，</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_15.png" alt></p>
<p>在纵轴上，希望学校慢一点，不需要摆动，横着上，加快学校，基于此就有了Gradient descent with momentum。</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W} :=\beta v_{d W}+(1-\beta) d W} \\ {v_{d b} :=\beta v_{d b}+(1-\beta) d b} \\ {w=w-\alpha v_{d W}} \\ {b=b-\alpha v_{d b}}\end{array}</script><p>这样，可以让gradient更平滑</p>
<ul>
<li>对于上图垂直方向，原来是会上下震荡，但引入了exponentially weighted average，相当于对前面的震荡进行了平均，<strong>结果就是上下震荡互相抵消了</strong>。而水平方向都是向右没有震荡，因此平均后还是向右。最终导致呈现上图红色的下降路线。</li>
<li><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_18.png" alt></li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_16.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_17.png" alt></p>
<h2><span id="l-07-rmsprop">L 07 : RMSprop</span><a href="#l-07-rmsprop" class="header-anchor">#</a></h2><p>RMSprop (Root Mean Square Propagation，均方根传递)，<strong>与momentum一样，也是降低梯度的抖动</strong>。<strong>而是平抑不同大小梯度的更新速率。实际上 作用在α上的</strong></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_19.png" alt></p>
<p>回忆一下我们之前的例子，如果你执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度摆动，为了分析这个例子，假设b纵轴代表参数，横轴代表参数W，可能有w1，或者w2其它重要的参数，为了便于理解，被称为b和w。</p>
<p>我们希望学习速度快，而在垂直方向，也就是例子中的方向，我们希望减缓纵轴上的摆动，所以有了$S_{d W} $和$ S_{d b}$，我们希望$S_{d W} $会相对较小，所以我们要除以一个较小的数，而希望$ S_{d b}$又较大，所以这里我们要除以较大的数字，这样就可以减缓纵轴上的变化。</p>
<p>这些微分，垂直方向的要比水平方向的大得多，所以斜率在方向特别大，所以这些微分中，db较大，dw较小，因为函数的倾斜程度，在纵轴上，也就是b方向上要大于在横轴上，也就是方向上W。db的平方较大，所以$Sdb$也会较大，而相比之下，dw会小一些，亦或dw平方会小一些，因此$Sdw$会小一些，结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p>
<p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率，然后加快学习，而无须在纵轴上垂直方向偏离。</p>
<p>实际中dw是一个高维度的参数向量，db也是一个高维度参数向量，但是你的直觉是，在你要消除摆动的维度中，最终你要计算一个更大的和值，这个平方和微分的加权平均值，所以你最后去掉了那些有摆动的方向。所以这就是<strong>RMSprop</strong>，全称是均方根，因为你将微分进行平方，然后最后使用平方根。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_20.png" alt></p>
<p>解释平方：</p>
<p>垂直方向，比较陡，梯度比较大，但我们又希望它下降的慢。因此对梯度除以一个较大的值，所以用梯度的平方的平均来表示。让不同的参数拥有不同的learning rate。</p>
<p><strong>从某种角度看，RMSprop会根据当前的梯度自动调整参数的learning rate，梯度大降低learning rate，梯度小的时候提高learning rate，从而一方面避免了震荡，另一方面避免在平坦的地方徘徊太久。</strong></p>
<p>为了避免出现分母为0</p>
<script type="math/tex; mode=display">
\begin{array}{c}{s_{d w}=\beta s_{d w}+(1-\beta)(d w)^{2}} \\ {s_{d b}=\beta s_{d b}+(1-\beta)(d b)^{2}} \\ {w :=w-\alpha \frac{d w}{\sqrt{s_{d w}+\varepsilon}}} \\ {b :=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}}\end{array}</script><p>$\varepsilon$取$10^{-8}$不错的选择.</p>
<p>补充：</p>
<p>RMSProp算法对梯度计算了<strong>微分平方加权平均数</strong>。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快</p>
<h2><span id="l-08-adam-optimization-algorithm">L 08 Adam optimization algorithm</span><a href="#l-08-adam-optimization-algorithm" class="header-anchor">#</a></h2><p>Adam（Adaptive Moment Estimation，自适应矩估计）就是momentum和RMSprop的结合。momentum负责平滑梯度，而RMSprop负责调解learning rate。</p>
<h3><span id="1-adam">1. Adam</span><a href="#1-adam" class="header-anchor">#</a></h3><p>a. 引入的变量有：</p>
<ul>
<li>$v$ : 计算同momentum算法，将梯度进行指数加权平均</li>
<li>$s$: 计算同RMSprop，将梯度的平方进行指数加权平均</li>
<li>$β1$ : 计算vv的加权参数</li>
<li>$β2$ : 计算ss的加权参数</li>
</ul>
<p>b. 在迭代前，初始化参数v和s</p>
<script type="math/tex; mode=display">
v_{d W}=0, s_{d W}=0, v_{d b}=0, s_{d b}=0</script><p>c. 对第t次梯度下降的迭代 a. 首先计算dw和db的v和s</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W}=\beta_{1} v_{d W}+\left(1-\beta_{1}\right) d W} \\ {s_{d W}=\beta_{2} s_{d W}+\left(1-\beta_{2}\right)(d W)^{2}} \\ {v_{d b}=\beta_{1} v_{d b}+\left(1-\beta_{1}\right) d b} \\ {s_{d b}=\beta_{2} s_{d b}+\left(1-\beta_{2}\right)(d b)^{2}}\end{array}</script><p>d. 修正</p>
<script type="math/tex; mode=display">
v_{d W}^{\text {corrected}}=\frac{v_{d W}}{1-\left(\beta_{1}\right)^{t}}\\
\begin{aligned} s_{d W}^{\text {corrected}} &=\frac{s_{d W}}{1-\left(\beta_{2}\right)^{t}} \\ v_{d b}^{\text {corrected}} &=\frac{v_{d b}}{1-\left(\beta_{1}\right)^{t}} \\ s_{d b}^{\text {corrected}} &=\frac{s_{d b}}{1-\left(\beta_{2}\right)^{t}} \end{aligned}</script><p>e. 最后更新参数W和b</p>
<script type="math/tex; mode=display">
W=W-\alpha \frac{v_{d W}^{\text {corrected}}}{\sqrt{s_{d W}^{\text { corrected }}+\varepsilon}}\\
b=b-\alpha \frac{v_{d b}^{\text {corrected}}}{\sqrt{s_{d b}^{\text { corrected }}+\varepsilon}}</script><p>超参的选择：</p>
<ul>
<li>α：需要调优</li>
<li>β1: 通常选择为0.9</li>
<li>β2: 通常选择为0.999</li>
<li>ε: 一般不需要调优，选择一个小数，比如10−8</li>
</ul>
<p>你可以尝试一系列值α，然后看哪个有效</p>
<h2><span id="l09-learning-rate-decay">L09 : Learning Rate Decay</span><a href="#l09-learning-rate-decay" class="header-anchor">#</a></h2><ol>
<li><p>why</p>
<p>为什么要做learning rate decay？ 较大的learning rate虽然在算法开始阶段会加快收敛速度，但在收敛接近到优化点的时候，算法会在优化点附近震荡，如下图：</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_22.png" alt></p>
<p>2.如何做learning rate decay？ <strong>思路很简单，就是引入一个函数，让α随着迭代（比如min-batch的epoch）递减</strong>。为此可以采用的decay函数有：</p>
<p>倒数：</p>
<script type="math/tex; mode=display">
\alpha :=\frac{1}{1+\text { decay rate * epoch num}} \alpha</script><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_23.png" alt></p>
<h2><span id="l-10-the-problem-of-local-optima">L 10: The Problem of local Optima</span><a href="#l-10-the-problem-of-local-optima" class="header-anchor">#</a></h2><p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>但是一个具有高维度空间的函数，如果梯度为0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在2万维空间中，那么想要得到局部最优，所有的2万个方向都需要是这样，但发生的机率也许很小，也许是$2^{-20000}$，因此更有可能遇到有些方向的曲线会这样向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，你更可能碰到鞍点。所有，担心收敛到local optima，真是人们想多了，实际上并没有想象的那么多local optima。在高维空间，几乎不太可能被困在一个local optima，这是一个好消息。</p>
<p>因此，在高维空间遇到的问题是高原问题（Problem of plateaus）</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_27.png" alt></p>
<p>Adam算法可以加速学习</p>
<h1><span id="w3-hyperparameter-tuning">W3 Hyperparameter tuning</span><a href="#w3-hyperparameter-tuning" class="header-anchor">#</a></h1><h2><span id="l01-tuning-process">L01 Tuning process</span><a href="#l01-tuning-process" class="header-anchor">#</a></h2><ol>
<li>到目前为止，我们接触到的hyperparameter有：</li>
<li>learning rate: αα</li>
<li>momentum 参数: ββ</li>
<li>Adam参数: β1β1和 β2β2以及εε</li>
<li>神经网络层数: L</li>
<li>神经网络隐藏层neuron数：n[l]n[l]</li>
<li>learning rate decay参数</li>
<li>min-batch size</li>
<li>这些hyperparameter重要性排序：</li>
<li>最重要的： learning rate: αα</li>
<li>比较重要的： momentum 参数: ββ 神经网络层数: L 神经网络隐藏层neuron数：n[l]n[l]</li>
<li>次重要的： 神经网络隐藏层neuron数 learning rate decay参数</li>
<li>基本不需调整的 β1β1和 β2β2以及ε</li>
</ol>
<h4><span id="1-try-random-values-don-t-use-a-grid">1. Try random values : Don’t use a grid</span><a href="#1-try-random-values-don-t-use-a-grid" class="header-anchor">#</a></h4><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_28.png" alt></p>
<p>why:</p>
<p>举个例子，假设超参数1是（学习速率），取一个极端的例子，假设超参数2是<strong>Adam</strong>算法中，分母中的$\varepsilon$。在这种情况下，a的取值很重要，而$\varepsilon$取值则无关紧要。如果你在网格中取点，接着，你试验了a的5个取值，那你会发现，无论$\varepsilon$取何值，结果基本上都是一样的。所以，你知道共有25种模型，但进行试验的值只有5个，我认为这是很重要的。</p>
<p>对比而言，如果你随机取值，你会试验25个独立的a,$\varepsilon$，似乎你更有可能发现效果做好的那个。</p>
<h3><span id="2-you-cu-cao-dao-jing-xi-de-ce-lue">2. 由粗糙到精细的策略</span><a href="#2-you-cu-cao-dao-jing-xi-de-ce-lue" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_29.png" alt></p>
<h2><span id="l-02-using-an-appropriate-scale-to-pick-hyperparameters">L 02: Using an Appropriate Scale to pick hyperparameters</span><a href="#l-02-using-an-appropriate-scale-to-pick-hyperparameters" class="header-anchor">#</a></h2><p>$a$取值0.0001,1,如果你画一条从0.0001到1的数轴，沿其随机均匀取值，那90%的数值将会落在0.1到1之间，结果就是，在0.1到1之间，应用了90%的资源，而在0.0001到0.1之间，只有10%的搜索资源，这看上去不太对。</p>
<p>同时在范围内搜索，也不是均匀分布（uniformly random）的，通常有这个参数的scale，<strong>比如对数scale</strong>。</p>
<p>反而，用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用，还有在0.001到0.01之间等等。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_32.png" alt></p>
<h2><span id="l-03-hyperparameter-tuning-i-practice">L 03 : Hyperparameter tuning i practice</span><a href="#l-03-hyperparameter-tuning-i-practice" class="header-anchor">#</a></h2><ol>
<li>不同的算法和场景，对超参的scale敏感性可能不一样.</li>
<li>根据计算资源和数据量，可以采取两种策略来调参<ul>
<li>Panda（熊猫策略）：对一个模型先后修改参数，查看其表现，最终选择最好的参数。就像熊猫一样，一次只抚养一个后代。</li>
<li>Caviar（鱼子酱策略）：计算资源足够，可以同时运行很多模型实例，采用不同的参数，然后最终选择一个好的。类似鱼类，一次下很多卵，自动竞争成活。 </li>
</ul>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_33.png" alt></p>
<h3><span id="l-04-normalizing-activations-in-a-network">L 04: Normalizing Activations in a Network</span><a href="#l-04-normalizing-activations-in-a-network" class="header-anchor">#</a></h3><h4><span id="1-implementing-batch-normalizing">1. Implementing Batch Normalizing</span><a href="#1-implementing-batch-normalizing" class="header-anchor">#</a></h4><p><strong>Batch</strong>归一化,<strong>Batch</strong>归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易，甚至是深层网络。</p>
<p>可以normalize $a^{[l]},z^{[l]}$,选择$z^{[L]}$</p>
<p><strong>设置 γ 和 β 的原因</strong>是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_34.png" alt></p>
<p>需要注意的是，β和γ不是超参，而是梯度下降需学习的参数。</p>
<h2><span id="l-05-fitting-batch-norm-into-neural-networks">L 05 : Fitting Batch Norm Into Neural Networks</span><a href="#l-05-fitting-batch-norm-into-neural-networks" class="header-anchor">#</a></h2><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_35.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_36.png" alt></p>
<p>注意</p>
<p>先前我说过每层的参数是$w^{[l]}$和$b^{[l]}$，还有$\beta^{[l]}$和$b^{[l]}$，请注意计算的方式如下，$z^{[l]}=w^{[l]} a^{[l-1]}+b^{[l]}$，但<strong>Batch</strong>归一化做的是，它要看这个<strong>mini-batch</strong>，先将$z^{[l]}$归一化，结果为均值0和标准方差，再由$\beta$和b重缩放，但这意味着，无论$b^{[l]}$的值是多少，都是要被减去的，因为在<strong>Batch</strong>归一化的过程中，你要计算的$z^{[l]}$均值，再减去平均值，在此例中的<strong>mini-batch</strong>中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消.</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_37.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_38.png" alt></p>
<p>最后，请记住的维$z^{[l]}$，因为在这个例子中，维数会是$\left(n^{[l]}, 1\right)$，的尺寸为，如果是l层隐藏单元的数量，那$ \beta^{[l]}$和$ \gamma^{[l]}$的维度也是$\left(n^{[l]}, 1\right)$，因为这是你隐藏层的数量，你有隐藏单元，<strong>所以$\gamma^{[l]}$和</strong>$  \beta^{[l]}$用来将每个隐藏层的均值和方差缩放为网络想要的值。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_39.png" alt></p>
<h3><span id="l-06-why-doest-batch-norm-work">L 06 Why Doest Batch Norm Work?</span><a href="#l-06-why-doest-batch-norm-work" class="header-anchor">#</a></h3><ol>
<li>首先，起到了normalization的作用，同对输入数据X的normalization作用类似。</li>
</ol>
<ol>
<li><p>让每一层的学习，<strong>一定程度解耦了前层参数和后层参数，让各层更加独立的学习</strong>。无论前一层如何变，本层输入的数据总是保持稳定的均值和方差。（主要原因）</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_40.png" alt></p>
<p>所以使你数据改变分布的这个想法，有个有点怪的名字“<strong>Covariate shift</strong>”，想法是这样的，如果你已经学习了到 的映射，如果 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由 到 映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p>
<p>关于第二点，如果实际应用样本和训练样本的数据分布不同（例如，橘猫图片和黑猫图片），我们称发生了“<strong>Covariate Shift</strong>”。这种情况下，一般要对模型进行重新训练。Batch Normalization 的作用就是减小 Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。</p>
<p>即使输入的值改变了，由于 Batch Normalization 的作用，使得均值和方差保持不变（由 γ 和 β 决定），限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_41.png" alt></p>
<p>另外，Batch Normalization 也<strong>起到微弱的正则化</strong>（regularization）效果。因为在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的 z~(i)z~(i)也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。</p>
<p>因为 Batch Normalization 只有微弱的正则化效果，因此可以和 dropout 一起使用，以获得更强大的正则化效果。通过应用更大的 mini-batch 大小，可以减少噪声，从而减少这种正则化效果。</p>
<p>最后，不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。正则化只是一种非期望的副作用，Batch Normalization 解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_42.png" alt></p>
</li>
</ol>
<h3><span id="l-07-batch-norm-at-test-time">L 07 : Batch Norm At Test Time</span><a href="#l-07-batch-norm-at-test-time" class="header-anchor">#</a></h3><p>问题：BN算法在训练时是一个批次的数据训练，能算出每一层Z的均值和方差；而在测试时，输入的则是单个数据，<strong>单条数据没法做均值和方差的计算</strong>，怎么在测试期输入均值和方差呢?</p>
<p>实际应用中一般不使用这种方法，而是使用之前学习过的指数加权平均的方法来预测测试过程单个样本的 μ 和 $σ^2$</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_43.png" alt></p>
<p>计算$z_{\text { norm }}^{(\hat{2})}$，用$\mu$ 和$ \sigma^{2}$的指数加权平均，用你手头的最新数值来做调整，然后你可以用左边我们刚算出来的和你在神经网络训练过程中得到的$\beta$和$\sigma$参数来计算你那个测试样本的z值。</p>
<h3><span id="l-08-softmax-regression">L 08 : Softmax Regression</span><a href="#l-08-softmax-regression" class="header-anchor">#</a></h3><h4><span id="1-multi-class-classification">1. [Multi-class classification]</span><a href="#1-multi-class-classification" class="header-anchor">#</a></h4><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_44.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_45.png" alt></p>
<p>最后一层是概率，之和为1，要用到<strong>Softmax</strong>层，<strong>Softmax</strong>激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_46.png" alt></p>
<h4><span id="2-softmax-example">2. Softmax example</span><a href="#2-softmax-example" class="header-anchor">#</a></h4><p>没有隐藏层的softmax,代表一些决策边界</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_47.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_48.png" alt></p>
<h3><span id="l-09-training-softmax-classifier">L 09 Training SoftMax classifier</span><a href="#l-09-training-softmax-classifier" class="header-anchor">#</a></h3><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_49.png" alt></p>
<p><strong>Softmax</strong>这个名称的来源是与所谓<strong>hardmax</strong>对比,<strong>Softmax</strong>回归或<strong>Softmax</strong>激活函数将<strong>logistic</strong>激活函数推广到类，而不仅仅是两类，结果就是如果C=2，那么C=2的<strong>Softmax</strong>实际上变回了<strong>logistic</strong>回归，</p>
<h4><span id="loss-function">Loss Function</span><a href="#loss-function" class="header-anchor">#</a></h4><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_50.png" alt></p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_51.png" alt></p>
<h4><span id="3-gradient-descent-with-softmax">3. Gradient descent with softmax</span><a href="#3-gradient-descent-with-softmax" class="header-anchor">#</a></h4><p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_52.png" alt></p>
<p>最后一层求导，softmax激活函数</p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><h2><span id="l11-tensorflow">L11 TensorFlow</span><a href="#l11-tensorflow" class="header-anchor">#</a></h2><h4><span id="1-ji-ben-liu-cheng">1. 基本流程</span><a href="#1-ji-ben-liu-cheng" class="header-anchor">#</a></h4><p>使用tensorflow，只要告诉tensorflow forward prop，它自己就会做backprop，因此不用自己实现backprop</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_53.png" alt></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#导入TensorFlow</span></span><br><span class="line">​</span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype = tf.float32)</span><br><span class="line"><span class="comment">#接下来，让我们定义参数w，在TensorFlow中，你要用tf.Variable()来定义参数</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后我们定义损失函数：</span></span><br><span class="line">​</span><br><span class="line">cost = tf.add(tf.add(w**<span class="number">2</span>,tf.multiply(- <span class="number">10.</span>,w)),<span class="number">25</span>)</span><br><span class="line"><span class="comment">#然后我们定义损失函数J</span></span><br><span class="line">然后我们再写：</span><br><span class="line">​</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"><span class="comment">#(让我们用0.01的学习率，目标是最小化损失)。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#最后下面的几行是惯用表达式:</span></span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">session = tf.Session()<span class="comment">#这样就开启了一个TensorFlow session。</span></span><br><span class="line">​</span><br><span class="line">session.run(init)<span class="comment">#来初始化全局变量。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后让TensorFlow评估一个变量，我们要用到:</span></span><br><span class="line">​</span><br><span class="line">session.run(w)</span><br><span class="line">​</span><br><span class="line"><span class="comment">#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以session.run(w)评估了w，让我：：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line">​</span><br><span class="line">所以如果我们运行这个，它评估等于<span class="number">0</span>，因为我们什么都还没运行。</span><br><span class="line"></span><br><span class="line"><span class="comment">#现在让我们输入：</span></span><br><span class="line">​</span><br><span class="line">$session.run(train)，它所做的就是运行一步梯度下降法。</span><br><span class="line"><span class="comment">#接下来在运行了一步梯度下降法后，让我们评估一下w的值，再print：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line"><span class="comment">#在一步梯度下降法之后，w现在是0.1。</span></span><br></pre></td></tr></tbody></table></figure>
<h4><span id="2-ru-he-yong-xun-lian-shu-ju">2. 如何用训练数据</span><a href="#2-ru-he-yong-xun-lian-shu-ju" class="header-anchor">#</a></h4><p>placeholder 在实际的训练过程中，要用不同的样本反复放到一个待优化函数中的，这个时候就可以用tensorflow的placeholder实现,在run的时候，对应给出<code>feed_dict</code>，表名占位符x的实际值。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 导入Tensorflow</span></span><br><span class="line"></span><br><span class="line">coefficient = np.array([[<span class="number">2.</span>],[<span class="number">-10.</span>],[<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># 3x1大小的placeholder</span></span><br><span class="line">cost = w**x[<span class="number">0</span>][<span class="number">0</span>] - x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>] <span class="comment"># 要优化的cost function（即forward prop的形式）</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost) </span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line">session.run(train, feed_dict={x:coefficient}) <span class="comment"># x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    session.run(train, feed_dict={x:coefficient}) <span class="comment"># # x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="3-ji-suan-liu">3. 计算流</span><a href="#3-ji-suan-liu" class="header-anchor">#</a></h4><p><strong>TensorFlow</strong>程序的核心是计算损失函数，然后<strong>TensorFlow</strong>自动计算出导数，以及如何最小化损失，因此这个等式或者这行代码所做的就是让<strong>TensorFlow</strong>建立计算图，</p>
<p>with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。建立计算流的过程，前向传播的过程，operation</p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_56.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_54.png" alt></p>
<p><img src="/2019/04/17/ImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning,%20Regularization%20and%20Optimization/L2_week2_55.png" alt></p>
<h1><span id="summary">Summary</span><a href="#summary" class="header-anchor">#</a></h1><font color="read">how to systematically organize the hyper parameter search process and  batch normalization and framework </font>

<p><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2">http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2</a></p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3">http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3</a></p>
<p><a href="http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview">http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview</a></p>
<p><a href="https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14">https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14</a></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>彩铅DailyLifeStyle</title>
    <url>/2019/04/16/%E5%BD%A9%E9%93%85DailyLifeStyle/</url>
    <content><![CDATA[<h1><span id="day-one">Day one</span><a href="#day-one" class="header-anchor">#</a></h1><h2><span id="1-gong-ju-jian-dan-jie-shao">1. 工具简单介绍</span><a href="#1-gong-ju-jian-dan-jie-shao" class="header-anchor">#</a></h2><ol>
<li><p>彩铅</p>
<p>水溶性，油性</p>
</li>
<li><p>彩铅纸</p>
</li>
<li><p>铅笔</p>
<ol>
<li><p>B</p>
<p>2B&lt;4B黑的程度</p>
</li>
<li><p>H</p>
<p>4H&lt;8H软度</p>
</li>
</ol>
</li>
<li><p>橡皮</p>
<ol>
<li>软橡皮</li>
<li>硬橡皮</li>
<li>电动橡皮擦</li>
</ol>
</li>
<li><p>铅笔刀</p>
<ol>
<li>可跳档类型</li>
</ol>
</li>
<li><p>勾线笔</p>
</li>
<li><p>针管笔 （樱花）</p>
</li>
<li><p>笔套</p>
</li>
<li><p>高光笔</p>
<ol>
<li>可以用修正液替换（三棱)</li>
</ol>
</li>
<li><p>纸擦笔</p>
<ol>
<li>玛丽</li>
</ol>
</li>
<li>刷子</li>
<li><p>画板</p>
<ol>
<li>速写板</li>
</ol>
</li>
</ol>
<h2><span id="2-yan-se">2. 颜色</span><a href="#2-yan-se" class="header-anchor">#</a></h2><p>三原色： 红 黄 蓝</p>
<ol>
<li><p>色相</p>
<p>颜色</p>
</li>
<li><p>饱和度</p>
<ol>
<li>鲜艳程度</li>
</ol>
</li>
<li><p>明度</p>
<ol>
<li>明暗程度</li>
</ol>
</li>
<li>邻近色</li>
<li>对比色<ol>
<li>红-绿</li>
<li>蓝-橙</li>
<li>紫-黄</li>
</ol>
</li>
<li>暖色和冷色</li>
</ol>
<h2><span id="3-pai-xian">3. 排线</span><a href="#3-pai-xian" class="header-anchor">#</a></h2><ol>
<li><p>一个方向</p>
<p>往同一个方向排，无连接</p>
</li>
<li><p>来回</p>
<p>相连接，一条线</p>
</li>
<li><p>不同方向排列</p>
</li>
</ol>
<p>注意：力度和间距</p>
<h2><span id="4-ping-tu">4. 平涂</span><a href="#4-ping-tu" class="header-anchor">#</a></h2><p>力度一致</p>
<h2><span id="5-jian-bian">5. 渐变</span><a href="#5-jian-bian" class="header-anchor">#</a></h2><p>力度不一致</p>
]]></content>
      <categories>
        <category>娱乐生活</category>
      </categories>
      <tags>
        <tag>彩铅</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2019/04/13/machine%20learning%20test/</url>
    <content><![CDATA[<h1><span id="logistics-regression">Logistics Regression</span><a href="#logistics-regression" class="header-anchor">#</a></h1><p>   如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p>
<p>​     这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。</p>
<a id="more"></a>
<h2><span id="luo-ji-hui-gui-de-ji-ben-jia-she"><strong>逻辑回归的基本假设</strong></span><a href="#luo-ji-hui-gui-de-ji-ben-jia-she" class="header-anchor">#</a></h2><ul>
<li><ul>
<li><p>任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的</p>
<p>第一个</p>
<p>基本假设是</p>
<p>假设数据服从伯努利分布。</p>
<p>伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是p,抛中为负面的概率是 1-p,在逻辑回归这个模型里面是假设 $h_θ(x)$为样本为正的概率，</p>
<p>$1−h_θ(x)$为样本为负的概率。那么整个模型可以描述为$h_θ(x;θ)=p$</p>
</li>
<li><p>逻辑回归的第二个假设是假设样本为正的概率是 $p=\frac{1}{1+e^{w^Tx}}$</p>
</li>
</ul>
</li>
<li><ul>
<li><p>所以逻辑回归的最终形式 </p>
<p>$h_θ(x;θ)=\frac{1}{1+e^{w^Tx}}$</p>
</li>
</ul>
</li>
</ul>
<h2><span id="luo-ji-hui-gui-de-sun-shi-han-shu"><strong>逻辑回归的损失函数</strong></span><a href="#luo-ji-hui-gui-de-sun-shi-han-shu" class="header-anchor">#</a></h2><ul>
<li><p>逻辑回归的损失函数是它的极大似然函数</p>
<p>$Lθ(x)=\pi_{i=1}^{m}h_θ(xi;θ)^y_i∗(1−h_θ(xi;θ))^{1−y_i}$</p>
</li>
</ul>
<h2><span id="luo-ji-hui-gui-de-qiu-jie-fang-fa"><strong>逻辑回归的求解方法</strong></span><a href="#luo-ji-hui-gui-de-qiu-jie-fang-fa" class="header-anchor">#</a></h2><ul>
<li><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。</p>
<ul>
<li>简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</li>
<li>随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</li>
<li>小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</li>
</ul>
</li>
<li><ul>
<li>其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。<ul>
<li>第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。</li>
<li>第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="luo-ji-hui-gui-de-mu-de"><strong>逻辑回归的目的</strong></span><a href="#luo-ji-hui-gui-de-mu-de" class="header-anchor">#</a></h2><ul>
<li><p>该函数的目的便是将数据二分类，提高准确率。</p>
</li>
<li><p>逻辑回归如何分类</p>
<ul>
<li>逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。</li>
</ul>
</li>
</ul>
<h2><span id="3-dui-luo-ji-hui-gui-de-jin-yi-bu-ti-wen"><strong>3.对逻辑回归的进一步提问</strong></span><a href="#3-dui-luo-ji-hui-gui-de-jin-yi-bu-ti-wen" class="header-anchor">#</a></h2><p>​    逻辑回归虽然从形式上非常的简单，但是其内涵是非常的丰富。有很多问题是可以进行思考的</p>
<ul>
<li><p><strong>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</strong></p>
<ul>
<li><p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新</p>
<p>$w_j=w_j−(y^i−h_w(x^i;w))∗x_j^i\theta$</p>
<p>这个式子的更新速度只和$x_j,y_j</p>
<p>相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</p>
</li>
<li><p>为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>
</li>
</ul>
</li>
<li><p><strong>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</strong></p>
</li>
<li><p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p>
</li>
<li>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。</li>
<li><p>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p>
</li>
<li><p><strong>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</strong></p>
<ul>
<li>去掉高度相关的特征会让模型的可解释性更好</li>
<li>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</li>
</ul>
</li>
</ul>
<h2><span id="4-luo-ji-hui-gui-de-you-que-dian-zong-jie"><strong>4.逻辑回归的优缺点总结</strong></span><a href="#4-luo-ji-hui-gui-de-you-que-dian-zong-jie" class="header-anchor">#</a></h2><p>​    面试的时候，别人也经常会问到，你在使用逻辑回归的时候有哪些感受。觉得它有哪些优缺点。</p>
<p>​     <strong>在这里我们总结了逻辑回归应用到工业界当中一些优点：</strong></p>
<ul>
<li><p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</p>
</li>
<li><p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</p>
</li>
<li><p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</p>
</li>
<li><p>资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。</p>
</li>
<li><p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p>
</li>
</ul>
<p>​      <strong>但是逻辑回归本身也有许多的缺点:</strong></p>
<ul>
<li><p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</p>
</li>
<li><p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</p>
</li>
<li><p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。</p>
</li>
<li><p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p>
</li>
</ul>
<h2><span id="mo-xing-ce-lue-suan-fa">模型、策略、算法</span><a href="#mo-xing-ce-lue-suan-fa" class="header-anchor">#</a></h2><h2><span id="codings">Codings</span><a href="#codings" class="header-anchor">#</a></h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2019/04/13/machine%20learning/</url>
    <content><![CDATA[<p><img src="/2019/04/13/machine%20learning/image-20200624204341481.png" alt="image-20200624204341481"></p>
<p>英伟达:芯片，GPU</p>
<p>开发框架：tensorflow，pytorch caffe</p>
<a id="more"></a>
<h2><span id="jian-du-xue-xi">监督学习</span><a href="#jian-du-xue-xi" class="header-anchor">#</a></h2><p>学习目的是学习一个输入到输出的映射，称为模型。模型的集合就是假设空间。</p>
<p>模型：概率模型；非概率模型；</p>
<p>学习过程：搜索过程</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2019/04/11/tensorflow/</url>
    <content><![CDATA[<h1><span id="official-definition">official definition</span><a href="#official-definition" class="header-anchor">#</a></h1><a id="more"></a>
<h1><span id="what-is-tensorflow">What is tensorflow</span><a href="#what-is-tensorflow" class="header-anchor">#</a></h1><p><em>flow of tensors</em></p>
<p><strong>“TensorFlow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represents mathematical operations, while graph edges represent multi-dimensional data arrays (aka tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.”*</strong></p>
<p><img src="/2019/04/11/tensorflow/MyBlog\hexo\source\_posts\tensorflow\tensors_flowing-3.gif" alt></p>
<p>A major difference between numpy and TensorFlow is that TensorFlow follows a lazy programming paradigm. It first builds a graph of all the operation to be done, and then when a “session” is called, it “runs” the graph. It’s built to be scalable, by changing internal data representation to tensors (aka multi-dimensional arrays). Building a computational graph can be considered as the main ingredient of TensorFlow. </p>
<p>It’s easy to classify TensorFlow as a neural network library, but it’s not just that. Yes, it was designed to be a powerful neural network library. But it has the power to do much more than that. You can build other machine learning algorithms on it such as decision trees or k-Nearest Neighbors. You can literally do everything you normally would do in numpy! It’s aptly called “numpy on steroids”</p>
<p>The advantages of using TensorFlow are:</p>
<ul>
<li><p><strong>It has an intuitive construct</strong>, because as the name suggests it has <em>“flow of tensors”.</em> You can easily visualize each and every part of the graph.</p>
</li>
<li><p><strong>Easily train on cpu/gpu for distributed computing</strong></p>
</li>
<li><p><strong>Platform flexibility</strong>. You can run the models wherever you want, whether it is on mobile, server or PC.</p>
</li>
</ul>
<p>  scikit-learn</p>
  <figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># define hyperparamters of ML algorithm</span></span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line"><span class="comment"># train </span></span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment"># test </span></span><br><span class="line">clf.predict(X_test)</span><br></pre></td></tr></tbody></table></figure>
<p>  The usual workflow of running a program in TensorFlow is as follows:</p>
<ul>
<li><strong>Build a computational graph,</strong> this can be any mathematical operation TensorFlow supports.</li>
<li><strong>Initialize variables,</strong> to compile the variables defined previously</li>
<li><strong>Create session(会话）,</strong> this is where the magic starts!</li>
<li><strong>Run graph in session,</strong> the compiled graph is passed to the session, which starts its execution. </li>
<li><strong>Close session,</strong> shutdown the session.</li>
</ul>
<p>Lets write a small program to add two numbers!</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># import tensorflow</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># build computational graph</span></span><br><span class="line">a = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.placeholder(tf.int16)</span><br><span class="line"></span><br><span class="line">addition = tf.add(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize variables</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create session and run the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Addition: %i"</span> % sess.run(addition, feed_dict={a: <span class="number">2</span>, b: <span class="number">3</span>})</span><br><span class="line"></span><br><span class="line"><span class="comment"># close session</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></tbody></table></figure>
<p>A typical implementation of Neural Network would be as follows:</p>
<ul>
<li><p>Define Neural Network architecture to be compiled</p>
</li>
<li><p>Transfer data to your model</p>
</li>
<li><p>Under the hood, the data is first divided into batches, so that it can be ingested. The batches are first preprocessed, augmented and then fed into Neural Network for training</p>
</li>
<li><p>The model then gets trained incrementally</p>
</li>
<li><p>Display the accuracy for a specific number of timesteps</p>
</li>
<li><p>After training save the model for future use</p>
</li>
<li><p>Test the model on a new data and check how it performs</p>
<h2><span id="san-lei-fei-chang-chong-yao-de-bian-liang">三类非常重要的变量</span><a href="#san-lei-fei-chang-chong-yao-de-bian-liang" class="header-anchor">#</a></h2><h3><span id="zhan-wei-fu">占位符</span><a href="#zhan-wei-fu" class="header-anchor">#</a></h3><p>tensorFlow中接收值的方式为占位符(placeholder)，创建placeholder</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">- <span class="comment"># b = tf.placeholder(tf.float32, [None, 1], name='b')</span></span><br><span class="line"></span><br><span class="line">第二个参数值为[<span class="literal">None</span>, <span class="number">1</span>]，其中<span class="literal">None</span>表示不确定，即不确定第一个维度的大小，第一维可以是任意大小。特别对应tensor数量(或者样本数量)，输入的tensor数目可以是<span class="number">32</span>、<span class="number">64</span>…</span><br></pre></td></tr></tbody></table></figure>
<p>placeholder: A way to feed data into the graphs<br>feed_dict: A dictionary to pass numeric values to computational graph</p>
<h3><span id="chang-liang">常量</span><a href="#chang-liang" class="header-anchor">#</a></h3><p>tf.constant()`定义常量</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">const = tf.constant(<span class="number">2.0</span>, name=<span class="string">'const'</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="bian-liang">变量</span><a href="#bian-liang" class="header-anchor">#</a></h3></li>
</ul>
<p>​    使用<code>tf.Variable()</code>定义变量</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">c = tf.Variable(<span class="number">1.0</span>, dtype=tf.float32, name=<span class="string">'c'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>TensorFlow中所有的变量必须经过初始化才能使用，**初始化方式分两步：</p>
<ol>
<li><p>定义初始化operation</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义init operation</span></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>运行初始化operation</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 2. 运行init operation</span></span><br><span class="line">	sess.run(init_op)</span><br></pre></td></tr></tbody></table></figure>
</li>
</ol>
<p>reference</p>
<p><a href="https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/">https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</a></p>
<p><a href="https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial">https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial</a></p>
<p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p>
<p>video:<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p>
<p>course: <a href="https://classroom.udacity.com/courses/ud187">https://classroom.udacity.com/courses/ud187</a></p>
<p>tensorflow: GOOGLE 开源、Deep learning</p>
<h2><span id="lian-shu-cheng-jin">练数成金</span><a href="#lian-shu-cheng-jin" class="header-anchor">#</a></h2><h3><span id="c1">C1</span><a href="#c1" class="header-anchor">#</a></h3><ol>
<li><p>tensorboard ：a tool;visual network;debug</p>
</li>
<li><p>alter dir of jupyter 顺便改了下新下载的路径（GOOD）</p>
</li>
<li><p>CPU or GPU</p>
</li>
</ol>
<h3><span id="c2">C2</span><a href="#c2" class="header-anchor">#</a></h3><p>graphs 代表计算任务，节点（op)，一个op可以获得o个或者多个tensor,输出1个或者多个tensor</p>
<p>Session(会话)的上下文（context)中执行</p>
<p>tensor表示数据,n维数组</p>
<h3><span id="c3">C3</span><a href="#c3" class="header-anchor">#</a></h3><ol>
<li><p>简单的回归神经网络（拟合二次函数），貌似学了理论没有实践，还真是忘得快啊</p>
</li>
<li><p>手写体分类、Softmax函数</p>
<p>softmax函数可以给不同的对象分配概率，softmax($x_i$)=$\frac{exp(x_i)}{\sum_j{exp(x_j)}}$</p>
<p>如输出[1,2,5] ,$p1=\frac{exp(1)}{exp(1)+exp(2)+exp(5)}$,$p2=\frac{exp(2)}{exp(1)+exp(2)+exp(5)}$,$p1=\frac{exp(5)}{exp(1)+exp(2)+exp(5)}$</p>
</li>
</ol>
<h2><span id="keras">Keras</span><a href="#keras" class="header-anchor">#</a></h2><ol>
<li><p>安装</p>
</li>
<li><p>backend</p>
<p>基于什么做运算（tensorflow or theano)</p>
<p>import keras 查看 底层搭建</p>
<p> a） /.keras/keras.json 相关的配置信息  </p>
<p>b) 终端改，单次</p>
<p>import os</p>
<p>os.environ[‘KERAS_BACKEND’]= ‘tensorflow’</p>
<p>import keras</p>
</li>
<li><p>For example</p>
<p>model :Sequential</p>
<p>layer : Dense activation</p>
<p>训练算法：model.compile(参数optimizer=’梯度下降法的变种’ , loss=’rms/‘)</p>
<p>训练：model. fit (x,y) model.train_on_batch</p>
<p>evaluate:model.evaluate</p>
<p>prediction:  model.predict(x_test, batch_size=128)</p>
</li>
</ol>
<p>   <a href="https://github.com/MorvanZhou/tutorials/blob/master/kerasTUT/5-classifier_example.py">https://github.com/MorvanZhou/tutorials/blob/master/kerasTUT/5-classifier_example.py</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 4 - Regressor example</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential <span class="comment"># 按顺序建立</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense <span class="comment"># 全连接层</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># create some data</span></span><br><span class="line">X = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)</span><br><span class="line">np.random.shuffle(X)    <span class="comment"># randomize the data</span></span><br><span class="line">Y = <span class="number">0.5</span> * X + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, (<span class="number">200</span>, ))</span><br><span class="line"><span class="comment"># plot data</span></span><br><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X_train, Y_train = X[:<span class="number">160</span>], Y[:<span class="number">160</span>]     <span class="comment"># first 160 data points</span></span><br><span class="line">X_test, Y_test = X[<span class="number">160</span>:], Y[<span class="number">160</span>:]       <span class="comment"># last 40 data points</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a neural network from the 1st layer to the last layer</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units=<span class="number">1</span>, input_dim=<span class="number">1</span>)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># choose loss function and optimizing method</span></span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>, optimizer=<span class="string">'sgd'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line">print(<span class="string">'Training -----------'</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">301</span>):</span><br><span class="line">    cost = model.train_on_batch(X_train, Y_train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'train cost: '</span>, cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line">cost = model.evaluate(X_test, Y_test, batch_size=<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'test cost:'</span>, cost)</span><br><span class="line">W, b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line">print(<span class="string">'Weights='</span>, W, <span class="string">'\nbiases='</span>, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plotting the prediction</span></span><br><span class="line">Y_pred = model.predict(X_test)</span><br><span class="line">plt.scatter(X_test, Y_test)</span><br><span class="line">plt.plot(X_test, Y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="5">5</span><a href="#5" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">To know more or get code samples, please visit my website:</span></span><br><span class="line"><span class="string">https://morvanzhou.github.io/tutorials/</span></span><br><span class="line"><span class="string">Or search: 莫烦Python</span></span><br><span class="line"><span class="string">Thank you for supporting!</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># please note, all tutorial code are running under python3.5.</span></span><br><span class="line"><span class="comment"># If you use the version like python2.7, please modify the code accordingly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 - Classifier example</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the mnist to the path '~/.keras/datasets/' if it is the first time to be called</span></span><br><span class="line"><span class="comment"># X shape (60,000 28x28), y shape (10,000, )</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># data pre-processing</span></span><br><span class="line">X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>   <span class="comment"># normalize</span></span><br><span class="line">X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">-1</span>) / <span class="number">255.</span>      <span class="comment"># normalize</span></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to build your neural net</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(<span class="number">32</span>, input_dim=<span class="number">784</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">10</span>),</span><br><span class="line">    Activation(<span class="string">'softmax'</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to define your optimizer</span></span><br><span class="line">rmsprop = RMSprop(lr=<span class="number">0.001</span>, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-08</span>, decay=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We add metrics to get more results you want to see</span></span><br><span class="line">model.compile(optimizer=rmsprop,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line"><span class="comment"># Another way to train the model</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">2</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line"><span class="comment"># Evaluate the model with the metrics we defined earlier</span></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'test loss: '</span>, loss)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, accuracy)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="6-cnn-juan-ji-shen-jing-wang-luo">6 CNN卷积神经网络</span><a href="#6-cnn-juan-ji-shen-jing-wang-luo" class="header-anchor">#</a></h2><p>不是对</p>
<p><a href="https://www.cnblogs.com/skyfsm/p/6790245.html">https://www.cnblogs.com/skyfsm/p/6790245.html</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">To know more or get code samples, please visit my website:</span></span><br><span class="line"><span class="string">https://morvanzhou.github.io/tutorials/</span></span><br><span class="line"><span class="string">Or search: 莫烦Python</span></span><br><span class="line"><span class="string">Thank you for supporting!</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># please note, all tutorial code are running under python3.5.</span></span><br><span class="line"><span class="comment"># If you use the version like python2.7, please modify the code accordingly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6 - CNN example</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># to try tensorflow, un-comment following two lines</span></span><br><span class="line"><span class="comment"># import os</span></span><br><span class="line"><span class="comment"># os.environ['KERAS_BACKEND']='tensorflow'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Convolution2D, MaxPooling2D, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># download the mnist to the path '~/.keras/datasets/' if it is the first time to be called</span></span><br><span class="line"><span class="comment"># training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># data pre-processing</span></span><br><span class="line">X_train = X_train.reshape(<span class="number">-1</span>, <span class="number">1</span>,<span class="number">28</span>, <span class="number">28</span>)/<span class="number">255.</span></span><br><span class="line">X_test = X_test.reshape(<span class="number">-1</span>, <span class="number">1</span>,<span class="number">28</span>, <span class="number">28</span>)/<span class="number">255.</span></span><br><span class="line">y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">10</span>)</span><br><span class="line">y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to build your CNN</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Conv layer 1 output shape (32, 28, 28)</span></span><br><span class="line">model.add(Convolution2D(</span><br><span class="line">    batch_input_shape=(<span class="literal">None</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</span><br><span class="line">    filters=<span class="number">32</span>,</span><br><span class="line">    kernel_size=<span class="number">5</span>,</span><br><span class="line">    strides=<span class="number">1</span>,</span><br><span class="line">    padding=<span class="string">'same'</span>,     <span class="comment"># Padding method</span></span><br><span class="line">    data_format=<span class="string">'channels_first'</span>,</span><br><span class="line">))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pooling layer 1 (max pooling) output shape (32, 14, 14)</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,</span><br><span class="line">    strides=<span class="number">2</span>,</span><br><span class="line">    padding=<span class="string">'same'</span>,    <span class="comment"># Padding method</span></span><br><span class="line">    data_format=<span class="string">'channels_first'</span>,</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Conv layer 2 output shape (64, 14, 14)</span></span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>, data_format=<span class="string">'channels_first'</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pooling layer 2 (max pooling) output shape (64, 7, 7)</span></span><br><span class="line">model.add(MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>, <span class="string">'same'</span>, data_format=<span class="string">'channels_first'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024)</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1024</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fully connected layer 2 to shape (10) for 10 classes</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another way to define your optimizer</span></span><br><span class="line">adam = Adam(lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We add metrics to get more results you want to see</span></span><br><span class="line">model.compile(optimizer=adam,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Training ------------'</span>)</span><br><span class="line"><span class="comment"># Another way to train the model</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">64</span>,)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nTesting ------------'</span>)</span><br><span class="line"><span class="comment"># Evaluate the model with the metrics we defined earlier</span></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntest loss: '</span>, loss)</span><br><span class="line">print(<span class="string">'\ntest accuracy: '</span>, accuracy)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tensorlow</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning Neural Network and Deep Learning</title>
    <url>/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/</url>
    <content><![CDATA[<h1><span id="course-one-neural-networks-and-deep-learning-course-1-of-the-deep-learning-specialization">Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)</span><a href="#course-one-neural-networks-and-deep-learning-course-1-of-the-deep-learning-specialization" class="header-anchor">#</a></h1><h1><span id="c1w1">C1W1</span><a href="#c1w1" class="header-anchor">#</a></h1><h2><span id="c1w1l01-welcome">C1W1L01: Welcome</span><a href="#c1w1l01-welcome" class="header-anchor">#</a></h2><p>AI is the new Electricity!</p>
<p>Course 1: Neural Networks and Deep Learning</p>
<p>Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization</p>
<p>Course 3: Structuring your Machine Learning project</p>
<p>Course 4: Convolutional Neural Networks</p>
<p>Course 5: Natural Langurge Processing: Building sequence models</p>
<h2><span id="c1w1l02-what-is-neural-network">C1W1L02 : What is Neural Network</span><a href="#c1w1l02-what-is-neural-network" class="header-anchor">#</a></h2><p>Deep Learning = training (very large) neural network</p>
<h3><span id="for-example-of-house-prize-prediction-the-simplest-neural-network">For example of house prize prediction : the simplest neural network</span><a href="#for-example-of-house-prize-prediction-the-simplest-neural-network" class="header-anchor">#</a></h3><p>如果现在有六栋房子的信息，分别是房子的大小(size of house)和对应的价格(prize),绘制出如下的。自然的想法：线性回归，得到拟合的直线。值得注意的是，房价不可能是负数吧！因此下图中蓝色的线，大致就是我们所需要的函数。这个对应一个最简单神经网络（neural network）</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/housr_prize_1.png" alt></p>
<p>上述是一个tiny little neural network，更大的，更复杂的神经网络是</p>
<p>把很多最简单的single neural堆积(stacking)到一起。</p>
<h3><span id="for-example-of-house-prize-prediction-stacking-the-neural">For example of house prize prediction : stacking the  neural</span><a href="#for-example-of-house-prize-prediction-stacking-the-neural" class="header-anchor">#</a></h3><p>上面这个例子，仅仅考虑特征是size,实际情况上，与房屋相关的特征还有number of bedrooms、zip code、wealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house_prize_2.png" alt></p>
<p>hidden layer 用输入层计算得到，因此说输入层与中间层紧密连接起来了</p>
<h3><span id="the-actual-application-of-neural-networks">The actual application of neural networks</span><a href="#the-actual-application-of-neural-networks" class="header-anchor">#</a></h3><p>hidden layer 与上一层的连接情况并不是手工确定，每一层都是上一层所有的输入函数，所以建立的神经网络如下：</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house prize 3.png" alt></p>
<p>The remarkable thing about neural network</p>
<ol>
<li>Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y</li>
<li>Most powerful in supervised learning</li>
</ol>
<h3><span id="c2w1cl03-supervised-learning-with-neural-network">C2W1CL03 : Supervised Learning with Neural Network</span><a href="#c2w1cl03-supervised-learning-with-neural-network" class="header-anchor">#</a></h3><h3><span id="chang-jian-de-jian-du-xue-xi">常见的监督学习</span><a href="#chang-jian-de-jian-du-xue-xi" class="header-anchor">#</a></h3><p>截止到目前，<strong>Neural Network的成功应用基本都在Supervised Learning</strong>。比如：Ad，Images vision, Audio to Text, Machine translation, Autonomous Driving</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/supervised-learning-exmples.png" alt></p>
<h3><span id="chang-jian-de-shen-jing-wang-luo-de-she-ji">常见的神经网络的设计</span><a href="#chang-jian-de-shen-jing-wang-luo-de-she-ji" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/NeuralNetworkExamples.png" alt></p>
<p>卷积神经网络：<strong>Convolutional Neural Network</strong> (CNN) 通常有用图像数据</p>
<p>递归神经网络： <strong>Recurrent Neural Network</strong> (RNN) 通常用于time series</p>
<p>对应复杂的应用中，定制一些复杂的混合的神经网络结构</p>
<h3><span id="jie-gou-hua-he-fei-jie-gou-hua-shu-ju">结构化和非结构化数据</span><a href="#jie-gou-hua-he-fei-jie-gou-hua-shu-ju" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/datastructure.png" alt></p>
<p>处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难</p>
<h3><span id="c1w1l04-why-is-deep-learning-taking-off">C1W1L04: Why is deep learning taking off</span><a href="#c1w1l04-why-is-deep-learning-taking-off" class="header-anchor">#</a></h3><p>Answer: scale</p>
<p>If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data.</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/scale.jpg" alt></p>
<ol>
<li>If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.在这个小的训练集中，各种算法的优先级事实上定义的也不是很明确，所以如果你没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。</li>
<li>这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用<strong>工程选择特征</strong>方面的能力以及<strong>算法处理方面</strong>的一些细节.</li>
<li>只是在某些大数据规模非常庞大的训练集，也就是在右边这个会非常的大时，我们能更加持续地看到更大的由神经网络控制其它方法.</li>
</ol>
<h3><span id="the-reason">The Reason</span><a href="#the-reason" class="header-anchor">#</a></h3><ol>
<li><p>the scale of data</p>
</li>
<li><p>the speed of computation  such as GPUS</p>
</li>
<li><p>innovation of algorithm </p>
<p>许多算法方面的创新，一直是在尝试着使得神经网络运行的更快</p>
<p>switch sigmoid function to relu function</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/algorithm——rul.jpg" alt></p>
</li>
</ol>
<p>在这个区域，也就是这个<strong>sigmoid</strong>函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做<strong>ReLU</strong>的函数（修正线性单元），<strong>ReLU</strong>它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。</p>
<p>训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/faster.jpg" alt></p>
<h2><span id="summary"><font color="red">Summary</font></span><a href="#summary" class="header-anchor">#</a></h2><font color="green">早上花了2h小时学习第一周的视频，先看一遍视频的字幕，逐字逐句的理解，虽然很多时候都是自己乱猜的，大概清楚讲的什么！然后再看大牛的笔记，然后再看一篇结合PPT。下午也看了半个多小时。问题：1. 自己的英文水平不够，这个需要大大的提高讷。2. 其实只要看别人的笔记就可以知道内容，但是还是想听andow ng的讲解。3. 视频都比较短，每个视频设计的知识点或者内容不多，1到3个，分成知识点做笔记还是不错的</font>

<font color="blue">这一周的内容，也就是今天我学习的知识简单和容易理解。学习了神经网络的大致结构，神经网络的应用领域，深度学习为什么取得快速的发展的三点原因，尤其是数据scale与其他方法和神经网络规模的大致性能关系</font>

<h1><span id="c1w2">C1W2</span><a href="#c1w2" class="header-anchor">#</a></h1><h3><span id="c1w2l01-binary-classification">C1W2L01: Binary Classification</span><a href="#c1w2l01-binary-classification" class="header-anchor">#</a></h3><p>In this week, we’re going to go over the basics of neural network programming. We are going to study handle data without for loop.</p>
<p>forward password for propagation </p>
<p> backward pass or what’s called a backward propagation step</p>
<p>Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(传达) theses ideas.</p>
<h3><span id="binary-classification">Binary Classification</span><a href="#binary-classification" class="header-anchor">#</a></h3><p>Input； an image . three separate matrices corresponding red green and blue color channels of this image. 如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值</p>
<p>unroll all of these pixel intensity values into  a feature vector </p>
<p>pixel intensity values of this image </p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/w_piexl.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_blue_green_read.jpg" alt></p>
<p>notation</p>
<p>(x,y)： a pair X comma Y</p>
<p>$M_{train}$: M subscript train</p>
<p>每条测试集在矩阵中都是以列向量的形式存在</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_noation.png" alt></p>
<p>Matrix capital</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_nation_2.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation_1.jpg" alt></p>
<h3><span id="model-hypothesis-function-logistic-regression">Model : hypothesis Function :Logistic Regression</span><a href="#model-hypothesis-function-logistic-regression" class="header-anchor">#</a></h3><p>So given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesn’t work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isn’t a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. </p>
<p>So,Y hat should really be between zero and one. This is what the sigmoid function looks like. </p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_1.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.jpg" alt></p>
<h3><span id="sigmoid-function">sigmoid function</span><a href="#sigmoid-function" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1+e^{-z}}</script><p>因为你想让$\hat{y}$表示实际值$y$等于1的机率的话， 应该在0到1之间。这是一个需要解决的问题，因为可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是等于由上面得到的线性函数式子作为自变量的<strong>sigmoid</strong>函数中，公式如上图最下面所示，将线性函数转换为非线性函数。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_2.jpg" alt></p>
<p>注意：原来$w,b$是分开在，这里就合并，引入变量$x_0=1$,对应偏置$b$,</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.png" alt></p>
<h3><span id="strategy-cost-function">Strategy：Cost function</span><a href="#strategy-cost-function" class="header-anchor">#</a></h3><p>Firstly : Loss function</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2}</script><p>这个优化问题不是凸优化问题(non-convex)，因此不选用这个</p>
<p>Secondly，</p>
<script type="math/tex; mode=display">
L(y,\hat{y})=-(ylog^{\hat{y}}+(1-y)log^{1-\hat{y}})</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_cost_1.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log__cost_2.jpg" alt></p>
<h3><span id="algorithm-gradient-descent">Algorithm: Gradient Descent</span><a href="#algorithm-gradient-descent" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/GD1.jpg" alt></p>
<p>Gradient Descent算法步骤：</p>
<ol>
<li>Initialize $w$, $b$ to zero</li>
<li>repeat：</li>
</ol>
<p>$w :=w−\alpha \frac{∂J(w,b)}{∂w}$</p>
<p>$b :=b-\alpha \frac{∂J(w,b)}{∂b}$</p>
<h3><span id="c1w2l05-amp-c1w2l06-derivatives">C1W2L05 &amp; C1W2L06 Derivatives</span><a href="#c1w2l05-amp-c1w2l06-derivatives" class="header-anchor">#</a></h3><p>求导，这个是微积分的内容，不用写了！</p>
<h3><span id="c1w2l07-computation-graph">C1W2L07： Computation Graph</span><a href="#c1w2l07-computation-graph" class="header-anchor">#</a></h3><h3><span id="c1w2l08-derivatives-with-compution-graphs">C1W2L08 : Derivatives with compution graphs</span><a href="#c1w2l08-derivatives-with-compution-graphs" class="header-anchor">#</a></h3><p>链式法则</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v}</script><h3><span id="c1w2l09-logistic-regression-gradient-descent">C1W2L09 : Logistic Regression Gradient Descent</span><a href="#c1w2l09-logistic-regression-gradient-descent" class="header-anchor">#</a></h3><h4><span id="single-training-example">single training example</span><a href="#single-training-example" class="header-anchor">#</a></h4><p>You’ve seen the loss function that measures how well you’re doing on the single training example. You’ve also seen the cost function that measures how well your parameters w and b are doing on your entire training set.</p>
<p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives.</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_1.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_2.png" alt></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w}
\\=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)x</script><h3><span id="c1w2l10-gradient-descent-on-m-example">C1W2L10 Gradient Descent on m example</span><a href="#c1w2l10-gradient-descent-on-m-example" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\\
\frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\\
\frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_3jpg.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_3.png" alt></p>
<p>上面的伪代码告诉我们，需要多次for loop完成代码，但是这会造成运算速度下降！因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个<strong>for</strong>循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰</p>
<h3><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h3><font color="red">今天主要学习了以logistics regression 为例，如何通过链式求导的过程，简单的练习一下，以及再次了解什么是梯度下降法，以及训练学习算法的需要一个损失函数，训练的过程就是求损失函数最优值的过程</font>

<h3><span id="c1w2l11-vectorization">C1W2L11: Vectorization</span><a href="#c1w2l11-vectorization" class="header-anchor">#</a></h3><h4><span id="1-shi-me-shi-vectorization-jiang-for-loop-jin-ke-neng-zhuan-huan-wei-ju-zhen-yun-suan">1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算</span><a href="#1-shi-me-shi-vectorization-jiang-for-loop-jin-ke-neng-zhuan-huan-wei-ju-zhen-yun-suan" class="header-anchor">#</a></h4><p>通过<strong>numpy</strong>内置函数和避开显式的循环(<strong>loop</strong>)的方式进行向量化，从而有效提高代码速度。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_1.jpg" alt></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">np.dot(a,b)</span><br><span class="line">如果a,b是一维数组，则计算点积</span><br><span class="line">如果a,b是多维数据，则矩阵乘法</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="2-an-example-of-vectorization">2. An example of vectorization</span><a href="#2-an-example-of-vectorization" class="header-anchor">#</a></h4><p>vectorization的好处：conciser code, but faster execution 一个简单的对比实验：1,000,000大小的两个向量内积计算，for loop要比Vectorization快300倍。 在Deep Learning时代，vectorization是一项重要的技能。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy库</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#创建一个数据a</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># [1 2 3 4]</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment">#导入时间库</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>) <span class="comment">#通过round随机得到两个一百万维度的数组</span></span><br><span class="line">tic = time.time() <span class="comment">#现在测量一下当前时间</span></span><br><span class="line"><span class="comment">#向量化的版本</span></span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line">print(“Vectorized version:” + str(<span class="number">1000</span>*(toc-tic)) +”ms”) <span class="comment">#打印一下向量化的版本的时间</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#继续增加非向量化的版本</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(“For loop:” + str(<span class="number">1000</span>*(toc-tic)) + “ms”)<span class="comment">#打印for循环的版本的时间</span></span><br></pre></td></tr></tbody></table></figure>
<h4><span id="3-gpu-or-cpu">3. GPU or CPU</span><a href="#3-gpu-or-cpu" class="header-anchor">#</a></h4><ol>
<li><p>大规模的深度学习再<strong>GPU</strong>或者图像处理单元运行”，<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算。</p>
</li>
<li><p>只是在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。SIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)</p>
</li>
</ol>
<h3><span id="c12l12-more-vectorization-example">C12L12 ： More Vectorization Example</span><a href="#c12l12-more-vectorization-example" class="header-anchor">#</a></h3><h3><span id="ju-zhen-he-xiang-liang-cheng-fa">矩阵和向量乘法</span><a href="#ju-zhen-he-xiang-liang-cheng-fa" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_2.png" alt></p>
<h3><span id="xiang-liang-han-shu">向量函数</span><a href="#xiang-liang-han-shu" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.png" alt></p>
<ol>
<li>原则：whenever possible, avoid explict for-loops</li>
<li>使用Element wised的矩阵运算，将函数作用在每个矩阵元素上，比如：<ul>
<li>np.exp()</li>
<li>np.log()</li>
<li>np.abs()</li>
<li>np.maxium()</li>
<li>1/v</li>
<li>v**2</li>
</ul>
</li>
</ol>
<h3><span id="c1w2l13-vectorizing-logistic-regression">C1W2L13: Vectorizing Logistic Regression</span><a href="#c1w2l13-vectorizing-logistic-regression" class="header-anchor">#</a></h3><h3><span id="1-qian-xiang-chuan-bo">1. 前向传播</span><a href="#1-qian-xiang-chuan-bo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.jpg" alt></p>
<script type="math/tex; mode=display">
\hat{y}=σ(w^TX+b)=(a(1),a(2),...,a(m−1),a(m))=\\
(\alpha(z_1),\alpha(z_m),...,\alpha(z_m))=\\
(\alpha(w^Tx_1+b),\alpha(w^Tx_2+b),...,\alpha(w^Tx_m+b))=</script><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z=np.dot(W^T,X)+b</span><br><span class="line"><span class="comment"># z这里就是python 巧妙的地方，b是实数，但是向量加上实数后，b扩展成向量，被称为广播（brosdcasting）</span></span><br></pre></td></tr></tbody></table></figure>
<p>个人经验：</p>
<ol>
<li>首先，熟悉每个变量的记号和维度，必要的话，可以画出来，更直观。</li>
<li>先从一个样本做向量化，再把m个样本的操作向量化。</li>
<li>for-loop里面是循环乘法，则向量化一定是一个乘法形式，若对于不确定乘法的左右关系，是否需转置，可以根据目标变量的维度推测。或者先乘起来，再根据目标变量看是否要转置。</li>
</ol>
<h3><span id="c1w2l14-vectorzing-logistic-regression-s-gradient-compution">C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution</span><a href="#c1w2l14-vectorzing-logistic-regression-s-gradient-compution" class="header-anchor">#</a></h3><ol>
<li><p>backforwd</p>
</li>
<li><script type="math/tex; mode=display">
\frac{∂J}{∂w}=\frac{1}{m}X(A−Y)T\\
\frac{∂J}{∂b}=\frac{1}{m}(a(i)−y(i))</script></li>
</ol>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v-4.jpg" alt></p>
<p>重要的是弄清楚，里面的行列关系，代表的意思，运算时候，先自己理清楚。还有点积、等等运算性质对应的操作，或者对应的内置函数</p>
<h3><span id="c1w2l15-broadcasting-in-python">C1W2L15: Broadcasting in Python</span><a href="#c1w2l15-broadcasting-in-python" class="header-anchor">#</a></h3><h3><span id="one-example">One Example</span><a href="#one-example" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_4.png" alt></p>
<p><code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<strong>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_5.png" alt></p>
<p>第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy</strong>中的广播机制。这里使用 3 by 4的矩阵除以1 by 4 的矩阵。技术上来讲，其实并不需要再将矩阵 <code>reshape</code>(重塑)成 ，因为矩阵本身已经是 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是，它的调用代价极低。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_6.png" alt></p>
<h3><span id="secondly-example">Secondly Example</span><a href="#secondly-example" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_2.png" alt></p>
<p>python的广播机制会将常数扩展成4by 1的列向量</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_3.png" alt></p>
<p>其实是将1by*n 的矩阵复制成为mbyn的矩阵</p>
<h3><span id="guang-bo-ji-zhi-de-ju-li">广播机制的举例</span><a href="#guang-bo-ji-zhi-de-ju-li" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordcasing_1.png" alt></p>
<h3><span id="axis">axis</span><a href="#axis" class="header-anchor">#</a></h3><p>补充：numpy中，类似sum的函数，经常涉及axis参数，可以取值为0或1，甚至其他。经常记不住，这里我查了了一下，是这样的（<a href="https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array">原文</a>）：</p>
<ol>
<li><strong>axis的数字，和数组的shape参数的索引是对应的</strong>。比如一个数组的shape是(5,6)，则代表5个row，6个column。即在shape中，row和column的个数的索引是0和1。也就第1个坐标，在shape中的第一个元素，<strong>索引是0，代表row的方向</strong>；第2个坐标，在shape中的第2个元素，<strong>索引是1，代表row的方向</strong>。</li>
<li>对于sum函数，axis指的是sum“<strong>沿着</strong>”的方向，经过计算，这个方向的维度因为求和后就消失了，比如sum(axis=0)代表是沿着“row”方向进行求和，</li>
<li>当然axis可以是一个tupe，那就相当于沿着多个多个方向求和。</li>
<li>sum如果不传入axis参数，默认是对所有维度求和。</li>
</ol>
<h3><span id="broadcasting">broadcasting</span><a href="#broadcasting" class="header-anchor">#</a></h3><p>  当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）。</p>
<p>三种广播情况</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing.png" alt></p>
<h3><span id="c1w2l16-a-note-on-python-numpy-vectors">C1W2L16 A Note on Python/numpy vectors</span><a href="#c1w2l16-a-note-on-python-numpy-vectors" class="header-anchor">#</a></h3><p>本节主要讲<strong>Python</strong>中的<strong>numpy</strong>一维数组的特性，以及与行向量或列向量的区别</p>
<h3><span id="1-yi-wei-shu-zu-de-te-xing">1. 一维数组的特性</span><a href="#1-yi-wei-shu-zu-de-te-xing" class="header-anchor">#</a></h3><p>首先设置a = np.array.random.randn(5)，这样会生成存储在数组a中的5个高斯随机数变量。之后输出 ，从屏幕上可以得知，此时a 的<strong>shape</strong>（形状）是一个的结构。这在<strong>Python</strong>中被称作<strong>一个一维数组</strong>。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和看起来一样，所以和的转置阵最终结果看起来一样。而如果我输出和的转置阵的内积，你可能会想：乘以的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。</p>
<p>所以我建议当你编写神经网络时，不要在使用的<strong>shape</strong>(5,1)是还是(n,)或者一维数组。相反，如果你设置(5,1)，那么这就是5行1列向量。在先前的操作里a和a的转置看起来一样，而现在这样的 a变成一个新的a 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出a 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p>
<h3><span id="2-xing-xiang-liang-he-lie-xiang-liang">2. 行向量和列向量</span><a href="#2-xing-xiang-liang-he-lie-xiang-liang" class="header-anchor">#</a></h3><p><strong>rank 1 array问题</strong>：shape是(x,)的数组，既不是行向量，也不是列向量，没法参与正常的矩阵运算，应该总是使用(x,1)或(1,x)的shape来表示向量。但可以通过reshape方法将rank 1 array转换为行向量或列向量。（什么是rank，就是一个数组的维度）一维的数组既不是行向量也不是列向量，转置后，依然是本身。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/note_1.jpg" alt></p>
<h3><span id="3-jie-jue-fang-fa">3. 解决方法</span><a href="#3-jie-jue-fang-fa" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape=（<span class="number">5</span>，<span class="number">1</span>)）</span><br><span class="line"><span class="comment"># 为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作</span></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="c1w2l18-quick-tour-of-jupyter-ipython-notebooks">C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks</span><a href="#c1w2l18-quick-tour-of-jupyter-ipython-notebooks" class="header-anchor">#</a></h3><h3><span id="c1w2l18-explanation-of-logistic-regression-cost-function">C1W2L18: Explanation of Logistic Regression Cost Function</span><a href="#c1w2l18-explanation-of-logistic-regression-cost-function" class="header-anchor">#</a></h3><p>对应logistic regression，输出$\hat{y}=p(y=1|x)$,那么$p(y=0|x)=1-\hat{y}$</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_2png.png" alt></p>
<p>综合上面</p>
<script type="math/tex; mode=display">
p(y|x)= \hat{y}^y*(1-\hat{y})^{1-y}</script><p>对于整个训练集，</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_1png.png" alt></p>
<p>假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:</p>
<script type="math/tex; mode=display">
p(labels \ in\  training\  set)=\Pi_{i=1}^mp(y_i|x_i)</script><p>如果利用极大似然法做，找到一组参数，使得样本观测值概率最大</p>
<script type="math/tex; mode=display">
\max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{y^i},y^i)</script><script type="math/tex; mode=display">
\min cost J(w,b)=\frac{1}{m}L(\hat{y^i},y^i)</script><p>总结一下，为了最小化成本函数，我们从<strong>logistic</strong>回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下</p>
<h3><span id="day3-summary">Day3 : summary</span><a href="#day3-summary" class="header-anchor">#</a></h3><font color="red">主要学习了python编程的如何才能高效率，内置函数的具有并行性，simd指令，以及一维数组的使用注意事项，logistic regression的lost function的原理证明</font>

<h1><span id="c1w3">C1W3</span><a href="#c1w3" class="header-anchor">#</a></h1><h2><span id="c1w3l01-neural-network-overview">C1W3L01 : Neural Network Overview</span><a href="#c1w3l01-neural-network-overview" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_2.png" alt></p>
<p>许多<strong>sigmoid</strong>单元堆叠起来形成一个神经网络。</p>
<p>正向传播：输入层到layer one</p>
<script type="math/tex; mode=display">
\left.\begin{array}{c}{x} \\ {W^{[1]}} \\ {b^{[1]}}\end{array}\right\} \Longrightarrow z^{[1]}=W^{[1]} x+b^{[1]} \Longrightarrow a^{[1]}=\sigma\left(z^{[1)}\right)</script><p>layer one 到layer two</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{a^{(1]}=\sigma\left(z^{[1]}\right)} \\ {W^{[2]}} \\ {b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longrightarrow z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \Longrightarrow a^{[2]}=\sigma\left(z^{[2]}\right)} \\ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array}</script><p>反向传播</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \\ {d W^{[2]}} \\ {d b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longleftarrow d z^{[2]}=d\left(W^{[2]} \alpha^{[1]}+b^{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \\ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array}</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_3.png" alt></p>
<p>$W$的行数是本次结点个数，列数是上层节点个数</p>
<h2><span id="c1w3l02-nerual-network-representations">C1W3L02 : Nerual Network Representations</span><a href="#c1w3l02-nerual-network-representations" class="header-anchor">#</a></h2><p>符号说明</p>
<h2><span id="c1w3l03-computation-neural-network-output">C1W3L03： Computation Neural Network Output</span><a href="#c1w3l03-computation-neural-network-output" class="header-anchor">#</a></h2><h3><span id="a-simple-training-examples">A simple training examples</span><a href="#a-simple-training-examples" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_5.png" alt></p>
<p>其中，x表示输入特征，a表示每个神经元的输出，W表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的<strong>符号惯例</strong>，下同。</p>
<p><strong>神经网络的计算</strong></p>
<p>关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出，然后在第二步中你以<strong>sigmoid</strong>函数为激活函数计算（得出），一个神经网络只是这样子做了好多次重复计算。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_6.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_4.png" alt></p>
<p>说明：$w_i^{[1]}$和$W^{[1]}$的关系，一个按照logistic regression ，一个是矩阵表示。</p>
<p><strong>向量化计算</strong> 如果你执行神经网络的程序，用for循环来做这些看起来真的很低效。所以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量，把这四个向量堆积在一起，你会得出这4×3的矩阵。</p>
<script type="math/tex; mode=display">
z^{[n]}=W^{[n]}X+b^{[n]}</script><script type="math/tex; mode=display">
a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \\ {a_{2}^{[1]}} \\ {a_{3}^{[1]}} \\ {a_{4}^{[1]}}\end{array}\right]=\sigma\left(z^{[1]}\right)</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_7.png" alt></p>
<p>Given input X（a single training set)</p>
<script type="math/tex; mode=display">
\begin{array}{c}{z^{[1]}=W^{[1]} a^{[0]}+b^{[1]}} \\ {a^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}} \\ {a^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}</script><p>说明：</p>
<p>$W$的第$i$行表示，当前层到上一层的权重行向量，再计算单个的时候，由于是按照logristics regression的方式，所以认为$w_i$是列向量，所以转置成行向量。上面的图也说明了：如何从单个操作到矩阵操作，权重矩阵是怎么构造，怎么表示的。</p>
<p>b是列向量。</p>
<h2><span id="c1w3l04-vectorizing-across-mutilple-example">C1W3L04: Vectorizing Across Mutilple Example</span><a href="#c1w3l04-vectorizing-across-mutilple-example" class="header-anchor">#</a></h2><p>Different training examples in different columns of the matrix </p>
<ol>
<li><p>for loop</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_8.png" alt></p>
</li>
<li><p>vectorizing : stacking training set in columns</p>
<script type="math/tex; mode=display">
x=\left[ \begin{array}{cccc}{\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {x^{(1)}} & {x^{(2)}} & {\dots} & {x} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots}\end{array}\right]</script></li>
</ol>
<p>就有</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{A^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} A^{[1]}+b^{[2]}} \\ {A^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}\right.</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_9.png" alt></p>
<p>当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这个训练样本中的最终训练样本。</p>
<p>从水平上看，矩阵代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元。</p>
<h2><span id="c1w3l05-explanation-for-vectorized-implement">C1W3L05 : Explanation for vectorized implement</span><a href="#c1w3l05-explanation-for-vectorized-implement" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_10.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_11.png" alt></p>
<h2><span id="c1w3l06-activation-function">C1W3L06 : Activation Function</span><a href="#c1w3l06-activation-function" class="header-anchor">#</a></h2><p>在讨论优化算法时，有一点要说明：基本已经不用<strong>sigmoid</strong>激活函数了，<strong>tanh</strong>函数在所有场合都优于<strong>sigmoid</strong>函数。</p>
<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>），<strong>ReLu</strong>函数图像是如下图。$ a = max(0,z)$：  所以，只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，=0的导数是没有定义的。但是当编程实现的时候，的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用<strong>Relu</strong>激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个缺点是：当z是负值的时候，导数等于0。</p>
<p>这里也有另一个版本的<strong>Relu</strong>被称为<strong>Leaky Relu</strong>。</p>
<p>当是负值时，这个函数的值不是等于0，而是轻微的倾斜。</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p>
<p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p>
<p>在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_12.png" alt></p>
<p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu</strong>激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>。</p>
<p>通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<h2><span id="c1w3l07-why-non-linear-activation-functions">C1W3L07 : Why non-linear activation Functions</span><a href="#c1w3l07-why-non-linear-activation-functions" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13.png" alt></p>
<p>通过推导可以得出，如果使用线性激活函数，相当于没有隐藏层。无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。当当然，在output layer是可以不用activation function，或者用linear activation function；这种情况一般是要求输出实数集结果（比如预测房价）。即便如此，在hidden layer还是要用non-linear activation function。</p>
<h3><span id="sigmoid-activation-function">sigmoid activation function</span><a href="#sigmoid-activation-function" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_14.png" alt></p>
<script type="math/tex; mode=display">
\frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))</script><h3><span id="tanh-activation-function">tanh activation function</span><a href="#tanh-activation-function" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_15.png" alt></p>
<script type="math/tex; mode=display">
g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{x}+e^{-z}}</script><script type="math/tex; mode=display">
\frac{d}{d z} g(z)=1-(\tanh (z))^{2}</script><h3><span id="rectified-linear-unit-relu">Rectified linear unit(RelU)</span><a href="#rectified-linear-unit-relu" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_16.png" alt></p>
<script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在z= 0的时候给定其导数1,0；当然=0的情况很少</p>
<h3><span id="leaky-linear-unit-leaky-relu"><strong>Leaky linear unit (Leaky ReLU)</strong></span><a href="#leaky-linear-unit-leaky-relu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
g(z)=\max (0.01 z, z)</script><script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0.01} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在的z=0时候给定其导数1,0.01；当然的情况很少。</p>
<h2><span id="c1w3l09-gradient-descent-for-neural-networks">C1W3L09 : Gradient Descent For Neural Networks</span><a href="#c1w3l09-gradient-descent-for-neural-networks" class="header-anchor">#</a></h2><ol>
<li><p>gradient descent的关键是求cost function对参数的偏导数</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_17.png" alt></p>
</li>
<li><p>求导过程使用的是Backpropagation</p>
<ol>
<li><p>首先做forward propagation，求解出每一层的输出A</p>
<script type="math/tex; mode=display">
(1) z^{[1]}=W^{[1]} x+b^{[1]}\\
(2) a^{[1]}=\sigma\left(z^{[1]}\right)\\(3) z^{[2]}=W^{[2]}=W^{[2]} a^{[1]}+b^{[2]}\\(4) a^{[2]}=g^{[2]}\left(z^{[z]}\right)=\sigma\left(z^{[2]}\right)</script></li>
<li><p>然后向后，逐层求解对每一层参数的偏导数</p>
</li>
</ol>
</li>
</ol>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_18.png" alt></p>
<p>sum，<code>keepdims</code>是防止<strong>python</strong>输出那些古怪的秩数(n,)，加上这个确保阵矩阵这个向量输出的维度为(n,1）这样标准的形式。</p>
<h2><span id="c1wl10-backpropagation-intuition-optional">C1WL10: Backpropagation intuition (optional)</span><a href="#c1wl10-backpropagation-intuition-optional" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_19.png" alt></p>
<p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配</p>
<p>其实，对于一个神经元，输入部分：是权重和上一层输出的线性组合；输出：激活函数作用于输入，因此对$W$求偏导时，对激活函数求一次，再对线性组合求一次。对$b$求偏导是，对线性部分求偏导是1,这里用求和。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_20.png" alt></p>
<h2><span id="c1w3l11-random-initialization">C1W3L11: Random Initialization</span><a href="#c1w3l11-random-initialization" class="header-anchor">#</a></h2><p>`</p>
<p>与logistic regression不同，初始化参数不可固定为0，而是每个参数都要随机初始化。</p>
<p>主要原因是：<strong>如果每个参数w和b都是0，则同一层的每个neuron计算结果完全一样</strong>（输入一样a，参数一样w，则z一样,<strong>symmetry breaking problem</strong>）；接下来反向传播时的偏导数也一样，下一轮迭代同一层的每个neuron的w又是一样的。这样整个neural Network上每一层的neuron是同质的，自然不会有好的performance。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13 (1" alt>.png)</p>
<p>不过，对b参数，可以都初始化为0。</p>
<p>另外需要注意，虽然w是随机初始化，但最好使用较小的随机数。主要是避免让z的计算值过大，导致activation function对z的偏导数趋于0，导致Gradient descent下降较慢。 通常的做法是对random的值乘以一个比率，比如0.01（但具体怎么选这个比率，也要根据情况而定，这应该又是一个超参了）：</p>
<p>$W[1]=np.random.randn((2,2))∗0.01$</p>
<p>因为如果你用<strong>tanh</strong>或者<strong>sigmoid</strong>激活函数，或者说只在输出层有一个<strong>Sigmoid</strong>，如果（数值）波动太大，当你计算激活值时如果很大，就会很大或者很小，因此这种情况下你很可能停在<strong>tanh</strong>/<strong>sigmoid</strong>函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。</p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">如何建立一个一层的神经网络了，初始化参数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。</font>

<font color="blue">

1. Define the neural network structure ( # of input units, # of hidden units, etc).
2. Initialize the model's parameters

1. Loop:
   - Implement forward propagation
   - Compute loss
   - Implement backward propagation to get the gradients
   - Update parameters (gradient descent)

</font>

<h1><span id="c1w4">C1W4</span><a href="#c1w4" class="header-anchor">#</a></h1><h2><span id="c1w4l01-deep-layer-neural-network">C1W4L01 Deep-layer neural network</span><a href="#c1w4l01-deep-layer-neural-network" class="header-anchor">#</a></h2><h3><span id="1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network">1. logistics regression and shallow neural network and deep-layer neural network</span><a href="#1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_1.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_2.png" alt></p>
<h3><span id="2-notation">2. notation</span><a href="#2-notation" class="header-anchor">#</a></h3><p>神经网络模型</p>
<script type="math/tex; mode=display">
\begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} 代表输入的矩阵\\{x^{(i)} \in \mathbb{R}^{n_{x}}} 代表第 i 个样本的列向量\\
{Y \in \mathbb{R}^{n_{y} \times n}} 标记矩阵\\ {y^{(i)} \in \mathbb{R}^{n_{v}}}是第i样本的输出标签\\
W^{[l]} \in \mathbb{R}^{l \times(l-1)}代表第[l]层的权重矩阵\\ b^{[l]} \in \mathbb{R}^{l}代表第[l]层的偏差矩阵\\
 {\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}是预测输出向量\end{array}</script><script type="math/tex; mode=display">
通用激活公式：
a_{j}^{[l]}=g^{[l]}\left(z_{j}^{[l]}\right)=g^{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}^{[l-1]}+b_{j}^{[l]}\right)</script><h2><span id="c1w4l02-forward-and-backward-propagation">C1W4L02： Forward and Backward propagation</span><a href="#c1w4l02-forward-and-backward-propagation" class="header-anchor">#</a></h2><h3><span id="1-forward-propagation">1. forward propagation</span><a href="#1-forward-propagation" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_3.png" alt></p>
<h3><span id="2-backward-propagation">2. backward propagation</span><a href="#2-backward-propagation" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\begin{array}{l}{d z^{[l]}=d a^{[l]} * g^{[l]}\left(z^{l l}\right)} \\ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\\d b^{[l]}=d z^{[l]}\\
d a^{[l-1]}=w^{[l]} \cdot d z^{[l]}\\
d z^{[l]}=w^{[l+1] T} d z^{[l+1]} \cdot g^{[l]^{\prime}}\left(z^{[l]}\right)\end{array}</script><p>向量化</p>
<script type="math/tex; mode=display">
\begin{array}{l}{d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right)} \\ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\\
\begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \\ {d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]}}\end{array}\end{array}</script><p>summary</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_5.png" alt></p>
<h2><span id="c1w4l03-forward-propagation-in-d-deep-network">C1W4L03 : Forward Propagation in d deep network</span><a href="#c1w4l03-forward-propagation-in-d-deep-network" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_6.png" alt></p>
<p>这里只能用一个显式<strong>for</strong>循环，从1到，然后一层接着一层去计算。</p>
<h2><span id="c1w4l04-getting-matrix-dimension-right">C1W4L04 Getting matrix dimension right</span><a href="#c1w4l04-getting-matrix-dimension-right" class="header-anchor">#</a></h2><p>当实现深度神经网络的时候，其中一个常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。</p>
<p>$d_w^{[l]}$和$w^{[l]}$维度相同，$db^{[l]}$和$b^{[l]}$维度相同，且w和b向量化维度不变，但z,a以及x的维度会向量化后发生变化。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_8.png" alt></p>
<p>反向传播的维数检查</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_7.png" alt></p>
<p>在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。</p>
<h2><span id="c1w4l05-why-deep-representations">C1W4L05 Why deep representations?</span><a href="#c1w4l05-why-deep-representations" class="header-anchor">#</a></h2><p>神经网络不需要很大，但是得有深度，也就是隐藏层需要很多，</p>
<h3><span id="1-for-example-of-face-detector">1. for example of face detector</span><a href="#1-for-example-of-face-detector" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_9.png" alt></p>
<h2><span id="c1w4l06-building-blocks-of-a-deep-neural-network">C1W4L06 :Building blocks of a deep neural network</span><a href="#c1w4l06-building-blocks-of-a-deep-neural-network" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_10.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_11.png" alt></p>
<p>可以看得出，再反向传播的时候，需要用到$Z^{[L]},W^{[L]},b^{[L]}$,因此cash them</p>
<p>正向传播：$Z^{[1]},A^{[1]}…………$,反向传播：$dA^{[L]},dZ{[L]},dW^{[L]}dB^{[L]},dA^{[L-1]}$</p>
<h2><span id="c1w4l07-parameters-vs-hyperparameters">C1W4L07：Parameters vs Hyperparameters</span><a href="#c1w4l07-parameters-vs-hyperparameters" class="header-anchor">#</a></h2><h3><span id="1-what">1 What</span><a href="#1-what" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_12.png" alt></p>
<h3><span id="2-how">2 How</span><a href="#2-how" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_13.png" alt></p>
<p><strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代</p>
<p>今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到0.05会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率，再看看损失函数J的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的并观察损失函数这么变了，试试一组值，然后可能损失函数变成这样，这个值会加快学习过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个值了。</p>
<p>在前面几页中，还有很多不同的超参数。然而，当你开始开发新应用时，预先很难确切知道，究竟超参数的最优值应该是什么。所以通常，你必须尝试很多不同的值，并走这个循环，试试各种参数。试试看5个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。这页的标题是，应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>所以我经常建议人们，特别是刚开始应用于新问题的人们，去试一定范围的值看看结果如何。然后下一门课程，我们会用更系统的方法，用系统性的尝试各种超参数取值。然后其次，甚至是你已经用了很久的模型，可能你在做网络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变化，因为电脑的基础设施，<strong>CPU</strong>或是<strong>GPU</strong>可能会变化很大。所以有一条经验规律可能每几个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
<p>有一条经验规律：经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。</p>
<p>总结：超参数的设定，靠经验，尝试，并调，根据结果调，</p>
<h2><span id="c1w4l08-what-does-this-have-to-do-with-the-brain">C1W4L08 : What does this have to do with the brain?</span><a href="#c1w4l08-what-does-this-have-to-do-with-the-brain" class="header-anchor">#</a></h2><h1><span id="summary-forward-prop-and-back-prop"># summary : forward prop and back prop</span><a href="#summary-forward-prop-and-back-prop" class="header-anchor">#</a></h1><h2><span id="1-logistics-regression-shallow-neural-network-and-deep-neural-network">1. logistics regression,shallow neural network and deep neural network</span><a href="#1-logistics-regression-shallow-neural-network-and-deep-neural-network" class="header-anchor">#</a></h2><p>logistics regression</p>
<script type="math/tex; mode=display">
Z = W^TX+B\\
A = \frac{1}{1+e^{-Z}}\\
L(A,Y)=-\frac{1}{m}(Ylog^A+(1-Y)log^{1-A}\\
\frac{\partial L}{\partial Z}=(A-Y)\\
\frac{\partial L}{\partial W}=X(A-Y)\\</script><p>说明：X是样本按列堆积，W是列向量</p>
<p>shallow neural network 以二分问题为例</p>
<script type="math/tex; mode=display">
Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\
A^{[1]}=g^{[1]}(Z^{[1]})\\
\ \\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=g^{[2]}(Z^{[2]})\\
\ \ \\
\ \\
L(A^{[2]},Y)=-\frac{1}{m}(Ylog^{A}+(1-Y)log^{1-A})\\
\frac{\partial L}{\partial Z^{[2]}}=(A^{[2]}-Y)\\
\frac{\partial L}{\partial W^{[2]}}=(A^{[2]}-Y)A^{[1]^T}\\
\frac{\partial L}{\partial b^{[2]}}=(A^{[2]}-Y)1_{1*m}^T\\
\frac{\partial L}{\partial a^{[1]}}=W^{[2]^T}(A^{[2]}-Y)\\
\ \\
\frac{\partial L}{\partial Z^{[1]}}=W^{[2]^T}(A^{[2]}-Y)* g^{'[1]}(Z^{[1]})\\</script><p>说明：W是按列排$W^{[L]}$是$n^{[L]}*n^{[L-1]}$矩阵，A,Z是按列堆积，记得检查矩阵维数就好了</p>
<h2><span id="deep-neural-network">deep neural network</span><a href="#deep-neural-network" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\
A^{[l]}=g^{[l]}(Z^{[l]})\\
\ \ \\
\ \\
\frac{\partial L}{\partial Z^{[l]}}=\partial A^*g^{'[l]}(Z^{l})\\
\frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A^{[1-1]^T}\\
\frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\\
\frac{\partial L}{\partial a^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}\\
\ \\
\frac{\partial L}{\partial Z^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}* g^{'[l-1]}(Z^{[l-1]})\\</script><h2><span id="2-vectorization">2. vectorization</span><a href="#2-vectorization" class="header-anchor">#</a></h2><ol>
<li>推导的时候要向量化，注意矩阵维数表示，可以从单个推导到mutli</li>
<li>充分利用python的广播属性，和内置函数的并行化</li>
<li>python一维，二维数组的特性</li>
</ol>
<h2><span id="3-zhi-shi-jie-gou">3. 知识结构</span><a href="#3-zhi-shi-jie-gou" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/C1.png" alt></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>deep_learning.ai深度学习笔记&lt;Andrew Ng&gt;</title>
    <url>/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1><span id="c5-sequence-models">C5: Sequence Models</span><a href="#c5-sequence-models" class="header-anchor">#</a></h1><h2><span id="w1-recurrent-neural-networks-xun-huan-xu-lie-mo-xing">W1 : Recurrent Neural Networks (循环序列模型)</span><a href="#w1-recurrent-neural-networks-xun-huan-xu-lie-mo-xing" class="header-anchor">#</a></h2><h3><span id="l1-why-sequence-models">L1 ： Why Sequence Models?</span><a href="#l1-why-sequence-models" class="header-anchor">#</a></h3><p>循环神经网络（<strong>RNN</strong>）之类的模型在语音识别、自然语言处理和其他领域中引起变革。</p>
<p>序列模型的列子</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c11.png" alt></p>
<h3><span id="l2-notation-shu-xue-fu-hao">L2 : Notation 数学符号</span><a href="#l2-notation-shu-xue-fu-hao" class="header-anchor">#</a></h3><p>NLP</p>
<p>我们用$X^{(i)}$来表示第个i训练样本，所以为了指代第个t元素，或者说是训练样本i的序列中第t个元素用$X^{(i)}<t>$这个符号来表示。如果是序列长度$T_x$，那么你的训练集里不同的训练样本就会有不同的长度，所以$T_x^{(i)}$就代表第个训练样本的输入序列长度。同样$y^{(i)}<t>$代表第i个训练样本中第t个元素，$T_y^{(i)}$就是第i个训练样本的输出序列的长度。</t></t></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c12.png" alt></p>
<p>预先有一个词典</p>
<h3><span id="l3-recurrent-neural-network-model-xun-huan-shen-jing-wang-luo-mo-xing">L3 : Recurrent Neural Network Model (循环神经网络模型)</span><a href="#l3-recurrent-neural-network-model-xun-huan-shen-jing-wang-luo-mo-xing" class="header-anchor">#</a></h3><p>现在我们讨论一下怎样才能建立一个模型，建立一个神经网络来学习X到Y的映射</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c13.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c14.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c15.png" alt></p>
<p>$a^{&lt;0&gt;}$通常 是零向量<!--0--></p>
<p>N模型包含三类权重系数，分别是Wax，Waa，Wya。且不同元素之间同一位置共享同一权重系数。</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c16.png" alt></p>
<p>RNN的正向传播（Forward Propagation）过程为：</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c17.png" alt></p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出y，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是k类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出y，对于命名实体识别来说y只可能是0或者1，那我猜这里第二个激活函数g可以是<strong>sigmoid</strong>激活函数。</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c18.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c19.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c110.png" alt></p>
<h3><span id="c4-backpropagation-through-time-tong-guo-shi-jian-de-fan-xiang-chuan-bo">c4: Backpropagation through time ( 通过时间的反向传播)</span><a href="#c4-backpropagation-through-time-tong-guo-shi-jian-de-fan-xiang-chuan-bo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c111.png" alt></p>
<p>参数的关系<img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c112.png" alt>*</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c113.png" alt></p>
<p>单个元素的Loss function:</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c114.png" alt></p>
<p>该样本所有元素的Loss function为：</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c115.png" alt></p>
<p>然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y^,y)对参数Wa，Wy，ba，by的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。这种从右到左的求导过程被称为Backpropagation through time</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c116.png" alt></p>
<h3><span id="l5-different-types-of-rnns-bu-tong-lei-xing-de-xun-huan-shen-jing-wang-luo">L5: Different types of <strong>RNN</strong>s (不同类型的循环神经网络)</span><a href="#l5-different-types-of-rnns-bu-tong-lei-xing-de-xun-huan-shen-jing-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c117.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c118.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c119.png" alt></p>
<h3><span id="l6-language-model-and-sequence-generation-yu-yan-mo-xing-he-xu-lie-sheng-cheng">L6 : Language model and sequence generation (语言模型和序列生成)</span><a href="#l6-language-model-and-sequence-generation-yu-yan-mo-xing-he-xu-lie-sheng-cheng" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c120.png" alt></p>
<h3><span id="l7-sampling-novel-sequences-dui-xin-xu-lie-cai-yang">L7 : Sampling novel sequences (对新序列采样)</span><a href="#l7-sampling-novel-sequences-dui-xin-xu-lie-cai-yang" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c121.png" alt></p>
<h3><span id="vanishing-gradients-with-rnns-xun-huan-shen-jing-wang-luo-de-ti-du-xiao-shi">Vanishing gradients with <strong>RNN</strong>s (循环神经网络的梯度消失)</span><a href="#vanishing-gradients-with-rnns-xun-huan-shen-jing-wang-luo-de-ti-du-xiao-shi" class="header-anchor">#</a></h3><p>首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的<strong>was</strong>或者<strong>were</strong>。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的<strong>RNN</strong>模型会有很多局部影响</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson5-week1.html">http://www.ai-start.com/dl2017/html/lesson5-week1.html</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect</a></p>
]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>我的读书笔记</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>deeplearningvideo</title>
    <url>/2019/04/03/deeplearningvideo/</url>
    <content><![CDATA[<p><strong>Coursera深度学习教程中文笔记</strong></p>
<!-- more  -->
<p>课程概述</p>
<p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>这些课程专为已有一定基础（基本的编程知识，熟悉<strong>Python</strong>、对机器学习有基本了解），想要尝试进入人工智能领域的计算机专业人士准备。介绍显示：“深度学习是科技业最热门的技能之一，本课程将帮你掌握深度学习。”</p>
<p>在这5堂课中，学生将可以学习到深度学习的基础，学会构建神经网络，并用在包括吴恩达本人在内的多位业界顶尖专家指导下创建自己的机器学习项目。<strong>Deep Learning Specialization</strong>对卷积神经网络 (<strong>CNN</strong>)、递归神经网络 (<strong>RNN</strong>)、长短期记忆 (<strong>LSTM</strong>) 等深度学习常用的网络结构、工具和知识都有涉及。</p>
<p><strong>笔记是根据视频和字幕写的，没有技术含量，只需要专注和严谨。</strong></p>
<p>2018-04-14</p>
<p><strong>本课程视频教程地址：</strong><a href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>（该视频从www.deeplearning.ai 网站下载，因众所周知的原因，国内用户观看某些在线视频非常不容易，故一些学者一起制作了离线视频，旨在方便国内用户个人学习使用，请勿用于商业用途。视频内嵌中英文字幕，推荐使用<strong>potplayer</strong>播放。版权属于吴恩达老师所有，若在线视频流畅，请到官方网站观看。）</p>
<p><a href="http://www.ai-start.com/">笔记网站(适合手机阅读)</a></p>
<p>吴恩达老师的机器学习课程笔记和视频：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p>
<h1><span id="shen-du-xue-xi-bi-ji-mu-lu">深度学习笔记目录</span><a href="#shen-du-xue-xi-bi-ji-mu-lu" class="header-anchor">#</a></h1><h2><span id="di-yi-men-ke-shen-jing-wang-luo-he-shen-du-xue-xi-neural-networks-and-deep-learning">第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</span><a href="#di-yi-men-ke-shen-jing-wang-luo-he-shen-du-xue-xi-neural-networks-and-deep-learning" class="header-anchor">#</a></h2><p>第一周：深度学习引言(Introduction to Deep Learning)</p>
<p>1.1 欢迎(Welcome)</p>
<p>1.2 什么是神经网络？(What is a Neural Network)</p>
<p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p>
<p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p>
<p>1.5 关于本课程(About this Course)</p>
<p>1.6 课程资源(Course Resources)</p>
<p>1.7 Geoffery Hinton 专访(Geoffery Hinton interview)</p>
<p>第二周：神经网络的编程基础(Basics of Neural Network programming)</p>
<p>2.1 二分类(Binary Classification)</p>
<p>2.2 逻辑回归(Logistic Regression)</p>
<p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p>
<p>2.4 梯度下降（Gradient Descent）</p>
<p>2.5 导数（Derivatives）</p>
<p>2.6 更多的导数例子（More Derivative Examples）</p>
<p>2.7 计算图（Computation Graph）</p>
<p>2.8 计算图导数（Derivatives with a Computation Graph）</p>
<p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p>
<p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p>
<p>2.11 向量化(Vectorization)</p>
<p>2.12 更多的向量化例子（More Examples of Vectorization）</p>
<p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p>
<p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p>
<p>2.15 Python中的广播机制（Broadcasting in Python）</p>
<p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p>
<p>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</p>
<p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p>
<p>第三周：浅层神经网络(Shallow neural networks)</p>
<p>3.1 神经网络概述（Neural Network Overview）</p>
<p>3.2 神经网络的表示（Neural Network Representation）</p>
<p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p>
<p>3.4 多样本向量化（Vectorizing across multiple examples）</p>
<p>3.5 向量化实现的解释（Justification for vectorized implementation）</p>
<p>3.6 激活函数（Activation functions）</p>
<p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p>
<p>3.8 激活函数的导数（Derivatives of activation functions）</p>
<p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p>
<p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p>
<p>3.11 随机初始化（Random+Initialization）</p>
<p>第四周：深层神经网络(Deep Neural Networks)</p>
<p>4.1 深层神经网络（Deep L-layer neural network）</p>
<p>4.2 前向传播和反向传播（Forward and backward propagation）</p>
<p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p>
<p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p>
<p>4.5 为什么使用深层表示？（Why deep representations?）</p>
<p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p>
<p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p>
<p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p>
<h2><span id="di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization">第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</span><a href="#di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization" class="header-anchor">#</a></h2><p>第一周：深度学习的实用层面(Practical aspects of Deep Learning)</p>
<p>1.1 训练，验证，测试集（Train / Dev / Test sets）</p>
<p>1.2 偏差，方差（Bias /Variance）</p>
<p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p>
<p>1.4 正则化（Regularization）</p>
<p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p>
<p>1.6 dropout 正则化（Dropout Regularization）</p>
<p>1.7 理解 dropout（Understanding Dropout）</p>
<p>1.8 其他正则化方法（Other regularization methods）</p>
<p>1.9 标准化输入（Normalizing inputs）</p>
<p>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</p>
<p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</p>
<p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p>
<p>1.13 梯度检验（Gradient checking）</p>
<p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p>
<p>第二周：优化算法 (Optimization algorithms)</p>
<p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p>
<p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p>
<p>2.3 指数加权平均（Exponentially weighted averages）</p>
<p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p>
<p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p>
<p>2.6 momentum梯度下降（Gradient descent with momentum）</p>
<p>2.7 RMSprop——root mean square prop（RMSprop）</p>
<p>2.8 Adam优化算法（Adam optimization algorithm）</p>
<p>2.9 学习率衰减（Learning rate decay）</p>
<p>2.10 局部最优问题（The problem of local optima）</p>
<p>第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</p>
<p>3.1 调试处理（Tuning process）</p>
<p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p>
<p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p>
<p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p>
<p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p>
<p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p>
<p>3.7 测试时的Batch Norm（Batch Norm at test time）</p>
<p>3.8 Softmax 回归（Softmax Regression）</p>
<p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p>
<p>3.10 深度学习框架（Deep learning frameworks）</p>
<p>3.11 TensorFlow（TensorFlow）</p>
<h2><span id="di-san-men-ke-jie-gou-hua-ji-qi-xue-xi-xiang-mu-structuring-machine-learning-projects">第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)</span><a href="#di-san-men-ke-jie-gou-hua-ji-qi-xue-xi-xiang-mu-structuring-machine-learning-projects" class="header-anchor">#</a></h2><p>第一周：机器学习策略（1）(ML Strategy (1))</p>
<p>1.1 为什么是ML策略？ (Why ML Strategy)</p>
<p>1.2 正交化(Orthogonalization)</p>
<p>1.3 单一数字评估指标(Single number evaluation metric)</p>
<p>1.4 满足和优化指标 (Satisficing and Optimizing metric)</p>
<p>1.5 训练集、开发集、测试集的划分(Train/dev/test distributions)</p>
<p>1.6 开发集和测试集的大小 (Size of the dev and test sets)</p>
<p>1.7 什么时候改变开发集/测试集和评估指标(When to change dev/test sets and metrics)</p>
<p>1.8 为什么是人的表现 (Why human-level performance?)</p>
<p>1.9 可避免偏差(Avoidable bias)</p>
<p>1.10 理解人类的表现 (Understanding human-level performance)</p>
<p>1.11 超过人类的表现(Surpassing human-level performance)</p>
<p>1.12 改善你的模型表现 (Improving your model performance)</p>
<p>第二周：机器学习策略（2）(ML Strategy (2))</p>
<p>2.1 误差分析 (Carrying out error analysis)</p>
<p>2.2 清除标注错误的数据(Cleaning up incorrectly labeled data)</p>
<p>2.3 快速搭建你的第一个系统，并进行迭代(Build your first system quickly, then iterate)</p>
<p>2.4 在不同的分布上的训练集和测试集 (Training and testing on different distributions)</p>
<p>2.5 数据分布不匹配的偏差与方差分析 (Bias and Variance with mismatched data distributions)</p>
<p>2.6 处理数据不匹配问题(Addressing data mismatch)</p>
<p>2.7 迁移学习 (Transfer learning)</p>
<p>2.8 多任务学习(Multi-task learning)</p>
<p>2.9 什么是端到端的深度学习？ (What is end-to-end deep learning?)</p>
<p>2.10 是否使用端到端的深度学习方法 (Whether to use end-to-end deep learning)</p>
<h2><span id="di-si-men-ke-juan-ji-shen-jing-wang-luo-convolutional-neural-networks">第四门课 卷积神经网络（Convolutional Neural Networks）</span><a href="#di-si-men-ke-juan-ji-shen-jing-wang-luo-convolutional-neural-networks" class="header-anchor">#</a></h2><p>第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</p>
<p>1.1    计算机视觉（Computer vision）</p>
<p>1.2    边缘检测示例（Edge detection example）</p>
<p>1.3    更多边缘检测内容（More edge detection）</p>
<p>1.4    Padding</p>
<p>1.5    卷积步长（Strided convolutions）</p>
<p>1.6    三维卷积（Convolutions over volumes）</p>
<p>1.7    单层卷积网络（One layer of a convolutional network）</p>
<p>1.8    简单卷积网络示例（A simple convolution network example）</p>
<p>1.9    池化层（Pooling layers）</p>
<p>1.10 卷积神经网络示例（Convolutional neural network example）</p>
<p>1.11 为什么使用卷积？（Why convolutions?）</p>
<p>第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</p>
<p>2.1 为什么要进行实例探究？（Why look at case studies?）</p>
<p>2.2 经典网络（Classic networks）</p>
<p>2.3 残差网络（Residual Networks (ResNets)）</p>
<p>2.4 残差网络为什么有用？（Why ResNets work?）</p>
<p>2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）</p>
<p>2.6 谷歌 Inception 网络简介（Inception network motivation）</p>
<p>2.7 Inception 网络（Inception network）</p>
<p>2.8 使用开源的实现方案（Using open-source implementations）</p>
<p>2.9 迁移学习（Transfer Learning）</p>
<p>2.10 数据扩充（Data augmentation）</p>
<p>2.11 计算机视觉现状（The state of computer vision）</p>
<p>第三周 目标检测（Object detection）</p>
<p>3.1 目标定位（Object localization）</p>
<p>3.2 特征点检测（Landmark detection）</p>
<p>3.3 目标检测（Object detection）</p>
<p>3.4 卷积的滑动窗口实现（Convolutional implementation of sliding windows）</p>
<p>3.5 Bounding Box预测（Bounding box predictions）</p>
<p>3.6 交并比（Intersection over union）</p>
<p>3.7 非极大值抑制（Non-max suppression）</p>
<p>3.8 Anchor Boxes</p>
<p>3.9 YOLO 算法（Putting it together: YOLO algorithm）</p>
<p>3.10 候选区域（选修）（Region proposals (Optional)）</p>
<p>第四周 特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）</p>
<p>4.1 什么是人脸识别？(What is face recognition?)</p>
<p>4.2 One-Shot学习（One-shot learning）</p>
<p>4.3 Siamese 网络（Siamese network）</p>
<p>4.4 Triplet 损失（Triplet 损失）</p>
<p>4.5 面部验证与二分类（Face verification and binary classification）</p>
<p>4.6 什么是神经风格转换？（What is neural style transfer?）</p>
<p>4.7 什么是深度卷积网络？（What are deep ConvNets learning?）</p>
<p>4.8 代价函数（Cost function）</p>
<p>4.9 内容代价函数（Content cost function）</p>
<p>4.10 风格代价函数（Style cost function）</p>
<p>4.11 一维到三维推广（1D and 3D generalizations of models）</p>
<h1><span id="di-wu-men-ke-xu-lie-mo-xing-sequence-models">第五门课 序列模型(Sequence Models)</span><a href="#di-wu-men-ke-xu-lie-mo-xing-sequence-models" class="header-anchor">#</a></h1><p>第一周 循环序列模型（Recurrent Neural Networks） 1.1 为什么选择序列模型？（Why Sequence Models?）</p>
<p>1.2 数学符号（Notation）</p>
<p>1.3 循环神经网络模型（Recurrent Neural Network Model）</p>
<p>1.4 通过时间的反向传播（Backpropagation through time）</p>
<p>1.5 不同类型的循环神经网络（Different types of RNNs）</p>
<p>1.6 语言模型和序列生成（Language model and sequence generation）</p>
<p>1.7 对新序列采样（Sampling novel sequences）</p>
<p>1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）</p>
<p>1.9 GRU单元（Gated Recurrent Unit（GRU））</p>
<p>1.10 长短期记忆（LSTM（long short term memory）unit）</p>
<p>1.11 双向循环神经网络（Bidirectional RNN）</p>
<p>1.12 深层循环神经网络（Deep RNNs）</p>
<p>第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）</p>
<p>2.1 词汇表征（Word Representation）</p>
<p>2.2 使用词嵌入（Using Word Embeddings）</p>
<p>2.3 词嵌入的特性（Properties of Word Embeddings）</p>
<p>2.4 嵌入矩阵（Embedding Matrix）</p>
<p>2.5 学习词嵌入（Learning Word Embeddings）</p>
<p>2.6 Word2Vec</p>
<p>2.7 负采样（Negative Sampling）</p>
<p>2.8 GloVe 词向量（GloVe Word Vectors）</p>
<p>2.9 情绪分类（Sentiment Classification）</p>
<p>2.10 词嵌入除偏（Debiasing Word Embeddings）</p>
<p>第三周 序列模型和注意力机制（Sequence models &amp; Attention mechanism）</p>
<p>3.1 基础模型（Basic Models）</p>
<p>3.2 选择最可能的句子（Picking the most likely sentence）</p>
<p>3.3 集束搜索（Beam Search）</p>
<p>3.4 改进集束搜索（Refinements to Beam Search）</p>
<p>3.5 集束搜索的误差分析（Error analysis in beam search）</p>
<p>3.6 Bleu 得分（选修）（Bleu Score (optional)）</p>
<p>3.7 注意力模型直观理解（Attention Model Intuition）</p>
<p>3.8注意力模型（Attention Model）</p>
<p>3.9语音识别（Speech recognition）</p>
<p>3.10触发字检测（Trigger Word Detection）</p>
<p>3.11结论和致谢（Conclusion and thank you）</p>
<p>人工智能大师访谈</p>
<p>吴恩达采访 Geoffery Hinton</p>
<p>吴恩达采访 Ian Goodfellow</p>
<p>吴恩达采访 Ruslan Salakhutdinov</p>
<p>吴恩达采访 Yoshua Bengio</p>
<p>吴恩达采访 林元庆</p>
<p>吴恩达采访 Pieter Abbeel</p>
<p>吴恩达采访 Andrej Karpathy</p>
<p>附件</p>
<p>深度学习符号指南（原课程翻译）</p>
]]></content>
      <categories>
        <category>视频学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯分类器</title>
    <url>/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1><span id="gai-lu-lun-de-zhi-shi">概率论的知识</span><a href="#gai-lu-lun-de-zhi-shi" class="header-anchor">#</a></h1><a id="more"></a>
<h2><span id="tiao-jian-gai-lu">条件概率</span><a href="#tiao-jian-gai-lu" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
P(A|B)=P(A\cap B)/P(B)</script><p>已知B发生的概率，求A发生的概率</p>
<h2><span id="quan-gai-lu">全概率</span><a href="#quan-gai-lu" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
P(B) = \sum_{i=1}^{N}P(B \cap A_i)P(A_i)</script><h2><span id="bei-xie-si-tui-duan">贝叶斯推断</span><a href="#bei-xie-si-tui-duan" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
P(A|B)=P(A)\frac{P(B|A)}{P(B)}</script><script type="math/tex; mode=display">
P(A_i|B)=P(A_i)\frac{P(B|A_i)}{\sum P(A_i)P(B|A_i)}</script><p>$P(A)$：Prior probability 先验概率，在B事件发生之前，对A事件做一个判断</p>
<p>$P(A|B)$:Posterior probability 后验概率，在B事件发生之后，对A事件的概率重新评估</p>
<p>$P(B|A)/P(B)$:称为可能性函数，一个调整因子</p>
<p>后验概率=先验概率*调整因子 （可知，调整因此&gt;1,发生概率增大了，</p>
<h1><span id="bei-xie-si-jue-ce-lun">贝叶斯决策论</span><a href="#bei-xie-si-jue-ce-lun" class="header-anchor">#</a></h1><p>英文：Bayesian decision theory</p>
<p>设有$N$种可能的类别, 即γ=${c_1,c_2,…,c_N}$. $λ_ij$是将一个真实类别为$c_j$的样本判为$c_x$的损失。 基于后验概率可得将样本分类所产生的期望损失, 或者成为条件风险(Conditional Risk) </p>
<script type="math/tex; mode=display">
R(C_i|x)=∑_{j=1}^Nλ_{ij}P(c_j|x)</script><p>于是， 我们的任务就是寻找判定准则h， 令$χ→γ$ 使得最小化总体风险，$R(h)=E_x[R(h(x)|x]$最小. 对于每一个$x$，若$h$都能最小化条件风险，那么总体也被最小化了。 可以简化为对每个样本选择其条件风险最小的分类, 即: </p>
<script type="math/tex; mode=display">
h(x)=arg \min_{c⊂λ}R(c|x)</script><p>此$h(x)$就是贝叶斯最优分类器。 $R(h)$为贝叶斯风险(Bayes Risk), $1−R(h)$反映了分类器的最优性能. </p>
<p>具体来说，如果目标是最小化分类错误率，</p>
<script type="math/tex; mode=display">\lambda_{ij}=\begin{cases} 0\ \  i==j\\1 \ \ \ i!=j \end{cases}</script><p>则$R(c|x)=1-p(c|x)$，因此可知，$h(x)=\max_{c\in C} p(c|x)$</p>
<p>对于样本$x$,选择后验概率$P(c|X)$最大的类别为标记。</p>
<p>问题转换为</p>
<script type="math/tex; mode=display">
P(c_i|x)=\frac{P(c_i)P(x|c_i)}{\sum P(x)}</script><p>求先验概率和似然($P(x|c)$)</p>
<p>其中</p>
<p>$P(c)$表达了样本空间种各类样本所占的比列，根据大数定律，当样本足够充分的独立同分布样本是，可以频率估计</p>
<p>$P(x|c)$,涉及关于x所以属性的联合概率，用频率估计概率可能不太好，对于估计类条件概率的一种宠用策略是先假设具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。</p>
<p>$P(x|c)$是类条件概率，由某个分布决定，$P(x|\theta_c)$来表示了</p>
<p>频率注意派认为可以通过优化似然函数估计参数。$D_c$类别c的样本集合，独立同分布</p>
<script type="math/tex; mode=display">
P(D_c|\theta_c)=\Pi_{x \in D_c}P(x|\theta_c)</script><script type="math/tex; mode=display">
LL(\theta_c)=log  P(D_c|\theta_c)</script><h1><span id="po-su-bei-xie-si-fen-lei-qi">朴素贝叶斯分类器</span><a href="#po-su-bei-xie-si-fen-lei-qi" class="header-anchor">#</a></h1><p>英文：naive Bayes classifier</p>
<p>假设：属性条件独立性假设，每个属性独立性对分类结果发生影响</p>
<script type="math/tex; mode=display">
P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)\Pi_{i=1}^{d}P(x_i|c)}{P(x)}</script><p>对于一个$x$，$P(x)$都是相同的，因此贝叶斯模型可写为</p>
<script type="math/tex; mode=display">
h_{nb}(x)=arg max_{c\in y}P(c)\Pi_{i=1}^{d}P(x_i|c)</script><h2><span id="ji-suan-guo-cheng">计算过程</span><a href="#ji-suan-guo-cheng" class="header-anchor">#</a></h2><p>假设$D_{c_i}$表示第i类的样本集合，</p>
<ol>
<li><p>$P(c_i)=\frac{|D_{c_i}|}{|D|}$</p>
</li>
<li><p>如果是离散属性</p>
</li>
</ol>
<script type="math/tex; mode=display">
P(x_i|c_i)=\frac{|D_{c,x_i}|}{|D_{c_i}|}</script><p>如果是连续属性，$P(x_i|c_i)$服从$N(u_{c,i},\theta_{c,i}^2)$的分布</p>
<script type="math/tex; mode=display">
P(x_i|c)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2})</script><ol>
<li>$P(c_i)\Pi_{i=1}^{N}P(x_i|c_i)$</li>
</ol>
<p>注意为了避免其他属性携带的信息被训练集中未出现的属性值抹去，因此用拉普拉斯修正（Laplacian correction)</p>
<script type="math/tex; mode=display">
P(c)=\frac{|D_{c_i}|+1}{|D|+N}\\
P(x_i|c)=\frac{|D_{x_i,c}|+1}{|D_c|+N_i}</script><p>$N$:训练集可能出现的类别数</p>
<p>$N_i$:第i个属性可能的取值数</p>
<p>显然，拉普拉斯修正避免因训练集不充分导出的概率估值为0的情况</p>
<h1><span id="po-su-bei-xie-si-de-chong-lei">朴素贝叶斯的种类</span><a href="#po-su-bei-xie-si-de-chong-lei" class="header-anchor">#</a></h1><p>再scikit-learn中，一共有三个朴素贝叶斯，分别是</p>
<h2><span id="gaussiannb">GaussianNB</span><a href="#gaussiannb" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
P(x_i|C_i)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2})</script><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#导入包</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris=datasets.load_iris()</span><br><span class="line"><span class="comment">#切分数据集</span></span><br><span class="line">Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data,</span><br><span class="line">                                                iris.target,</span><br><span class="line">                                                random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">clf = GaussianNB()</span><br><span class="line">clf.fit(Xtrain, ytrain)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在测试集上执行预测，proba导出的是每个样本属于某类的概率</span></span><br><span class="line">clf.predict(Xtest)</span><br><span class="line">clf.predict_proba(Xtest) <span class="comment">#每一类计算结果都输出</span></span><br><span class="line"><span class="comment">#测试准确率</span></span><br><span class="line">accuracy_score(ytest, clf.predict(Xtest))</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">dataSet =pd.read_csv(<span class="string">'iris.txt'</span>,header = <span class="literal">None</span>)</span><br><span class="line">dataSet.head()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randSplit</span>(<span class="params">dataSet, rate</span>):</span></span><br><span class="line">    l = list(dataSet.index) <span class="comment">#提取出索引</span></span><br><span class="line">    random.shuffle(l) <span class="comment">#随机打乱索引</span></span><br><span class="line">    dataSet.index = l <span class="comment">#将打乱后的索引重新赋值给原数据集</span></span><br><span class="line">    n = dataSet.shape[<span class="number">0</span>] <span class="comment">#总行数</span></span><br><span class="line">    m = int(n * rate) <span class="comment">#训练集的数量</span></span><br><span class="line">    train = dataSet.loc[range(m), :] <span class="comment">#提取前m个记录作为训练集</span></span><br><span class="line">    test = dataSet.loc[range(m, n), :] <span class="comment">#剩下的作为测试集</span></span><br><span class="line">    dataSet.index = range(dataSet.shape[<span class="number">0</span>]) <span class="comment">#更新原数据集的索引</span></span><br><span class="line">    test.index = range(test.shape[<span class="number">0</span>]) <span class="comment">#更新测试集的索引</span></span><br><span class="line"></span><br><span class="line">train,test=randSplit(dataSet, <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gnb_classify</span>(<span class="params">train,test</span>):</span></span><br><span class="line">    labels = train.iloc[:,<span class="number">-1</span>].value_counts().index <span class="comment">#提取训练集的标签种类</span></span><br><span class="line">    mean =[] <span class="comment">#存放每个类别的均值</span></span><br><span class="line">    std =[] <span class="comment">#存放每个类别的方差</span></span><br><span class="line">    result = [] <span class="comment">#存放测试集的预测结果</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> labels:</span><br><span class="line">        item = train.loc[train.iloc[:,<span class="number">-1</span>]==i,:] <span class="comment">#分别提取出每一种类别</span></span><br><span class="line">        m = item.iloc[:,:<span class="number">-1</span>].mean() <span class="comment">#当前类别的平均值</span></span><br><span class="line">        s = np.sum((item.iloc[:,:<span class="number">-1</span>]-m)**<span class="number">2</span>)/(item.shape[<span class="number">0</span>]) <span class="comment">#当前类别的方差</span></span><br><span class="line">        mean.append(m) <span class="comment">#将当前类别的平均值追加至列表</span></span><br><span class="line">        std.append(s) <span class="comment">#将当前类别的方差追加至列表</span></span><br><span class="line">    means = pd.DataFrame(mean,index=labels) <span class="comment">#变成DF格式，索引为类标签</span></span><br><span class="line">    stds = pd.DataFrame(std,index=labels) <span class="comment">#变成DF格式，索引为类标签</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(test.shape[<span class="number">0</span>]):</span><br><span class="line">        iset = test.iloc[j,:<span class="number">-1</span>].tolist() <span class="comment">#当前测试实例</span></span><br><span class="line">        iprob = np.exp(<span class="number">-1</span>*(iset-means)**<span class="number">2</span>/(stds*<span class="number">2</span>))/(np.sqrt(<span class="number">2</span>*np.pi*stds)) <span class="comment">#正态分布公式</span></span><br><span class="line">        prob = train.iloc[:,<span class="number">-1</span>].value_counts()/len(train.iloc[:,<span class="number">-1</span>]) <span class="comment">#初始化当前实例总概率</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(test.shape[<span class="number">1</span>]<span class="number">-1</span>): <span class="comment">#遍历每个特征</span></span><br><span class="line">            prob *= iprob[k] <span class="comment">#特征概率之积即为当前实例概率</span></span><br><span class="line">        cla = prob.index[np.argmax(prob.values)] <span class="comment">#返回最大概率的类别</span></span><br><span class="line">        result.append(cla)</span><br><span class="line">    test[<span class="string">'predict'</span>]=result</span><br><span class="line">    acc = (test.iloc[:,<span class="number">-1</span>]==test.iloc[:,<span class="number">-2</span>]).mean() <span class="comment">#计算预测准确率</span></span><br><span class="line">    print(<span class="string">f'模型预测准确率为<span class="subst">{acc}</span>'</span>)</span><br><span class="line">    <span class="keyword">return</span> test</span><br><span class="line"></span><br><span class="line">gnb_classify(train,test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    train,test= randSplit(dataSet, <span class="number">0.8</span>)</span><br><span class="line">    gnb_classify(train,test)</span><br><span class="line">   </span><br></pre></td></tr></tbody></table></figure>
<h2><span id="multinomialnb">MultinomialNB</span><a href="#multinomialnb" class="header-anchor">#</a></h2><p>先验概率多项式分布的朴素贝叶斯，假设特征是由一共简单多项式分布生成，多项分布可以描述各种类型样本出现的频率，该模型常用于文本分类，特别表示次数。$\lambda$常取值1</p>
<script type="math/tex; mode=display">
P(x_{il}|c)=\frac{x_{il}+\lambda}{m_k+n\lambda}</script><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    dataSet=[[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</span><br><span class="line">             [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">             [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">             [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">             [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">             [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]] <span class="comment">#切分好的词条</span></span><br><span class="line">    classVec = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>] <span class="comment">#类别标签向量，1代表侮辱性词汇，0代表非侮辱性词汇</span></span><br><span class="line">    <span class="keyword">return</span> dataSet,classVec</span><br><span class="line">dataSet,classVec = loadDataSet()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    vocabSet = set() <span class="comment">#创建一个空的集合</span></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> dataSet: <span class="comment">#遍历dataSet中的每一条言论</span></span><br><span class="line">        vocabSet = vocabSet | set(doc) <span class="comment">#取并集</span></span><br><span class="line">        vocabList = list(vocabSet)</span><br><span class="line">    <span class="keyword">return</span> vocabList</span><br><span class="line"></span><br><span class="line">vocabList = createVocabList(dataSet)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span>(<span class="params">vocabList, inputSet</span>):</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList) <span class="comment">#创建一个其中所含元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet: <span class="comment">#遍历每个词条</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList: <span class="comment">#如果词条存在于词汇表中，则变为1</span></span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">f" <span class="subst">{word}</span> is not in my Vocabulary!"</span> )</span><br><span class="line">    <span class="keyword">return</span> returnVec <span class="comment">#返回文档向量</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_trainMat</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    trainMat = [] <span class="comment">#初始化向量列表</span></span><br><span class="line">    vocabList = createVocabList(dataSet) <span class="comment">#生成词汇表</span></span><br><span class="line">    <span class="keyword">for</span> inputSet <span class="keyword">in</span> dataSet: <span class="comment">#遍历样本词条中的每一条样本</span></span><br><span class="line">        returnVec=setOfWords2Vec(vocabList, inputSet) <span class="comment">#将当前词条向量化</span></span><br><span class="line">        trainMat.append(returnVec) <span class="comment">#追加到向量列表中</span></span><br><span class="line">    <span class="keyword">return</span> trainMat</span><br><span class="line">trainMat = get_trainMat(dataSet)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB</span>(<span class="params">trainMat,classVec</span>):</span></span><br><span class="line">    n = len(trainMat) <span class="comment">#计算训练的文档数目</span></span><br><span class="line">    m = len(trainMat[<span class="number">0</span>]) <span class="comment">#计算每篇文档的词条数</span></span><br><span class="line">    pAb = sum(classVec)/n <span class="comment">#文档属于侮辱类的概率</span></span><br><span class="line">    p0Num = np.zeros(m) <span class="comment">#词条出现数初始化为0</span></span><br><span class="line">    p1Num = np.zeros(m) <span class="comment">#词条出现数初始化为0</span></span><br><span class="line">    p0Denom = <span class="number">0</span> <span class="comment">#分母初始化为0</span></span><br><span class="line">    p1Denom = <span class="number">0</span> <span class="comment">#分母初始化为0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n): <span class="comment">#遍历每一个文档</span></span><br><span class="line">        <span class="keyword">if</span> classVec[i] == <span class="number">1</span>: <span class="comment">#统计属于侮辱类的条件概率所需的数据</span></span><br><span class="line">            p1Num += trainMat[i]</span><br><span class="line">            p1Denom += sum(trainMat[i])</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#统计属于非侮辱类的条件概率所需的数据</span></span><br><span class="line">            p0Num += trainMat[i]</span><br><span class="line">            p0Denom += sum(trainMat[i])</span><br><span class="line">    p1V = p1Num/p1Denom</span><br><span class="line">    p0V = p0Num/p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0V,p1V,pAb <span class="comment">#返回属于非侮辱类,侮辱类和文档属于侮辱类的概率</span></span><br><span class="line"></span><br><span class="line">p0V,p1V,pAb=trainNB(trainMat,classVec)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span>(<span class="params">vec2Classify, p0V, p1V, pAb</span>):</span></span><br><span class="line">    p1 = reduce(<span class="keyword">lambda</span> x,y:x*y, vec2Classify * p1V) * pAb   <span class="comment">#对应元素相乘</span></span><br><span class="line">    p0 = reduce(<span class="keyword">lambda</span> x,y:x*y, vec2Classify * p0V) * (<span class="number">1</span> - pAb)</span><br><span class="line">    print(<span class="string">'p0:'</span>,p0)</span><br><span class="line">    print(<span class="string">'p1:'</span>,p1)</span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span>(<span class="params">testVec</span>):</span></span><br><span class="line">    dataSet,classVec = loadDataSet() <span class="comment">#创建实验样本</span></span><br><span class="line">    vocabList = createVocabList(dataSet) <span class="comment">#创建词汇表</span></span><br><span class="line">    trainMat= get_trainMat(dataSet) <span class="comment">#将实验样本向量化</span></span><br><span class="line">    p0V,p1V,pAb = trainNB(trainMat,classVec) <span class="comment">#训练朴素贝叶斯分类器</span></span><br><span class="line">    thisone = setOfWords2Vec(vocabList, testVec) <span class="comment">#测试样本向量化</span></span><br><span class="line">    <span class="keyword">if</span> classifyNB(thisone,p0V,p1V,pAb):</span><br><span class="line">        print(testVec,<span class="string">'属于侮辱类'</span>) <span class="comment">#执行分类并打印分类结果</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(testVec,<span class="string">'属于非侮辱类'</span>) <span class="comment">#执行分类并打印分类结果</span></span><br><span class="line">        </span><br><span class="line">testVec1 = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">testingNB(testVec1)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="bernoullinb">BernoulliNB</span><a href="#bernoullinb" class="header-anchor">#</a></h2><p>伯努利分布，如果是二元伯努利分布</p>
<script type="math/tex; mode=display">
P(x_{il}|C_i)=P(i|Y=C_i)x_{il}+(1-P(i|Y=C_i))(1-x_{il})</script><p>如果样本属性大多数属于连续，GaussionNB</p>
<p>如果是离散值，使用MultinomialNB</p>
<p>如果样本特征是二元离散值或者稀疏离散值，BernoulliNB</p>
<h1><span id="ban-po-su-bei-xie-si">半朴素贝叶斯</span><a href="#ban-po-su-bei-xie-si" class="header-anchor">#</a></h1><h2><span id="xin-xi-liang-shang-lian-he-shang-tiao-jian-shang-hu-xin-xi">信息量、熵、联合熵、条件熵、互信息</span><a href="#xin-xi-liang-shang-lian-he-shang-tiao-jian-shang-hu-xin-xi" class="header-anchor">#</a></h2><h3><span id="xin-xi-liang">信息量</span><a href="#xin-xi-liang" class="header-anchor">#</a></h3><p>反应了随机变量取某个值含的可能性大小，或者是含有的信息多少</p>
<script type="math/tex; mode=display">
I(X=x)=-log_2^{p(x）}</script><h3><span id="shang-entropy">熵(entropy)</span><a href="#shang-entropy" class="header-anchor">#</a></h3><p>反应了信源平均每个符号的信息量,或者是随机变量不确定性的衡量</p>
<script type="math/tex; mode=display">
H(X)=E(I(X))=\sum p(X=x)(-log_2^{p(x)})</script><h2><span id="lian-he-shang">联合熵</span><a href="#lian-he-shang" class="header-anchor">#</a></h2><p>反应了多个随机变量的平均信息量</p>
<script type="math/tex; mode=display">
H(X,Y)=\sum p(x,y)(-log_2^{p(x,y)})</script><h3><span id="tiao-jian-shang-conditional-entropy">条件熵（Conditional entropy）</span><a href="#tiao-jian-shang-conditional-entropy" class="header-anchor">#</a></h3><p>反应了已知一个随机变量下，另一个随机变量的不确定性</p>
<script type="math/tex; mode=display">
H(X|Y)=-\sum p(y)H(X|Y=y)=-\sum p(x,y)log_2^{p(x|y)}</script><h3><span id="hu-xin-xi-mutual-information">互信息(mutual information)</span><a href="#hu-xin-xi-mutual-information" class="header-anchor">#</a></h3><p>反应了已知一个随机变量的情况下，另外一个随机变量不确定性减少了多少,可以把互信息看成由于知道 y 值而造成的 x 的不确定性的减小</p>
<script type="math/tex; mode=display">
I(X;Y)=\sum \sum p(x,y)log(\frac{p(x,y)}{p(x)p(y)})\\
=H(X)-H(X|Y)=H(Y)-H(Y|X)</script><p>如果两个随机变量独立，则互信息为0,因此，互信息可以衡量两个随机变量的相关程度</p>
<h2><span id="tiao-jian-hu-xin-xi">条件互信息</span><a href="#tiao-jian-hu-xin-xi" class="header-anchor">#</a></h2><p>在条件z发生时的条件互信息</p>
<script type="math/tex; mode=display">
I(X;Y|Z) = \sum\sum p(x,y|z)log_2^{\frac{p(x,y|z)}{p(x|z)p(y|z)}}</script><p><img src="/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\hexo\source\_posts\贝叶斯分类器\熵.png" alt></p>
<h2><span id="ban-po-su-bei-xie-si">半朴素贝叶斯</span><a href="#ban-po-su-bei-xie-si" class="header-anchor">#</a></h2><p>适当的考虑一部分属性间的相互依赖关系，这个关系可以用互信息描述</p>
<h3><span id="du-yi-lai">独依赖</span><a href="#du-yi-lai" class="header-anchor">#</a></h3><p>假设每个属性只有一个其他 的属性.则计算公式改下如下</p>
<script type="math/tex; mode=display">
p(C)\Pi_{i=1}^{d} P(x_i|C_i,pa_i)</script><p>$pa_i$是属性$x_i$所依赖的属性，被称为$x_i$的父属性</p>
<p>1)  <strong>SPODE </strong>最简单的方法是：都选一个属性作为父属性</p>
<p>可以通过交叉验证的方法</p>
<p>2)  TAN :最大带权生成树</p>
<p>权重：当y划分为$c_k$类时条件熵</p>
<script type="math/tex; mode=display">
I(x_i;y_i|y)=\sum_{x_i,y_i,c_k}p(x_i,y_j|c_k)log^{\frac{p(x_i;y_j|c_k)}{p(x_i|c_k)p(y_i|c_k)}}</script><p>step 1: 计算任意两个属性之间条件互信息</p>
<script type="math/tex; mode=display">
I(X;Y|Y)=\sum_{i}I(X;Y|c_i)</script><p>step 2: 以属性为结点构建完全图</p>
<p>step 3: 最大带权生成树，挑选根变量</p>
<p>step 4: 加入类别结点y,增加到每个属性的有向边</p>
<p>条件互信息反应了属性在已知类别下的相关性大小</p>
<p><img src="/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\hexo\source\_posts\贝叶斯分类器\2.png" alt></p>
<h2><span id="ji-cheng-xue-xi">集成学习</span><a href="#ji-cheng-xue-xi" class="header-anchor">#</a></h2><p>AODE选择模型尝试将每个属性作为超父构建SPODE</p>
<script type="math/tex; mode=display">
P(c_i|X)正比于 \sum_{i=1,|D_{x_i}>=m}p(c,x_i)\Pi_{j=1}^{d}p(x_j|c_i,x_i)</script><p>$m$通常取30,</p>
<script type="math/tex; mode=display">
P(c,x_i)=\frac{|D_{c,x_i}|+1}{|{D}|+N*N_i}\\
P(x_j|c,x_i)=\frac{|D_{c,x_i,x_j}+1|}{|D_{c,xi}|+N_j}</script><h1><span id="bei-xie-si-wang-bayesian-network">贝叶斯网(Bayesian network)</span><a href="#bei-xie-si-wang-bayesian-network" class="header-anchor">#</a></h1><p>借助有向无环图来刻画属性之间的依赖关系，条件概率表来描述属性的联合概率分布。</p>
<p>一个贝叶斯网络$B$,包括结构$G$和参数$\Theta$ ,$B(G,\Theta)$,如果两个属性有直接依赖关系，用边连接，对于属性$x_i$,其父节点集合$G_i$,则$\Theta$包括每个属性条件概率$\Theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$</p>
<h2><span id="jie-gou">结构</span><a href="#jie-gou" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
p(x_1,x_2,...,x_n)=\Pi_{i=1}^{n}p_{B}(x_i|\pi_i)=\Pi_{i=1}^{d}\Theta_{xi|\pi_i}\\
=\Pi_{i=1}^{d}P(x_i|Parents(x_i))</script><h2><span id="tui-duan">推断</span><a href="#tui-duan" class="header-anchor">#</a></h2><p>一旦训练好贝叶斯网后，就能回答query,通过一些属性的观测者来推断其他属性变量的取值，其中，已知变量的值观测推测待查询的过程“推断”,已知变量的观测者”证据“</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯分类器</tag>
      </tags>
  </entry>
  <entry>
    <title>二次规划</title>
    <url>/2019/03/25/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1><span id="kkt-karush-kuhn-tucher-tiao-jian">KKT(Karush-Kuhn-Tucher)条件</span><a href="#kkt-karush-kuhn-tucher-tiao-jian" class="header-anchor">#</a></h1><a id="more"></a>
<p>给定优化问题</p>
<script type="math/tex; mode=display">
\min f(x)\\
subject\ to \begin{cases}
g_i(x) = 0 (i=1,,,,m\\
h_i(x) <= 0 (i=m+1,...,n)
\end{cases}</script><p>构造lagrange函数</p>
<script type="math/tex; mode=display">
L(x,\lambda) = f(x)+\sum_{i=1}^{m}\lambda_ig_i(x)+\sum_{i=m+1}^{n}\lambda_ih_i(x)</script><p>KKT条件</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial x}=0\\
g_i(x)=0\\
\lambda_i>=0 (i=m+1,...,n)\\
\lambda_i h_i(x)=0(i=m+1,..,n)</script><h1><span id="er-ci-gui-hua-wen-ti">二次规划问题</span><a href="#er-ci-gui-hua-wen-ti" class="header-anchor">#</a></h1><p>问题的数学表达</p>
<script type="math/tex; mode=display">
\min Q(x) = \frac{1}{2}x^THx+g^Tx\\
s.t. a_i^Tx = b_i (i=1,..,m)\\
\ \ \  \ \ \ \ a_i^Tx <= b_i(i=m+1,...n)</script><p>KKT条件</p>
<script type="math/tex; mode=display">
\bigtriangledown f(x)-A^T\lambda =0\\
A_{E}x - b_{E}=0\\
A_{L}x-b_L<=0\\
\lambda_L>=0\\
\lambda_L^T(A_LX_L-b_L)=0</script><p>如果$H$半正定，二次规划问题的全局极小值的充要条件，$x^{*}$是一个K-T条件</p>
<p>证明：</p>
<p>必要性：KKT</p>
<p>充分性：</p>
<script type="math/tex; mode=display">
f(x)-f(x^{*})=\frac{1}{2}x^THx+g^Tx-\frac{1}{2}x^{*T}Hx^{*}-g^Tx^{*}\\
=\frac{1}{2}(x-x^{*})^TH(x-x^{*})+x^{*T}H(x-x^{*})+g^T(x-x^{*})\\
>=x^{*T}H(x-x^{*})+g^T(x-x^{*})=\lambda^TA(x-x^{*})</script><p><a href="http://www.hankcs.com/ml/lagrange-duality.html#h3-7">http://www.hankcs.com/ml/lagrange-duality.html#h3-7</a></p>
<h1><span id="smo-sequential-minimal-optimization">SMO ：Sequential minimal optimization</span><a href="#smo-sequential-minimal-optimization" class="header-anchor">#</a></h1><p>支持向量机的对偶问题</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{m}\alpha_i\\
s.t. \sum_{i=1}^{m}\alpha_iy_i=0\\
0<=\alpha_i<=C</script><p>这个优化问题，可以根据二次规划求解，但是如果样本 过多，特别慢</p>
<p>Platt提出了一种更快的方法</p>
<p>SMO算法是一种启发式算法，其基本思路是：</p>
<p>如果所有变量的解都满足此最优化问题的KKT条件（Karush-Kuhn-Tuckerconditions)，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。</p>
<p>否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。</p>
<p>假设$\alpha_i,\alpha_j$为选择的两个优化变量，则优化问题</p>
<script type="math/tex; mode=display">
\min W(\alpha_i,\alpha_j)=\frac{1}{2}\alpha_i\alpha_iy_iy_iK(x_i,x_i)+\frac{1}{2}\alpha_j\alpha_jy_jy_jK(x_j,x_j)+\alpha_i\alpha_jy_iy_jK(x_i,x_j)\\
+\sum_{k_1,k_2!=i,j}^{m}\alpha_{k_1}\alpha_{k_2}y_{k_1}y_{k_2}K(x_{k_1},x_{k_2})-(\alpha_i+\alpha_j)\\
s.t\ \ \  \alpha_iy_i+\alpha_jy_j=-\sum_{k!=i,j}^{m}\alpha_ky_k=\xi\\
0<=\alpha_i<=C</script><p>上述问题就是关于</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>数学</category>
      </categories>
      <tags>
        <tag>二次规划</tag>
      </tags>
  </entry>
  <entry>
    <title>kaggle</title>
    <url>/2019/03/24/kaggle/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title>scikit-learn</title>
    <url>/2019/03/23/scikit-learn/</url>
    <content><![CDATA[<h1><span id><!-- more  --></span><a href="#" class="header-anchor">#</a></h1><h1><span id="cross-validation-evaluating-estimator-performancep">Cross-validation: evaluating estimator performance</span><a href="#cross-validation-evaluating-estimator-performancep" class="header-anchor">#</a></h1><p><img src="https://scikit-learn.org/stable/_images/grid_search_workflow.png" alt="Grid Search Workflow"></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 调用train_test_split函数 自动划分数据集 40%for testing</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, test_size=0.4, random_state=0)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="corss-validation">corss validation</span><a href="#corss-validation" class="header-anchor">#</a></h2><p><img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" alt="../_images/grid_search_cross_validation.png"></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from sklearn.model_selection import cross_validate</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">scoring = ['precision_macro', 'recall_macro']</span><br><span class="line">clf = svm.SVC(kernel='linear', C=1, random_state=0)</span><br><span class="line">scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,</span><br><span class="line">                        cv=5, return_train_score=False)</span><br><span class="line">sorted(scores.keys())</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="cross-validation-of-time-series-data">Cross validation of time series data</span><a href="#cross-validation-of-time-series-data" class="header-anchor">#</a></h2><p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png" alt="../_images/sphx_glr_plot_cv_indices_0101.png"></p>
<h1><span id="tuning-the-hyper-parameters-of-an-estimator">Tuning the hyper-parameters of an estimator</span><a href="#tuning-the-hyper-parameters-of-an-estimator" class="header-anchor">#</a></h1><p>A search consists of:</p>
<ul>
<li><p>an estimator (regressor or classifier such as <code>sklearn.svm.SVC()</code>);</p>
</li>
<li><p>a parameter space;</p>
</li>
<li><p>a method for searching or sampling candidates;</p>
</li>
<li><p>a cross-validation scheme; and</p>
</li>
<li><p>a <a href="https://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring">score function</a>.</p>
</li>
</ul>
<h2><span id="grid-search">Grid Search</span><a href="#grid-search" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">param_grid = [</span><br><span class="line">  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},</span><br><span class="line">  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},</span><br><span class="line"> ]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loading the Digits dataset</span></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># To apply an classifier on this data, we need to flatten the image, to</span></span><br><span class="line"><span class="comment"># turn the data in a (samples, feature) matrix:</span></span><br><span class="line">n_samples = len(digits.images)</span><br><span class="line">X = digits.images.reshape((n_samples, <span class="number">-1</span>))</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the dataset in two equal parts</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the parameters by cross-validation</span></span><br><span class="line">tuned_parameters = [{<span class="string">'kernel'</span>: [<span class="string">'rbf'</span>], <span class="string">'gamma'</span>: [<span class="number">1e-3</span>, <span class="number">1e-4</span>],</span><br><span class="line">                     <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]},</span><br><span class="line">                    {<span class="string">'kernel'</span>: [<span class="string">'linear'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]}]</span><br><span class="line"></span><br><span class="line">scores = [<span class="string">'precision'</span>, <span class="string">'recall'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scores:</span><br><span class="line">    print(<span class="string">"# Tuning hyper-parameters for %s"</span> % score)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line">    clf = GridSearchCV(SVC(), tuned_parameters, cv=<span class="number">5</span>,</span><br><span class="line">                       scoring=<span class="string">'%s_macro'</span> % score)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Best parameters set found on development set:"</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(clf.best_params_)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"Grid scores on development set:"</span>)</span><br><span class="line">    print()</span><br><span class="line">    means = clf.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">    stds = clf.cv_results_[<span class="string">'std_test_score'</span>]</span><br><span class="line">    <span class="keyword">for</span> mean, std, params <span class="keyword">in</span> zip(means, stds, clf.cv_results_[<span class="string">'params'</span>]):</span><br><span class="line">        print(<span class="string">"%0.3f (+/-%0.03f) for %r"</span></span><br><span class="line">              % (mean, std * <span class="number">2</span>, params))</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Detailed classification report:"</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"The model is trained on the full development set."</span>)</span><br><span class="line">    print(<span class="string">"The scores are computed on the full evaluation set."</span>)</span><br><span class="line">    print()</span><br><span class="line">    y_true, y_pred = y_test, clf.predict(X_test)</span><br><span class="line">    print(classification_report(y_true, y_pred))</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note the problem is too easy: the hyperparameter plateau is too flat and the</span></span><br><span class="line"><span class="comment"># output model is the same for precision and recall with ties in quality.</span></span><br></pre></td></tr></tbody></table></figure>
<h2><span id="randomized-parameter-optimization">Randomized Parameter Optimization</span><a href="#randomized-parameter-optimization" class="header-anchor">#</a></h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span>(<span class="params">results, n_top=<span class="number">3</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: {0}"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: {0:.3f} (std: {1:.3f})"</span>.format(</span><br><span class="line">                  results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                  results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: {0}"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters and distributions to sample from</span></span><br><span class="line">param_dist = {<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">"max_features"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">"min_samples_split"</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># run randomized search</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                   n_iter=n_iter_search, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line">print(<span class="string">"RandomizedSearchCV took %.2f seconds for %d candidates"</span></span><br><span class="line">      <span class="string">" parameter settings."</span> % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use a full grid over all parameters</span></span><br><span class="line">param_grid = {<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">"max_features"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">"min_samples_split"</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># run grid search</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid, cv=<span class="number">5</span>)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"GridSearchCV took %.2f seconds for %d candidate parameter settings."</span></span><br><span class="line">      % (time() - start, len(grid_search.cv_results_[<span class="string">'params'</span>])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></tbody></table></figure>
<p>  step1： 交叉验证（评价模型）</p>
<p>step2: 超参数选择，每一组参数：对应一次交叉验证</p>
<p>step 3: 集成学习</p>
<p>也可进行参数的调解</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">clf = AdaBoostClassifier(n_estimators=100)</span><br><span class="line">scores = cross_val_score(clf, iris.data, iris.target, cv=5)</span><br><span class="line">scores.mean()                             </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from itertools import product</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line"># Loading some example data</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, [0, 2]]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># Training classifiers</span><br><span class="line">clf1 = DecisionTreeClassifier(max_depth=4)</span><br><span class="line">clf2 = KNeighborsClassifier(n_neighbors=7)</span><br><span class="line">clf3 = SVC(gamma='scale', kernel='rbf', probability=True)</span><br><span class="line">eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],</span><br><span class="line">                        voting='soft', weights=[2, 1, 2])</span><br><span class="line"></span><br><span class="line">clf1 = clf1.fit(X, y)</span><br><span class="line">clf2 = clf2.fit(X, y)</span><br><span class="line">clf3 = clf3.fit(X, y)</span><br><span class="line">eclf = eclf.fit(X, y)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Boosting</title>
    <url>/2019/03/22/Boosting/</url>
    <content><![CDATA[<p>[TOC]</p>
<p>Boosting</p>
<a id="more"></a>
<h1><span id="boosting">Boosting</span><a href="#boosting" class="header-anchor">#</a></h1><h2><span id="yuan-li">原理</span><a href="#yuan-li" class="header-anchor">#</a></h2><p>Boosting算法是将“弱学习算法“提升为“强学习算法”的过程。</p>
<ol>
<li><p>加法模型</p>
<script type="math/tex; mode=display">
F_n(x;P) = \sum_{t=1}^{n}\alpha_th_t(x;a_t)</script></li>
<li><p>前向分步</p>
<script type="math/tex; mode=display">
F_m(x) = F_{m-1}(x)+\alpha_mh_m(x,a_m)</script><p>如果选取不同损失函数，则产生不同的类型</p>
</li>
</ol>
<h1><span id="adaboost">AdaBoost</span><a href="#adaboost" class="header-anchor">#</a></h1><p>AdaBoost就是损失函数为指数损失的Boosting算法。</p>
<ol>
<li><p>每一次迭代的弱学习$h(x;a_m)$有何不一样，如何学习？</p>
<p>AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。</p>
</li>
<li><p>弱分类器权值$β_m$如何确定？</p>
<p>AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。</p>
</li>
</ol>
<h2><span id="yuan-li-li-jie">原理理解</span><a href="#yuan-li-li-jie" class="header-anchor">#</a></h2><p>基于Boosting的理解，对于AdaBoost，我们要搞清楚两点：</p>
<p>每一次迭代的弱学习h(x;am)有何不一样，如何学习？<br>弱分类器权值βm如何确定？<br>对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。</p>
<p>对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。</p>
<h2><span id="gong-shi-tui-dao">公式推导</span><a href="#gong-shi-tui-dao" class="header-anchor">#</a></h2><p>指数损失函数</p>
<script type="math/tex; mode=display">
L(Y,f(x))=exp(-Yf(x))</script><p>权重更新公式: 采用的指数误差函数</p>
<script type="math/tex; mode=display">
l_{exp}(a_th_t|D_t)=E(exp(-f(x)a_th_t(x)))\\
=p(f(x)=h_t(x))e^{-at}+p(f(x)!=h_t(x))e^{at}\\
=e^{-at}(1-\xi)+e^{at}\xi</script><script type="math/tex; mode=display">
a_t=\frac{1}{2}ln \frac{1-\xi}{\xi}</script><p>分布更新公式</p>
<script type="math/tex; mode=display">
\begin{aligned} l\left(H_{t-1}(x)+\alpha h_{t}(x) | D\right) &=E_{X \sim D}\left(\exp \left(-y(x)\left(H_{t-1}(x)+\alpha h_{t}(x)\right)\right)\right) \\ &=E_{x \sim D}\left(\exp \left(-y(x) H_{t-1}(x)\right) \exp \left(-y(x) \alpha h_{t}(x)\right)\right) \end{aligned}</script><p>在泰勒展开$exp(-y(x)h_t(x))$</p>
<script type="math/tex; mode=display">
\begin{aligned} l\left(H_{t-1}(x)+h_{t}(x) | D\right) & \approx E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-\alpha y(x) h_{t}(x)+\frac{\alpha^{2} y^{2}(x) h_{t}^{2}(x)}{2}\right)\right] \\ &=E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-y(x) h_{t}(x)+0.5 \alpha^{2}\right)\right] \end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned} h(x) &=\arg \min _{h} l\left(H_{t-1}(x)+\alpha h_{t} | D\right) \\ &=\arg \max _{h} E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right) \alpha y(x) h_{t}(x)\right] \\ &=\arg \max _{h}\left[\frac{\exp \left(-y(x) H_{t-1}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} y(x) h(x)\right] \end{aligned}</script><script type="math/tex; mode=display">
</script><p>令一个新分布,注意分子是常数</p>
<script type="math/tex; mode=display">
D_{t}(x)=\frac{D(x) \exp \left(-y(x) H_{t-1}(x)\right)^{L}}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]}</script><script type="math/tex; mode=display">
\begin{aligned} h(x) &=\arg \max _{h} E_{x \sim D,}(y(x) h(x)) \\ &=\arg \max _{h} E_{x \sim D_{t}}(1-2 \mathcal{I}(y(x) \neq h(x))) \\ &=\arg \min _{h} E_{x \sim D_{i}}(\mathcal{I}(y(x) \neq h(x))) \end{aligned}</script><p>同理可得</p>
<script type="math/tex; mode=display">
\begin{aligned} D_{t+1} &=\frac{D(x) \exp \left(-y(x) H_{t}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=\frac{D_{t}(x) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right] \cdot \exp \left(-y(x) H_{t}(x)\right)}{\exp \left(-y(x) H_{t-1}(x)\right) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=D_{t}(x) \exp \left(-y(x) \alpha h_{t}(x)\right) \cdot C . \quad(C i s a \text {constant}) \end{aligned}</script><script type="math/tex; mode=display">
Z_{t}=\sum_{i}^{m} D_{t}(x) \exp \left(-y(x) \alpha_{t} h_{y}(x)\right)</script><p>指数误差函数</p>
<script type="math/tex; mode=display">
\begin{aligned} l(H(x) | D) &=\frac{1}{m} \sum_{i}^{m} \exp \left(-y_{i} H\left(x_{i}\right)\right) \\ &=\frac{1}{m} \sum_{i}^{m} \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=\sum_{i}^{m} D_{1}\left(x_{i}\right) \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=Z_{1} Z_{2}\left(x_{i}\right) \exp \left(-\sum_{j=2}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ & \vdots \\ &=\prod_{i=1}^{T} Z_{i} \end{aligned}</script><h2><span id="suan-fa-miao-shu">算法描述</span><a href="#suan-fa-miao-shu" class="header-anchor">#</a></h2><p>总结一下，得到AdaBoost的算法流程：</p>
<p>输入：训练数据集$T={(x1,y1),(x2,y2),(xN,yN)}T={(x1,y1),(x2,y2),(xN,yN)}$，其中，$xi∈X⊆Rnxi∈X⊆Rn，yi∈Y=−1,1yi∈Y=−1,1，$迭代次数M</p>
<p>初始化训练样本的权值分布：$D1=(w1,1,w1,2,…,w1,i),w,i=1,2,…,N$。</p>
<p>对于$m=1,2,…,M$</p>
<p>(a)　使用具有权值分布$D_m$的训练数据集进行学习，得到弱分类器$h_m(x)$　(b)　计算$h_m(x)$在训练数据集上的分类误差率：</p>
<p>$e_m=∑_{i=1}^{N}w_m,iI(h_m(xi)≠y_i)$</p>
<p>(c)　计算$h_m(x)$在强分类器中所占的权重：</p>
<p>$\alpha_m=\frac{1}{2}log(\frac{1−e_m}{e_m})$</p>
<p>(d)　更新训练数据集的权值分布（这里，$z_m是归一化因子，为了使样本的概率分布和为1）：</p>
<script type="math/tex; mode=display">w_{m+1,i}=\frac{w_{m,i}}exp(−α_my_ih_m(xi))，i=1,2,…,10</script><script type="math/tex; mode=display">z_m=∑_{i=1}^{N}w_{m,i}exp(−α_my_ih_m(xi))</script><p> 得到最终分类器：</p>
<script type="math/tex; mode=display">F(x)=sign(∑_{i=1}^{N}α_mh_m(x))</script><h2><span id="mian-jing">面经</span><a href="#mian-jing" class="header-anchor">#</a></h2><p>今年8月开始找工作，参加大厂面试问到的相关问题有如下几点：</p>
<ol>
<li>手推AdaBoost</li>
</ol>
<ol>
<li>与GBDT比较</li>
</ol>
<ol>
<li>AdaBoost几种基本机器学习算法哪个抗噪能力最强，哪个对重采样不敏感？</li>
</ol>
<h2><span id="suan-fa-liu-cheng">算法流程</span><a href="#suan-fa-liu-cheng" class="header-anchor">#</a></h2><h2><span id="shi-li-ji-suan">实例计算</span><a href="#shi-li-ji-suan" class="header-anchor">#</a></h2><h2><span id="python-shi-xian">Python实现</span><a href="#python-shi-xian" class="header-anchor">#</a></h2><p><a href="https://www.cnblogs.com/davidwang456/articles/8927029.html">https://www.cnblogs.com/davidwang456/articles/8927029.html</a></p>
<h1><span id="ji-cheng-xue-xi">集成学习</span><a href="#ji-cheng-xue-xi" class="header-anchor">#</a></h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Boosting, AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量回归</title>
    <url>/2019/03/19/SVR/</url>
    <content><![CDATA[<p>[TOC]</p>
<p>支持向量机用于分类:硬间隔和软件间隔支持向量机。尽可能分对</p>
<p>支持向量机回归： 希望$f(x)$与$y$尽可能的接近。</p>
<a id="more"></a>
<h1><span id="zhi-chi-xiang-liang-ji-ji-ben-si-xiang">支持向量机基本思想</span><a href="#zhi-chi-xiang-liang-ji-ji-ben-si-xiang" class="header-anchor">#</a></h1><p>英文名:support vector regression</p>
<p>简记：SVR</p>
<h2><span id="biao-zhun-de-xian-xing-zhi-chi-xiang-liang-hui-gui-mo-xing">标准的线性支持向量回归模型</span><a href="#biao-zhun-de-xian-xing-zhi-chi-xiang-liang-hui-gui-mo-xing" class="header-anchor">#</a></h2><p>学习的模型:</p>
<script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>假设能容忍$f(x)$与$y$之间差别绝对值$\xi$,这就以$f(x)=w^Tx+b$形成了一个$2\xi$的间隔带，因此模型</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}w^Tw\\
s.t -\xi<=f(x_i)-y_i<=\xi</script><p>但是上述条件太过严苛，因此增加惩罚项，</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}w^Tw+C\sum(\epsilon_i+\hat{\epsilon}_i)\\
s.t. \begin{cases}f(x_i)-y_i<=\xi+\epsilon_i\\
y_i-f(x_i)<=\xi+\hat{\epsilon}_i\\
\hat{\epsilon}_i>=0,\epsilon_i>=0
\end{cases}</script><p>构造Lagrange函数</p>
<script type="math/tex; mode=display">
\begin{aligned} L :=\frac{1}{2}\|\omega\|^{2} &+C \sum\left(\xi_i+\xi^{\prime}_i\right)-\sum_{i=1}^{N}\left(\eta_{i} \xi_{i}+\eta_{i}^{'} \xi_{i}6{'}\right) \\ &+\sum \alpha_{i}\left(y_{i}-\omega^{T} x_{i}-b-\varepsilon-\xi_{i}\right) \\ &+\sum \alpha_{i}^{'}\left(\omega^{T} x_{i}+b-y_{i}-\varepsilon-\xi_{i}^{\prime}\right) \end{aligned}\tag{1}</script><p>求偏导</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \omega}=\omega-\sum\left(\alpha_{i}-\alpha_{i}\right) x_{i}=0 \Rightarrow \omega=\sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right) x_{i}\tag{2}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial b}=\sum_{i=1}^{N}\left(\alpha_{i}-\alpha_{i}^{\prime}\right)=0 \tag{3}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial \xi_{i}^{\prime}}=C-\alpha_{i}^{'}-\eta_{i}^{\prime}=0 \tag{4}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial \xi_{i}}=C-\alpha_{i}-\eta_{i}=0 \tag{5}</script><p>将(2)-(4)带回(1),可得对偶问题</p>
<script type="math/tex; mode=display">
\begin{aligned} \min L(\boldsymbol{\alpha})=& \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}\left(\alpha_{i}-\alpha_{i}^{*}\right)\left(\alpha_{j}-\alpha_{j}^{*}\right)\left\langle x_{i}, x_{j}\right\rangle \\ &+\varepsilon \sum_{i=1}^{N}\left(\alpha_{i}+\alpha_{i}^{*}\right)-\sum_{i=1}^{N} y_{i}\left(\alpha_{i}-\alpha_{i}^{*}\right) \\ \text { s.t. } & \sum_{n=1}^{N}\left(\alpha_{n}-\alpha_{n}^{*}\right)=0 \end{aligned}</script><p>再将(2)带回$Y=w^Tx+b$,可得线性回归模型</p>
<script type="math/tex; mode=display">
y(x)=\sum_{i=1}^{N}\left(\alpha_{i}-\alpha_{i}^{*}\right) x_{i}^{T} x+b</script><h2><span id="fei-xian-xing-zhi-chi-xiang-liang-ji">非线性支持向量机</span><a href="#fei-xian-xing-zhi-chi-xiang-liang-ji" class="header-anchor">#</a></h2><p>考虑模型</p>
<script type="math/tex; mode=display">
y=f(x)+b</script><p>$f(x)$是非线性函数，存在一个由$X$所在空间到希尔伯特空间的映射，使得</p>
<script type="math/tex; mode=display">
f(x)=w^T\varphi(x)</script><p>因此，建立如下的优化问题</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}\|\omega\|^{T}+C \sum_{i}\left(\xi_{i}+\xi_{i}^{\prime}\right)\\
\begin{cases} y\left(x_{i}\right)-\omega^{T} \varphi\left(x_{i}\right)-b \leq \xi_{i} \\ \omega^{T} \varphi\left(x_{i}\right)+b-y\left(x_{i}\right) & \leq \xi_{i} \\ \xi_{i} & \geq 0 \\ \xi_{i} & \geq 0 \end{cases}</script><p>构造lagrange函数</p>
<script type="math/tex; mode=display">
\begin{aligned} L :=\frac{1}{2}\|\omega\|^{2} &+C \sum\left(\xi+\xi^{\prime}\right)-\sum\left(\eta_{i} \xi_{i}+\eta_{i} \xi_{i}^{\prime}\right) \\ &+\sum \alpha_{i}\left(y_{i}-w^{T} \varphi\left(x_{i}\right)-b-\varepsilon_{i}-\xi_{i}\right) \\ &+\sum \alpha_{\mathrm{i}}^{\prime}\left(w^{T} \varphi\left(x_{i}\right)+b-y_{i}-\varepsilon_{i}^{'}-\xi_{i}^{\prime}\right) \end{aligned}</script><p>求偏导</p>
<script type="math/tex; mode=display">
\begin{cases}\frac{\partial L}{\partial w}=w-\sum\left(\alpha_{i}-\alpha_{i}\right) \varphi\left(x_{i}\right)=0\\
 \frac{\partial L}{\partial b} =\sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right)=0 \\ \frac{\partial L}{\partial \xi_{i}^{\prime}} =C-\alpha_{i}^{'}-\eta_{i}^{\prime}=0 \\ \frac{\partial L}{\partial \xi_{i}} =C-\alpha_{i}-\eta_{i}=0 \end{cases}</script><p>再带回优化问题可得</p>
<script type="math/tex; mode=display">\min _{t}-\frac{1}{2} \sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right)\left(\alpha_{j}-\alpha_{j}^{\prime}\right) \varphi\left(x_{i}\right)^{T} \varphi\left(x_{j}\right)-\varepsilon \sum\left(\alpha_{i}+\alpha_{i}^{\prime}\right)+\sum y_{i}\left(\alpha_{i}-\alpha_{i}^{'}\right)\\s t . \sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right)=0</script><p>再次将$w$带回模型</p>
<script type="math/tex; mode=display">
y=\sum\left(\alpha_{i}-\alpha_{i}^{'}\right) \varphi\left(x_{i}\right)^{T} \varphi(x)+b</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>支持向量机回归</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机(SVM) ----- 分类器</title>
    <url>/2019/03/17/SVMClassifiar/</url>
    <content><![CDATA[<p>[TOC]</p>
<a id="more"></a>
<h1><span id="yu-bei-de-shu-xue-zhi-shi">预备的数学知识</span><a href="#yu-bei-de-shu-xue-zhi-shi" class="header-anchor">#</a></h1><h2><span id="yue-shu-you-hua-wen-ti">约束优化问题</span><a href="#yue-shu-you-hua-wen-ti" class="header-anchor">#</a></h2><p>原问题,带等式约束，也带不等式约束的一般约束问题</p>
<script type="math/tex; mode=display">
\begin{cases} \min_{x}f(x)\\
s.t \begin{cases} m_i(x)>=0, i=1,..,m\\
n_j(x)=0，j=1,..,m\\
\end{cases}
\end{cases}\tag{1}</script><p>构造lagrange乘子法</p>
<script type="math/tex; mode=display">
L(x,\lambda_i,\eta_j)= f(x)-\sum_{i=1}^{m}\lambda_im_i(x)-\sum_{j=1}^{n}\eta_j \tag{2}</script><script type="math/tex; mode=display">
\begin{cases} \min_{x} max_{\lambda_i,\eta_j} L(R^p)\\
s.t \lambda_i>=0
\end{cases}</script><p>上述两个问题的等价性证明</p>
<p>如果x不满足约束$m_i(x)$,则$\lambda_i&gt;=0$,同时$m_i(x)&lt;$,则$L(R^{p},\lambda,\eta)$趋近无穷，反之，则存在最大值</p>
<script type="math/tex; mode=display">
min_{x} max_{\lambda,\eta}=min_{x}(max f满足条件,max f不满足约束)\\=min_{x} max_{\lambda,\eta}{f满足条件}</script><p>对偶问题: 关于$\lambda,\eta$的最大化问题</p>
<script type="math/tex; mode=display">max min L(x,\lambda,\eta)\\
s.t \lambda_i>=0</script><p><strong>弱对偶问题：对偶问题&lt;=原问题</strong> </p>
<p>证明: $max_{x} min(\lambda \eta ) L&lt;=min_{\eta,\lambda } max_{x} L$</p>
<script type="math/tex; mode=display">
\underbrace{\min_{x}L(x,\lambda,\eta)}_{A(\lambda,\eta)}<=L(x,\lambda,\eta)<=\underbrace{\max_{\lambda,\eta} L(x,\lambda,\eta)}_{B(x)}</script><h1><span id="fen-lei">分类</span><a href="#fen-lei" class="header-anchor">#</a></h1><p>hard-margin SVM、 soft-margin SVM 、kernel SVM</p>
<h1><span id="xian-xing-ke-fen-zhi-chi-xiang-liang-ji">线性可分支持向量机</span><a href="#xian-xing-ke-fen-zhi-chi-xiang-liang-ji" class="header-anchor">#</a></h1><p><img src="/2019/03/17/SVMClassifiar/MyBlog\hexo\source\_posts\SVMClassifiar\1.png" alt></p>
<p>对于A子图，可以用一个超平面($w^Tx+b$)去分类两类数据，建立如下的数学模型</p>
<script type="math/tex; mode=display">f(w,b)=sign(w^Tx+b)</script><p>B,C,D子图提供了超平面都可以分类，显然B,C图的超平面的鲁棒性不如D图。SVM就是找到最好的一个超平面，怎么衡量好呢？找到平面离样本点的距离最大</p>
<h2><span id="hard-margin-svm-zui-da-jian-ge-svm">hard-margin SVM： 最大间隔SVM</span><a href="#hard-margin-svm-zui-da-jian-ge-svm" class="header-anchor">#</a></h2><h2><span id="di-yi-bao-jian-ge">第一宝 间隔</span><a href="#di-yi-bao-jian-ge" class="header-anchor">#</a></h2><p>首先，看下margin的定义</p>
<p><img src="/2019/03/17/SVMClassifiar/MyBlog\hexo\source\_posts\SVMClassifiar\2.png" alt></p>
<script type="math/tex; mode=display">margin(w,b) = min(\frac{|w^Tx_i+b|}{||w||})</script><p>接下来</p>
<p>数学模型：</p>
<script type="math/tex; mode=display">\begin{cases}  \max margin(w,b)\\ st. y_i(w^Tx_i+b)>0\end{cases}</script><script type="math/tex; mode=display">\Longrightarrow\begin{cases}  max \frac{1}{||w||}min(y_i(w^Tx_i+b))\\  st. y_i(w^Tx_i+b)>0\end{cases}</script><p>注意，$y_i(w^Tx_i+b)&gt;0$,所以$\exists r&gt;0, min(y_i(w^Tx_i+b))=r$,可令$r=1$,这是对超平面范数的固定作用，因为$y=w^Tx+b$和$y=2w^T+2b$是同一个超平面，总能找到缩放$w,b$使得，可以将$r$缩放到1</p>
<script type="math/tex; mode=display">\Longrightarrow\begin{cases}  max \frac{1}{||w||}\\  st. y_i(w^Tx_i+b)>=1\end{cases}</script><script type="math/tex; mode=display">\Longrightarrow\begin{cases}  \min \frac{1}{2}w^Tw\\  st. y_i(w^Tx_i+b)>=1\end{cases}</script><p>这是一个土二次规划问题</p>
<h2><span id="di-er-bao-dui-ou">第二宝 对偶</span><a href="#di-er-bao-dui-ou" class="header-anchor">#</a></h2><p>利用lagrange乘子法得出对偶问题</p>
<p><strong>带约束</strong></p>
<script type="math/tex; mode=display">\begin{cases}  \min \frac{1}{2}w^Tw\\  st. y_i(w^Tx_i+b)-1>=0\end{cases}</script><script type="math/tex; mode=display">\Longrightarrow L(w,b,\lambda）=\frac{1}{2}w^Tw-\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b)</script><p><strong>无约束</strong></p>
<script type="math/tex; mode=display">\begin{cases}min_{w,b} max_{\lambda}L(w,b,\lambda) \\ s.t \lambda_i>=0\end{cases}</script><p>此时关于$w,b$无约束的。</p>
<p>对$(L(w,b,\lambda))$ 对$w$,$b$求偏导</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=w+\sum_{i=1}^{N}y_ix_i\lambda_i=0 \Longrightarrow w=-\sum_{i=1}^{N}y_ix_i\lambda_i\\
\frac{\partial L}{\partial b}=-\sum_{i=1}^{N}\lambda_iy_i=0</script><p>带回$L(w,b,\lambda)$,可得<strong>对偶问题</strong></p>
<script type="math/tex; mode=display">\begin{cases} max_{\lambda}L(w,b,\lambda ) =-\frac{1}{2}\sum_i^N\sum_j^N\lambda_i  \lambda_jy_iy_jx_i^Tx_j +\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases} \Longrightarrow\\\begin{cases} min_{\lambda}L(w,b,\lambda ) =\frac{1}{2}\sum_i^N\sum_j^N\lambda_i  \lambda_jy_iy_jx_i^Tx_j -\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases}</script><h3><span id="yuan-wen-ti-he-dui-ou-wen-ti-you-xiang-tong-jie-de-chong-yao-tiao-jian">原问题和对偶问题有相同解的充要条件</span><a href="#yuan-wen-ti-he-dui-ou-wen-ti-you-xiang-tong-jie-de-chong-yao-tiao-jian" class="header-anchor">#</a></h3><p>满足 KKT</p>
<script type="math/tex; mode=display">
\begin{cases}
\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\\
\lambda_i(y_i(w^Tx_i+b)-1)=0\\
\lambda_i>=0\\
y_i(w^Tx_i+b)-1>=0
\end{cases}</script><p>如果存在$(x_k,y_k)=+1or -1$使得$y_i(w^Tx_i+b)-1=0$即可求解$b=y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k$</p>
<p>代入模型</p>
<script type="math/tex; mode=display">f(x)=sign(\sum_i^Na_iy_ix_i^Tx+y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k)</script><p>注意，对于任意的训练样本，总有$\lambda_i=0$或者$y_if(x_i)=1$,如果$\lambda_i&gt;0$,说明样本点落在最大间隔的边界上，这些点就是支持向量，这条边界$w^Tx+b=1or-1$</p>
<h1><span id="soft-marign-ruan-jian-ge">soft-marign 软间隔</span><a href="#soft-marign-ruan-jian-ge" class="header-anchor">#</a></h1><p>  想法：允许一部分样本可以不被正确分类</p>
<h2><span id="you-hua-mu-biao">优化目标</span><a href="#you-hua-mu-biao" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
\min_{w,b} \frac{1}{2}w^Tw+loss</script><h2><span id="yi-xie-sun-shi-han-shu">一些损失函数</span><a href="#yi-xie-sun-shi-han-shu" class="header-anchor">#</a></h2><ol>
<li><p>0-1损失 个数</p>
<script type="math/tex; mode=display">loss=\sum_{i=1}^NI\{y_i(w^Tx+b)<1\}</script><p>数学性质不好，不连续</p>
</li>
<li><p>0-1损失 距离 <strong>hinge loss</strong></p>
<script type="math/tex; mode=display">
loss = \begin{cases}
0 , y_i(w^Tx_i+b)>=0,\\
1-y_i(w^tx_i+b),  y_i(w^Tx_i+b)<1\\
\end{cases}</script><script type="math/tex; mode=display">
loss_{max} = max(0,1-y_i(w^Tx_i+b)=1-z)</script><p>此时优化问题，令$\xi_i=1-y_i(w^Tx_i+b)$</p>
<script type="math/tex; mode=display">
\min \frac{1}{2}w^Tw+\sum_{i=1}^{N}\xi_i\\
s.t \begin{cases}
y_i(w^Tx_i+b)>=1-\xi_i\\
\xi_i>=0
\end{cases}</script></li>
<li><p>指数损失（<strong>exponential loss </strong>)</p>
<script type="math/tex; mode=display">
l_{exp}(z)=exp(-z)</script></li>
<li><p>对率损失<strong>logistic loss</strong></p>
<script type="math/tex; mode=display">
l_{log}(z)=log(1+exp(-z)）</script></li>
</ol>
<p><img src="/2019/03/17/SVMClassifiar/MyBlog\hexo\source\_posts\SVMClassifiar\3.jpg" alt></p>
<h1><span id="he-fang-fa">核方法</span><a href="#he-fang-fa" class="header-anchor">#</a></h1><h2><span id="he-han-shu-de-ding-yi">核函数的定义</span><a href="#he-han-shu-de-ding-yi" class="header-anchor">#</a></h2><p>设 $\chi$为输入空间（Input Space），  $\mathrm{H}$为特征空间(Feature Space,一定是希尔伯特空间），存在一个映射</p>
<script type="math/tex; mode=display">
\varphi : \chi \rightarrow \mathrm{H}</script><p>对任意的 $x, y \in \mathrm{X}$，函数  $K(x, y)$，满足</p>
<script type="math/tex; mode=display">
K(x, y)=<\varphi(x), \varphi(y)></script><p>则称  $K(x, y)$为核函数。可以看出，我们并不需要知道输入空间和特征空间满足的映射关系   ，只需要知道核函数就可以算出，输入空间中任意两点映射到特征空间的内积。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title>回归树</title>
    <url>/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1><span id="fen-lei-shu-yu-hui-gui-shu">分类树与回归树</span><a href="#fen-lei-shu-yu-hui-gui-shu" class="header-anchor">#</a></h1><p>分类树用于分类问题。分类决策树在选取划分点，用信息熵、信息增益、或者信息增益率、或者基尼系数为标准。<br>Classification tree analysis is when the predicted outcome is the class to which the data belongs.</p>
<p>回归决策树用于处理输出为连续型的数据。回归决策树在选取划分点，就希望划分的两个分支的误差越小越好。</p>
<p>Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital)。</p>
<a id="more"></a>
<h1><span id="hui-gui-shu">回归树</span><a href="#hui-gui-shu" class="header-anchor">#</a></h1><p>英文名字：Regression Tree</p>
<h2><span id="yuan-li-jie-shao">原理介绍</span><a href="#yuan-li-jie-shao" class="header-anchor">#</a></h2><p>决策树最直观的理解其实就是，输入特征空间($R^n$)，然后对特征空间做划分，每一个划分属于同一类或者对于一个输出的预测值。那么这个算法需要解决的问题是1. 如何决策边界(划分点)？2. 尽可能少的比较次数(决策树的形状)</p>
<p><img src="/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/MyBlog\hexo\source\_posts\回归树\原理介绍1.PNG" alt="原理1"></p>
<p>如上图，每一个非叶子对于某个特征的划分。</p>
<h3><span id="zui-xiao-er-cheng-hui-gui-shu-sheng-cheng-suan-fa">最小二乘回归树生成算法</span><a href="#zui-xiao-er-cheng-hui-gui-shu-sheng-cheng-suan-fa" class="header-anchor">#</a></h3><p>Q1: 选择划分点？遍历所有的特征($n$),对于每一个特征对应$s_i$个取值，尝试完所有特征，以及特征所以有划分，选择使得损失函数最小的那组特征以及特征的划分取值。</p>
<p>Q2: 叶节点的输出？取每个区域所以结果的平均数作为输出</p>
<p>节点的损失函数的形式</p>
<script type="math/tex; mode=display">
 \min _{j, s}\left[\min _{c_{1}} Loss(y_i,c_1)+\min _{c_{2}} Loss(y_i,c_2)\right]</script><p>节点有两条分支，$c1$是左节点的平均值，$c2$是右节点的平均值，换句话说，分一次划分都是使得划分出的两个分支的误差和最小。最终得到函数是<font color="red">分段函数</font></p>
<h2><span id="cart-suan-fa">CART算法</span><a href="#cart-suan-fa" class="header-anchor">#</a></h2><p>输入： 训练数据集</p>
<p>输出：回归树$f(x)$</p>
<ol>
<li><p>选择最优的特征$j$和分切点$s$</p>
<script type="math/tex; mode=display">
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]</script></li>
<li><p>对于选定的$(j,s)$划分区域，并确定该区域的预测值</p>
</li>
<li><p>对两个区域递归1. 2. 直到满足停止条件</p>
</li>
<li><p>返回生成树</p>
<p>注：分切点选择：先排序，二分。</p>
</li>
</ol>
<h1><span id="python-dai-ma">Python代码</span><a href="#python-dai-ma" class="header-anchor">#</a></h1><h2><span id="jie-dian-lei">节点类</span><a href="#jie-dian-lei" class="header-anchor">#</a></h2><p>属性：左右节点、loss、特征编号或者特征、分割点</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, score=None</span>):</span></span><br><span class="line">        <span class="comment"># 构造函数</span></span><br><span class="line">        self.score = score</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line">        self.feature = <span class="literal">None</span></span><br><span class="line">        self.split = <span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<h2><span id="hui-gui-shu-lei">回归树类</span><a href="#hui-gui-shu-lei" class="header-anchor">#</a></h2><p>构造方法</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RegressionTree</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.root = Node()</span><br><span class="line">        self.height = <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure>
<p>给定特征、划分点，返回计算MAPE</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_split_mse</span>(<span class="params">self, X, y, idx, feature, split</span>):</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">	X:训练样本输入</span></span><br><span class="line"><span class="string">	y:训练样本输出</span></span><br><span class="line"><span class="string">	idx:该分支对应的样本编号</span></span><br><span class="line"><span class="string">	feaure: 特征</span></span><br><span class="line"><span class="string">	split: 划分点</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line">	split_x1=X[X[idex,feature]&lt;split]</span><br><span class="line">	split_y1=y[X[idex,feature]&lt;split]</span><br><span class="line">	split_x2=X[X[idex,feature]&gt;=split]</span><br><span class="line">	split_y2=y[X[idex,feature]&gt;=split]</span><br><span class="line">	</span><br><span class="line">    split_avg = [np.mean(split_y1), np.mean(split_y2)]</span><br><span class="line">    split_mape = [np.sum((split_y1-split_avg[<span class="number">0</span>])**<span class="number">2</span>),np.sum((split_y2-split_avg[<span class="number">1</span>])**<span class="number">2</span>)]</span><br><span class="line">    <span class="keyword">return</span> split_mse, split, split_avg</span><br></pre></td></tr></tbody></table></figure>
<p>计算给定特征的最佳分割点</p>
<p>遍历特征某一列的所有的不重复的点，找出MAPE最小的点作为最佳分割点。如果特征中没有不重复的元素则返回None。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_choose_split_point</span>(<span class="params">self, X, y, idx, feature</span>):</span></span><br><span class="line">    feature_x = X[idx,feature]</span><br><span class="line">    uniques = np.unique(feature_x)</span><br><span class="line">    <span class="keyword">if</span> len(uniques)==<span class="number">1</span>:</span><br><span class="line">    	<span class="keyword">return</span> Noe</span><br><span class="line"></span><br><span class="line">    mape, split, split_avg = min(</span><br><span class="line">   (self._get_split_mse(X, y, idx, feature, split)</span><br><span class="line">       <span class="keyword">for</span> split <span class="keyword">in</span> unique[<span class="number">1</span>:]), key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> mape, feature, split, split_avg</span><br></pre></td></tr></tbody></table></figure>
<p>选择特征<br>遍历全部特征，计算mape,然后确定特征和对应的切割点，注意如果某个特征的值是一样的，则返回None<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_choose_feature</span>(<span class="params">self, X, y, idx</span>):</span></span><br><span class="line">    m = len(X[<span class="number">0</span>])</span><br><span class="line">    split_rets = [x <span class="keyword">for</span> x <span class="keyword">in</span> map(<span class="keyword">lambda</span> x: self._choose_split_point(</span><br><span class="line">        X, y, idx, x), range(m)) <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> split_rets == []:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    _, feature, split, split_avg = min(</span><br><span class="line">        split_rets, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">    idx_split = [[], []]</span><br><span class="line">    <span class="keyword">while</span> idx:</span><br><span class="line">        i = idx.pop()</span><br><span class="line">        xi = X[i][feature]</span><br><span class="line">        <span class="keyword">if</span> xi &lt; split:</span><br><span class="line">            idx_split[<span class="number">0</span>].append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx_split[<span class="number">1</span>].append(i)</span><br><span class="line">    <span class="keyword">return</span> feature, split, split_avg, idx_split</span><br></pre></td></tr></tbody></table></figure><br>对应叶子节点，打印相关的信息<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_expr2literal</span>(<span class="params">self, expr</span>):</span></span><br><span class="line">        feature, op, split = expr</span><br><span class="line">        op = <span class="string">"&gt;="</span> <span class="keyword">if</span> op == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"&lt;"</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Feature%d %s %.4f"</span> % (feature, op, split)  </span><br></pre></td></tr></tbody></table></figure><br>建立好二叉树以后，遍历操作<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_rules</span>(<span class="params">self</span>):</span></span><br><span class="line">    que = [[self.root, []]]</span><br><span class="line">    self.rules = []</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">while</span> que:</span><br><span class="line">        nd, exprs = que.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span>(nd.left <span class="keyword">or</span> nd.right):</span><br><span class="line">            literals = list(map(self._expr2literal, exprs))</span><br><span class="line">            self.rules.append([literals, nd.score])</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">if</span> nd.left:</span><br><span class="line">            rule_left = []</span><br><span class="line">            rule_left.append([nd.feature, <span class="number">-1</span>, nd.split])</span><br><span class="line">            que.append([nd.left, rule_left])</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">if</span> nd.right:</span><br><span class="line">            rule_right =[]</span><br><span class="line">            rule_right.append([nd.feature, <span class="number">1</span>, nd.split])</span><br><span class="line">            que.append([nd.right, rule_right])</span><br></pre></td></tr></tbody></table></figure><br>建立二叉树的过程，也就是训练的过程          <p></p>
<ol>
<li>控制深度</li>
<li>控制节叶子节点的最少样本数量</li>
<li>至少有一个特征是不重复的<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y, max_depth=<span class="number">5</span>, min_samples_split=<span class="number">2</span></span>):</span></span><br><span class="line">        self.root = Node()</span><br><span class="line">        que = [[<span class="number">0</span>, self.root, list(range(len(y)))]]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">while</span> que:</span><br><span class="line">            depth, nd, idx = que.pop(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">if</span> depth == max_depth:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">            <span class="keyword">if</span> len(idx) &lt; min_samples_split <span class="keyword">or</span> set(map(<span class="keyword">lambda</span> i: y[i,<span class="number">0</span>], idx)) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">            feature_rets = self._choose_feature(X, y, idx)</span><br><span class="line">            <span class="keyword">if</span> feature_rets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">            nd.feature, nd.split, split_avg, idx_split = feature_rets</span><br><span class="line">            nd.left = Node(split_avg[<span class="number">0</span>])</span><br><span class="line">            nd.right = Node(split_avg[<span class="number">1</span>])</span><br><span class="line">            que.append([depth+<span class="number">1</span>, nd.left, idx_split[<span class="number">0</span>]])</span><br><span class="line">            que.append([depth+<span class="number">1</span>, nd.right, idx_split[<span class="number">1</span>]])</span><br><span class="line">    </span><br><span class="line">        self.height = depth</span><br><span class="line">        self._get_rules()</span><br></pre></td></tr></tbody></table></figure>
打印叶子节点<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_rules</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, rule <span class="keyword">in</span> enumerate(self.rules):</span><br><span class="line">            literals, score = rule</span><br><span class="line">            print(<span class="string">"Rule %d: "</span> % i, <span class="string">' | '</span>.join(</span><br><span class="line">                literals) + <span class="string">' =&gt; split_hat %.4f'</span> % score)</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure>
预测单样本<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span>(<span class="params">self, row</span>):</span></span><br><span class="line">        nd = self.root</span><br><span class="line">        <span class="keyword">while</span> nd.left <span class="keyword">and</span> nd.right:</span><br><span class="line">            <span class="keyword">if</span> row[nd.feature] &lt; nd.split:</span><br><span class="line">                nd = nd.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nd = nd.right</span><br><span class="line">        <span class="keyword">return</span> nd.score</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 预测多条样本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [self._predict(Xi) <span class="keyword">for</span> Xi <span class="keyword">in</span> X]</span><br><span class="line">  </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    print(<span class="string">"Tesing the accuracy of RegressionTree..."</span>)</span><br><span class="line">    X_train=np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">9</span>],[<span class="number">10</span>]])</span><br><span class="line">    y_train=np.array([[<span class="number">5.56</span> ],[<span class="number">5.7</span>],[<span class="number">5.91</span>],[<span class="number">6.4</span></span><br><span class="line">                      ],[<span class="number">6.8</span>],[<span class="number">7.05</span>],[<span class="number">8.9</span>],[<span class="number">8.7</span></span><br><span class="line">                        ],[<span class="number">9</span> ],[<span class="number">9.05</span>]])</span><br><span class="line">    reg = RegressionTree()</span><br><span class="line">    print(reg)</span><br><span class="line">    reg.fit(X=X_train, y=y_train, max_depth=<span class="number">3</span>)</span><br><span class="line">    reg.print_rules()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></tbody></table></figure>
<h1><span id="jian-dan-de-li-zi">简单的例子</span><a href="#jian-dan-de-li-zi" class="header-anchor">#</a></h1></li>
</ol>
<p>训练数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>x</th>
<th style="text-align:center">1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td style="text-align:center">5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
<td>8.9</td>
<td>8.7</td>
<td>9</td>
<td>9.05</td>
</tr>
</tbody>
</table>
</div>
<p>根据上表，只有一个特征$x$.</p>
<ol>
<li><p>选择最优的特征$j$和分切点$s$</p>
<p>| 分切点(s) | 1.5   | 2.5   | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  | 8.5   | 9.5   |<br>| ————- | ——- | ——- | —— | —— | —— | —— | —— | ——- | ——- |<br>| $c_1$     | 5.56  | 5.63  | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88  | 7.11  |<br>| $c_2$     | 7.5   | 7.73  | 7.99 | 8.25 | 8.54 | 8.91 | 8.92 | 9.03  | 9.05  |<br>| loss      | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |</p>
<p>当分切点取$s=6.5$,损失最小$l(s=6.5)=1.93$,此时划分出两个分支，分别是$R_1=\{1,2,3,4,5,6\}$,$c_1=6.42$,$R_2=\{7,8,9,10\}$,$c_2=8.91$</p>
<ol>
<li><p>a) 对R1继续划分</p>
<p>| x    | 1    | 2    | 3    | 4    | 5    | 6    |<br>| —— | —— | —— | —— | —— | —— | —— |<br>| y    | 5.56 | 5.7  | 5.91 | 6.4  | 6.8  | 7.05 |</p>
<p>| 分切点(s) | 1.5    | 2.5   | 3.5    | 4.5    | 5.5    |<br>| ————- | ——— | ——- | ——— | ——— | ——— |<br>| $c_1$     | 5.56   | 5.63  | 5.72   | 5.89   | 6.07   |<br>| $c_2$     | 6.37   | 6.54  | 6.75   | 6.93   | 7.05   |<br>| loss      | 1.3087 | 0.754 | 0.2771 | 0.4368 | 1.0644 |</p>
<p>当分切点取$s=3.5$,损失函数$l(s=3.6)=0.2771$(假设此时满足停止条件）,此时得到两个分支，分别是$R_1=\{1,2,3\}$，$c_1=5.72$,$R_2={4,,5,6}$,$c_2=6.75$</p>
<p>b) 对R2继续划分</p>
<p>| x    | 7    | 8    | 9    | 10   |<br>| —— | —— | —— | —— | —— |<br>| y    | 8.9  | 8.7  | 9    | 9.05 |</p>
<p>| 分切点(s) | 7.5    | 8.5    | 9.5    |<br>| ————- | ——— | ——— | ——— |<br>| $c_1$     | 8.9    | 8.8    | 8.87   |<br>| $c_2$     | 8.92   | 9.03   | 9.05   |<br>| loss      | 0.0717 | 0.0213 | 0.0467 |</p>
<p>当分切点取$s=8.5$,损失函数$l(s=8,5)=0.0213$(假设此时满足停止条件）,此时得到两个分支，分别是$R_1=\{7,8\}$，$c_1=8.8$,$R_2=\{9,10\}$,$c_2=9.03$</p>
</li>
<li><p>函数表达式</p>
</li>
</ol>
</li>
</ol>
<pre><code>  $$
  \begin{equation}
  f(x)=\left\{
  \begin{aligned}
  5.72 &amp; &amp;  x&lt;3.5\\
  6.7 5&amp; &amp;3.5&lt;=x&lt;6.5\\
  8.8&amp; &amp;6.5&lt;=x&lt;8.5\\
  9.03&amp; &amp;8.5&lt;=x&lt;10\\
  \end{aligned}
  \right.
  \end{equation}
  $$
</code></pre><h1><span id="python-ku">Python库</span><a href="#python-ku" class="header-anchor">#</a></h1><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span>(<span class="params">criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, class_weight=None, presort=False</span>)</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Mar 13 19:59:53 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: 23230</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X=np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">9</span>],[<span class="number">10</span>]])</span><br><span class="line">y=np.array([[<span class="number">5.56</span> ],[<span class="number">5.7</span>],[<span class="number">5.91</span>],[<span class="number">6.4</span>],[<span class="number">6.8</span>],[<span class="number">7.05</span>],[<span class="number">8.9</span>],[<span class="number">8.7</span>],[<span class="number">9</span> ],[<span class="number">9.05</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit regression model</span></span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=<span class="number">3</span>)</span><br><span class="line">regr_3 = DecisionTreeRegressor(max_depth=<span class="number">4</span>)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line">regr_3.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.copy(X)</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line">y_3 = regr_3.predict(X_test)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Plot the results</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=<span class="number">20</span>, edgecolor=<span class="string">"black"</span>,c=<span class="string">"darkorange"</span>, label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(X_test, y_1, color=<span class="string">"cornflowerblue"</span>,label=<span class="string">"max_depth=2"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_2, color=<span class="string">"yellowgreen"</span>, label=<span class="string">"max_depth=4"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_3, color=<span class="string">"r"</span>, label=<span class="string">"max_depth=8"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">"data"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"target"</span>)</span><br><span class="line">plt.title(<span class="string">"Decision Tree Regression"</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/re.png" alt="1552478769877"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归树</tag>
      </tags>
  </entry>
  <entry>
    <title>BP算法</title>
    <url>/2019/03/05/BP%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>[TOC]<br>BP</p>
<a id="more"></a>
<h1><span id="1-xu-yao-de-wei-ji-fen-zhi-shi">1. 需要的微积分知识</span><a href="#1-xu-yao-de-wei-ji-fen-zhi-shi" class="header-anchor">#</a></h1><h2><span id="1-1-dao-shu">1.1 导数</span><a href="#1-1-dao-shu" class="header-anchor">#</a></h2><p>对于一元函数，在导数存在的情况下，在某一点的导数，也就是该点的斜率。<br>对于多元函数，对于某一点求导，则需要指明方向，两个特殊的方向，1. 偏导：在坐标轴方向的导数 2. 梯度的方向:总有一个方向是变化最快的。</p>
<h2><span id="1-2-qiu-dao-de-lian-shi-fa-ze">1.2 求导的链式法则</span><a href="#1-2-qiu-dao-de-lian-shi-fa-ze" class="header-anchor">#</a></h2><ol>
<li><p>$x \in R$, $z=g(f(x))$, $y=f(x)$</p>
<script type="math/tex; mode=display">\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \frac{\partial y}{\partial x}</script></li>
<li><p>$ x \in R^m $, $f(x)$是$R^M$到$R^n$的映射，$g(f)$是$R^n$到R的映射</p>
<script type="math/tex; mode=display">\frac{\partial g}{\partial x_i}=\sum_j^n \frac{\partial g}{\partial f_i} \frac{\partial f_i}{\partial x_i}</script><p> 如果使用向量表示</p>
<script type="math/tex; mode=display">\nabla_x^z=(\frac{\partial f}{\partial x})^T \nabla_y^z</script><h1><span id="2-ti-du-xia-jiang-fa">2. 梯度下降法</span><a href="#2-ti-du-xia-jiang-fa" class="header-anchor">#</a></h1><h2><span id="2-1-ti-du">2.1 梯度</span><a href="#2-1-ti-du" class="header-anchor">#</a></h2><p>梯度其实本质也是一个向量，对于函数$f(X,y)$<br>在$(W,y)$这一点的梯度 $(\frac{\partial f}{\partial X},\frac{\partial f}{\partial y})$<br>梯度的几何意义：在该店变化增加最快的地方</p>
</li>
</ol>
<h2><span id="2-2-ti-du-suan-fa-de-jie-shi">2.2 梯度算法的解释</span><a href="#2-2-ti-du-suan-fa-de-jie-shi" class="header-anchor">#</a></h2><p>图来自吴恩达的机器学习课程<br><img src="/2019/03/05/BP%E7%AE%97%E6%B3%95/2.1.1.png" alt="tu"><br>颜色偏红(A)的地方开始，根据梯度的负方向通过9次更新，达到了最小值(B)。<br>现在给定一个点$A(\theta_0,\theta_1)$,干嘛呢，我们想从A到B点（最小值点),类似人类下山，需要知道往那个方向吧、走大多一步呢？<br>方向：梯度的负方向 $ \delta=(\frac{\partial L}{\partial \theta_0},\frac{\partial L}{\partial \theta_1})$)<br>步长：学习率（$\alpha$)<br>因此，计算一次里目标更近了 $(\theta_0,\theta_1)=(\theta_0,\theta_1)-\alpha \dot (\delta)$<br>在重复上两步，直到满意为止。</p>
<h1><span id="3-wu-chai-fan-xiang-chuan-bo-suan-fa">3.误差反向传播算法</span><a href="#3-wu-chai-fan-xiang-chuan-bo-suan-fa" class="header-anchor">#</a></h1><h2><span id="3-1-li-lun-tui-dao">3.1 理论推导</span><a href="#3-1-li-lun-tui-dao" class="header-anchor">#</a></h2><p><img src="/2019/03/05/BP%E7%AE%97%E6%B3%95/3.1.1.png" alt="计算图"></p>
<h3><span id="3-1-1-fu-hao-shuo-ming">3.1.1 符号说明</span><a href="#3-1-1-fu-hao-shuo-ming" class="header-anchor">#</a></h3><p>上图是一个L层的神经网络，输入层为第一层，隐藏层：2至$L-1$层，输出层L</p>
<p>令 输入向量 $\vec{X}$</p>
<script type="math/tex; mode=display">\vec{X} = (x_1,x_2,...,x_{m-1},x_m)</script><p>输出向量 $\vec{Y}$</p>
<script type="math/tex; mode=display">\vec{Y}=(y_1,y_2,...,y_{n-1},y_n)</script><p>第j层隐藏层的输出向量 $\vec{h^{(j)}}$,有 </p>
<script type="math/tex; mode=display">\vec{h^{(j)}}=(h_1^{(j)},h_2^{(j)},...,h_{t-1}^{(j)},h_tj^{(j)})</script><p>其中，$t_j$:表示第j的隐藏层个数<br>第$(l-1)$层的第i个神经元到第$l$层的第j个神经元的连接权重：$w_{ij}^{(l)}$，则第$(l-1)$层神经元到第$l$层神经元的连接权重矩阵<script type="math/tex">W^{(l)}=\left( \begin{matrix}w_{11}^{(l)}& \cdots & w_{1(tj)}\\
    &   \dots &\\
    w_{s(l-1)}^{l}&\cdots&w_{s(l-1)s(l)}^{l}
\end{matrix}\right)</script></p>
<h3><span id="3-1-2-tui-dao-guo-cheng">3.1.2 推导过程</span><a href="#3-1-2-tui-dao-guo-cheng" class="header-anchor">#</a></h3><h4><span id="3-1-2-1-wu-chai">3.1.2.1 误差</span><a href="#3-1-2-1-wu-chai" class="header-anchor">#</a></h4><p>定义的误差函数,常见的衡量性指标见 <a href="#3.6">戳我</a>,这里选择的误差平方和最小<br>第$i$个输出的误差,假设实际输出$(d(1),d(2),…,d(n))$：,一个输入样本对应的误差</p>
<script type="math/tex; mode=display">E(i)=\frac{1}{2}\sum_{k=1}^n(y(i)-d(i))^2=\frac{1}{2}||y-d||^2</script><p>所有训练样本($N$)的误差：</p>
<script type="math/tex; mode=display">E(i)=\frac{1}{2}\sum_{j=1}^{N}(\sum_{k=1}^n(y(i)-d(i))^2)=\frac{1}{2N}\sum_{j=1}^{N}(||y(i)-d(i)||^2)</script><p>因此，</p>
<script type="math/tex; mode=display">E = \frac{1}{2N}\sum_{i=1}^N(||y(i)-d(i)||^2)</script><p>其实，神经网络的输出是关于节点的复合函数。代价函数是关于$W$和$b$的函数。</p>
<h4><span id="3-1-2-2-zheng-xiang-chuan-bo">3.1.2.2 正向传播</span><a href="#3-1-2-2-zheng-xiang-chuan-bo" class="header-anchor">#</a></h4><p>输入层$\hat{X}$：</p>
<script type="math/tex; mode=display">X =(x_1,x_2,x_3,...,x_m)</script><p>当有$N$个训练样本时，可用矩阵表示</p>
<script type="math/tex; mode=display">X=\left( \begin{matrix}
x_{11} &x_{12}&...&x_{1m}\\
x_{21} & x_{22}&...&x_{2m}\\
\vdots & \vdots&\dots&\vdots\\
x_{N1} & \vdots&\vdots&x_{Nm}\\
\end{matrix}  \right)</script><p>第二层 $h^{(2)}$,一共$s2$个节点:<br>第i个节点的计算</p>
<script type="math/tex; mode=display">h^{(2)}(i)=f(\sum_{j=1}^{s2}x(j)*w_{ji}^{(l)}+b_i)=f(x*w(:,i)+b_i)</script><p>矩阵表示</p>
<script type="math/tex; mode=display">h^{(2)}=f(x*W^{(l)}+b^{(2)})</script><p>第i层 矩阵形式</p>
<script type="math/tex; mode=display">h^{(l)}=f(h^{(l-1)}*W^{(l)}+b)</script><h4><span id="3-1-2-3-fan-xiang-chuan-bo">3.1.2.3 反向传播</span><a href="#3-1-2-3-fan-xiang-chuan-bo" class="header-anchor">#</a></h4><p>梯度下降法更新权重，不断迭代到最优解。<br>对$w_{ij}$求导数可得,可更新$w_{ij}$更新公式：</p>
<script type="math/tex; mode=display">w_{ij}=w_{ij}-\alpha \frac{\partial E}{\partial w_{ij}}</script><p>当然简单的情况下，可直接写出公式，当太复杂的时候，引入BP简化求导</p>
<p>方便书写公式，对于第i的输入$h^{(i-1)}*W^{(i)}+b^{(i)}$记作$net^{(i)}$,其中，第$i$的输入和输出的关系，$输入=f(输出)$<br>下面开始推导</p>
<p>首先，对于$L$层，</p>
<p>对于$W^{(L)}$，先看对$W_{ij}^{(L)}$求导，</p>
<script type="math/tex; mode=display">\frac{\partial E}{\partial W_{ij}^{(L)}}
=\frac{\partial E}{\partial y(j)} * \frac{\partial y(i)}{\partial net_{j}^{L}} * \frac{\partial net_{j}^{L}}{\partial W_{ij}^{(L)}}\\
=(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}h_i^{(L-1)}</script><p>令$\delta_i^{(L)}=y(i)-d(i)$</p>
<p>上述给出了单个分量的求偏导的结果，对于$W^{(L)}$</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial W^{(L)}}
=\left[\begin{matrix} 
\frac{\partial E}{\partial W_{11}^{(L)}} & \frac{\partial E}{\partial W_{12}^{(L)}}&\dots & \frac{\partial E}{\partial W_{1n}^{(L)}}\\
\frac{\partial E}{\partial W_{21}^{(L)}} & \frac{\partial E}{\partial W_{22}^{(L)}}&\dots& \frac{\partial E}{\partial W_{2n}^{(L)}}\\
\vdots& \dots& \dots& \dots\\
\frac{\partial E}{\partial W_{sL,1}^{(L)}} & \frac{\partial E}{\partial W_{sL,2}^{(L)}}&\dots& \frac{\partial E}{\partial W_{sL,n}^{(L)}}
\end{matrix}\right]
\\= \left[
\begin{matrix}
h^{(L-1)}_1\\h^{(L-1)}_2\\ \dots\\h^{(L-1)}_n
\end{matrix}
\right] *\left[\begin{matrix}
\delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\
\delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\
\dots\\
\delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}}
\end{matrix}\right] ^T
=h^{(L-1)}S^{(L)}</script><p>其中，</p>
<script type="math/tex; mode=display">
S^{(L)}=\left[\begin{matrix}

\delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\
\delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\
\dots\\
\delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}}
\end{matrix}\right]^T</script><p>同理可得，</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_k^{(L)}}=(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}</script><p>其次，对于隐含层$L-1$层，对$W_{ij}^{(L)}$求导</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial W_{ij}^{(L-1)}}
=\sum_{k=1}^{n}\frac{\partial E}{\partial y(k)} * \frac{\partial y(k)}{\partial net_{k}^{L}} * \frac{\partial net_{k}^{L}}{\partial f(net_j^{(L-1)})}*\frac{\partial f(net_j^{(L-1)})}{\partial net_j^{(L-1)}}*\frac{\partial net_j^{(L-1)}}{\partial W_{ij}^{(L-1)}}\\
=\sum_{k=1}^{n} (y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\
=\sum_{k=1}^{n}S_i^{(L)}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\</script><p>写出矩阵形式,对$W^{(L-1)}$</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial W^{(L-1)}}=\left[\begin{matrix} h^{(L-2)}_1\\h^{(L-2)}_2\\\vdots\\h^{(L-2)}_{s(L-2)}\end{matrix}\right] \left[\begin{matrix}

\delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\
\delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\
\dots\\
\delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}}
\end{matrix}\right]^T
\left[\begin{matrix} 
W_{11}^{(L)} &  W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\
W_{21}^{(L)} &  W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\
\vdots& \dots& \dots& \dots\\
 W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}
\end{matrix}\right]^T
 \\
\left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\
0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\
=h^{(L-2)}S^{(L-1)}</script><script type="math/tex; mode=display">
S^{(L-1)}=\left(\left[\begin{matrix}

f(x)^{'(L)}|_{x=net_1^{(L)}}&0& \dots& 0\\
0&f(x)^{'}|_{x=net_2^{(L)}}0& \dots& 0\\
0&\dots&\dots&0\\
0&0&0&f(x)^{'(L)}|_{x=net_n^{(L)}}
\end{matrix}\right]\left[\begin{matrix} \delta_1^{(L)}\\\delta_2^{(L)}\\\vdots\\\delta_n^{(L)}\end{matrix}\right] \right)^T\\
\left[\begin{matrix} 
W_{11}^{(L)} &  W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\
W_{21}^{(L)} &  W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\
\vdots& \dots& \dots& \dots\\
 W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}*
\end{matrix}\right]^T
\left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\
0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\
=S^{(L)}\left[\begin{matrix} 
W_{11}^{(L)} &  W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\
W_{21}^{(L)} &  W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\
\vdots& \dots& \dots& \dots\\
 W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}*
\end{matrix}\right]^T\left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\
0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]*\\</script><p>对$1&lt;l&lt;L$,求$W^{(l)}$的偏导,</p>
<p>最后，根据上述的推导喔，很容易得出$S^{(l)}$和$S^{(l+1)}$,</p>
<script type="math/tex; mode=display">
S^{(l)}=S^{(l+1)}W^{(l+1)^T}F^{'(l)}(net^{(l)})\\
S^{(L)}=(Y-\hat{Y})F^{'(L)}(net^{(L)})</script><script type="math/tex; mode=display">
\frac{\partial E}{\part W^{(l)}}=\left[\begin{matrix}h^{(l-1)}_1\\h^{(l-1)}_2 \\\dots \\h^{(l-1)}_{sl}\end{matrix}\right]S^{(l+1)} \left[\begin{matrix}W_{11}^{(l+1)}&W_{12}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\
W_{21}^{(l+1)}&W_{22}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\
\dots&\dots&\dots&\dots\\
W_{sl1}^{(l+1)}&W_{sl2}^{(l+1)} &\dots& W_{sl(sl+1)}^{(l+1)}\\
\end{matrix}  \right]^T\left[\begin{matrix} \part f^{'(l)}(net_1^{l})&0&\dots & 0\\
0\\0 &\part f^{'(l)}(net_2^{l})&\dots&0\\
0 & 0&\dots&0\\
0&0&\dots&\part f^{'(l)}(net_l^{l})\end{matrix}\right]</script><h2><span id="3-2-bp-suan-fa-de-xiao-jie">3.2 BP算法的小结</span><a href="#3-2-bp-suan-fa-de-xiao-jie" class="header-anchor">#</a></h2><p>算法分为两个阶段：前向阶段和后向传播阶段</p>
<p>后向阶段算法：</p>
<p>Step 1:  计算$\hat{y}^{(L)}$</p>
<p>Step 2:  for l =L:2</p>
<p>​        计算$S^{(l)}=S^{(l+1)}W^{(l+1)}F’(net^{(l)})$</p>
<p>​        计算 $\Delta W^{(l)}=h^{(l-1)}S^{(l)} $</p>
<p>​        计算$W^{(l)}=W^{(l)}-\delta \Delta W^{(l)}$</p>
<h2><span id="3-3-python-shi-xian">3.3 Python实现</span><a href="#3-3-python-shi-xian" class="header-anchor">#</a></h2><h3><span id="3-3-1-zui-jian-dan-san-ceng-wang-luo">3.3.1 最简单三层网络</span><a href="#3-3-1-zui-jian-dan-san-ceng-wang-luo" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">不用任何框架，自己写一个三层的神经网络</span></span><br><span class="line"><span class="string"># input-3,hidden-4 output-1</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input Matrix</span></span><br><span class="line">X = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span> ,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output Matrix</span></span><br><span class="line">y = np.array([[<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># Nonlinear function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">X,derive=False</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> derive:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-X))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> X*(<span class="number">1</span>-X)</span><br><span class="line"><span class="comment"># relu</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">X,derive = False</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> derive:</span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>,X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (X&gt;<span class="number">0</span>).astype(float)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Weight bias</span></span><br><span class="line">W1 = <span class="number">2</span> * np.random.random((<span class="number">3</span>, <span class="number">4</span>))<span class="number">-1</span></span><br><span class="line">b1 = <span class="number">0.1</span> * np.ones((<span class="number">4</span>,))</span><br><span class="line"> </span><br><span class="line">W2 = <span class="number">2</span> * np.random.random((<span class="number">4</span>,<span class="number">1</span>))<span class="number">-1</span></span><br><span class="line">b2 = <span class="number">0.1</span> * np.ones((<span class="number">1</span>,))</span><br><span class="line"> </span><br><span class="line">rate = <span class="number">0.1</span></span><br><span class="line">noline = relu</span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">train_times = <span class="number">200</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> time <span class="keyword">in</span> range(train_times):</span><br><span class="line">    <span class="comment"># Layer one</span></span><br><span class="line">    A1 = np.dot(X,W1)+b1</span><br><span class="line">    Z1 = noline(A1)</span><br><span class="line">    <span class="comment"># Layer two </span></span><br><span class="line">    A2 = np.dot(Z1, W2)+b2</span><br><span class="line">    Z2 = noline(A2)</span><br><span class="line">    </span><br><span class="line">    cost = -y+Z2</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calc deltas </span></span><br><span class="line">    S2= cost*noline(A2,<span class="literal">True</span>)</span><br><span class="line">    delta_W2 = np.dot(Z1.T,S2)</span><br><span class="line">    bias2 = S2.sum(axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    S1 = np.dot(S2, W2.T)*noline(A1,<span class="literal">True</span>)</span><br><span class="line">    delta_W1= np.dot(X.T, S1)</span><br><span class="line">    bias1 = S1.sum(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    W1 = W1-rate*delta_W1</span><br><span class="line">    b1 = b1-rate*bias1</span><br><span class="line">    W2 = W2-rate*delta_W2</span><br><span class="line">    b2 = b2-rate*bias2</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'error'</span>,np.mean(((y-Z2)*(y-Z2))**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"prediction"</span>,Z2)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="3-4-fu-lu"><font id="3.6">3.4  附录</font>：</span><a href="#3-4-fu-lu" class="header-anchor">#</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Abbreviation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean absolute percentage error</td>
<td>MAPE</td>
</tr>
<tr>
<td>Root mean squares percentage error</td>
<td>RMSPE</td>
</tr>
<tr>
<td>Mean absolute percentage error</td>
<td>MAE</td>
</tr>
<tr>
<td>Mean squares error</td>
<td>MSE</td>
</tr>
<tr>
<td>Index of agreement</td>
<td>IA</td>
</tr>
<tr>
<td>Theil U statistic 1</td>
<td>U1</td>
</tr>
<tr>
<td>Theil U statistic 2</td>
<td>U2</td>
</tr>
<tr>
<td>Correlation coefficient</td>
<td>R</td>
</tr>
</tbody>
</table>
</div>
<p>MAPE    =    $\frac{1}{n} \sum_{k=1}^{n}\left|\frac{x^{(0)}(k)-\hat{x}^{(0)}(k)}{x^{(0)}(k)}\right| \times 100$<br>RMSPE    =    $\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(\frac{\hat{x}^{(0)}(k)-x^{(0)}(k)}{x^{(0)}(k)}\right)^{2}} \times 100$<br>MAE    =    $\frac{1}{n} \sum_{k=1}^{n}\left|\hat{x}^{(0)}(k)-x^{(0)}(k)\right|$<br>MSE    =    $\frac{1}{n} \sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}$<br>IA    =    $1-\frac{\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}}{\sum_{k=1}^{n} \left( \left| \hat{x}^{(0)}(k)-\overline{x} \right|+\left| x^{(0)}(k)-\overline{x}\right| \right)^{2}}$<br>U1    =    $\frac{\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(x^{(0)}(k)-x^{(0)}(k)\right)^{2}}}{\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}+\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}}$<br>U2    =    $\frac{\left[\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}\right]^{1 / 2}}{\left[\sum_{k=1}^{n} x^{(0)}(k)^{2}\right]^{1 / 2}}$<br>R    =    $\frac{\operatorname{Cov}(\hat{x}^{(0)}, x^{(0)})}{\sqrt{\operatorname{Var}[\hat{x}^{(0)}] \operatorname{Var}[x^{(0)}]}}$</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2019/03/03/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>主要是分享决策的基本知识点，重点在分类决策树上，对于回归的决策树后面在给出。希望大家和我一起做知识的传播者啦！<span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😁</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😮</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f62e.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p>
<a id="more"></a>
<p>[TOC]</p>
<h1><span id="jue-ce-shu">决策树</span><a href="#jue-ce-shu" class="header-anchor">#</a></h1><p>英文名字：Descision Tree</p>
<h2><span id="shi-me-shi-jue-ce-shu">什么是决策树</span><a href="#shi-me-shi-jue-ce-shu" class="header-anchor">#</a></h2><p>举个校园相亲的例子，今天校园的小猫(女)和小狗(男)准备配对，小猫如何才能在众多的优质🐶的心仪的狗呢？于是呢？有一只特乖巧的小猫找到了你，你正在学习机器学习，刚好学习了决策树，准备给这只猫猫挑选优质狗，当然，你不仅仅是直接告诉猫哪些狗是合适你的？你更应该详细的给猫讲解决策树是如何根据它提出的标准选出的符合要求的狗呢？<br>猫给出如下信息：<br>年龄&lt;0.5 不心仪；年龄大于&gt;=0.5  6.5&lt;=体重&lt;=8.5;心仪; 年龄&gt;=0.5 体重&gt;8.5 长相好 心仪;其余情况不心仪; 根据上述条件可以构造一颗树：<br><img src="/2019/03/03/%E5%86%B3%E7%AD%96%E6%A0%91/1.bmp" alt="tuyi"><br>上面的图就是决策树，最终的结果是心仪或者不心仪。决策树算法以树形结构表示数据分类的结果<!--0.5--></p>
<h2><span id="ji-ben-gai-nian">基本概念</span><a href="#ji-ben-gai-nian" class="header-anchor">#</a></h2><p>决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。<br>根节点：决策树具有数据结构里面的二叉树、树的全部属性<br>非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试<br>叶子节点 ：分类后获得分类标记<br>分支： 测试的结果</p>
<h2><span id="shu-xue-wen-ti-shang-gini-xi-shu">数学问题-熵-Gini系数</span><a href="#shu-xue-wen-ti-shang-gini-xi-shu" class="header-anchor">#</a></h2><p>什么是熵：熵的概念源于物理学，用于度量一个热力学系统的无序程度。<br>信息熵：不得不提香农这个大写的人啦！信息论里面的知识。在信息论里面，<font color="red">信息熵衡量信息量的大小，也就是对随机变量不确定度的一个衡量。熵越大，不确定性越大；</font><br>对于某个单符号无记忆信源，发出符号($x_i$)的概率是$p_i$,概率越大，符号的信息量就越小，香农公式 $I(x_i)=-log_{p_i}$。信源所含的信息熵就是信息量的期望]<br>$H(x)=-\sum p_i*log_{p_i}$<br>Gini系数： $Gimi(p) = 1-\sum_{k=1}^{K}p_k^2$</p>
<h2><span id="jue-ce-shu-ru-he-gou-jian-de-wen-ti">决策树如何构建的问题</span><a href="#jue-ce-shu-ru-he-gou-jian-de-wen-ti" class="header-anchor">#</a></h2><p>自我提问阶段：</p>
<p><font color="green" size="3">每个节点的位置如何确定？</font><br>特征的选择：每次选入的特征作为分裂的标准，都是使得决策树在这个节点的根据你自己选择的标准（信息熵最小、信息增益最大、gini系数最小）.</p>
<p><font color="green" size="3">每个节点在哪个值上做划分，确定分支结构呢？</font><br>遍历划分的节点的分界值操作来解决这个问题</p>
<p><font color="green" size="3">可以想象，我们构造的决策树足够庞大，决策树可以把每一个样本都分对，那么决策树的泛化能力就可以很差了</font><br>为了解决这个问题，就需要剪枝操作了</p>
<h3><span id="xun-lian-suan-fa">训练算法</span><a href="#xun-lian-suan-fa" class="header-anchor">#</a></h3><h4><span id="ji-yu-xin-xi-shang-de-gou-zao">基于信息熵的构造</span><a href="#ji-yu-xin-xi-shang-de-gou-zao" class="header-anchor">#</a></h4><p>当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小。<br>计算特征的信息熵公式如下：</p>
<script type="math/tex; mode=display">H(x) = -p_i(x)log^{p_i(x)}
= -\frac{n_j}{S}log^{\frac{n_j}{S}}</script><p>$n_j$: 第j个类别，在样本中出现的频数<br>$S$: 样本个数<br>对于离散属性，直接计算信息熵，连续属性，就需要划分区间，按区间计算信息熵。</p>
<ol>
<li>基于某一层的数据集<br> a. 遍历计算所有属性，遍历相应属性以不同值为分截点的信息熵<br> b. 选择信息熵最小的作为节点<ol>
<li>如果到达终止条件，返回相应信息，否则，按照分支重复步骤1<h4><span id="id3-suan-fa-xin-xi-zeng-yi-zui-da-hua">ID3算法： 信息增益最大化</span><a href="#id3-suan-fa-xin-xi-zeng-yi-zui-da-hua" class="header-anchor">#</a></h4>C:类别<script type="math/tex; mode=display">H(C)=-\sum_{i=1}^{m}p_i log _2^{p_i}</script>按照D组划分C<script type="math/tex; mode=display">H(C/D)=\sum_{i=1}^{v}\frac{|C_i|}{|C|}H(C_i)</script>信息增益<script type="math/tex; mode=display">gain(D) = gain(C)-H(C/D)</script>这里我就以网上给出的数据为例，给出根据信息熵构成决策树的计算过程。</li>
</ol>
</li>
<li><p>确定特征，统计属性值和分解结果，总共四个特征，四种特征的统计结果如下图：<br><img src="/2019/03/03/%E5%86%B3%E7%AD%96%E6%A0%91/2.jpg" alt="图er"></p>
</li>
<li><p>根据历史数据，在不知到任何情况下，计算数据本身的熵为</p>
<script type="math/tex; mode=display">- \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940</script></li>
<li>计算每个特征做为节点的信息熵<br>以天气为例，天气三种属性，当Outlook = sunny时，H(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; 当Outlook= overcast,$H(x)=0$,当Outlook = rainy ,$H(x) = 0.971$<br>所以，当选天气作为节点时，此时$H(x)=\frac{5}{14}<em>0.971+\frac{4}{14}</em>0+\frac{5}{14}*0.971 = 0.693$,gain(天气) = 0.247<br>同理，可得gain(温度) =0.029  gain(湿度)=0.152，gain(风)=0.048<br>因此选择天气节点，在递归实现其他节点的选择。<br>信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的；</li>
</ol>
<h4><span id="c4-5-xin-xi-zeng-yi-lu">C4.5: 信息增益率</span><a href="#c4-5-xin-xi-zeng-yi-lu" class="header-anchor">#</a></h4><p>如果这里考虑了一列ID,每个ID出现一次，所以算出的信息增益大。<br>$ H(x) = 0$,信息增益最大化了，可以引入信息增益率</p>
<script type="math/tex; mode=display">C(T) = \frac{信息增益}{H(T)}
=\frac{H(C)-H(C/T)}{H(T)}</script><h4><span id="cart-ji-ni-gini-xi-shu">CART:基尼(Gini)系数</span><a href="#cart-ji-ni-gini-xi-shu" class="header-anchor">#</a></h4><script type="math/tex; mode=display">G = 1-\sum_{i=l_k}^{k}p_i^2$$,也是对随机变量不确定性的一个衡量，gini越大，不确定性越大
### 连续属性的处理方法
选取分解点的问题： 分成不同的区间（二分、三分....)，分别计算增益值，然后比较选择。
将需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序
假设该属性对应不同的属性值共N个，那么总共有N-1个可能的候选分割值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点
## 评价
评价函数：
$$C(T) = \sum_{releaf} N_t*H(T)</script><p>$ N_t$：每个叶子节点里面含有的样本个数<br>$H(T)$:叶子节点含有的信息熵</p>
<h3><span id="guo-ni-he">过拟合</span><a href="#guo-ni-he" class="header-anchor">#</a></h3><p>如果决策树过于庞大，分支太多，可能造成过拟合。对应训练样本都尽可能的分对，也许样本本身就存在异常点呢？<br>I. 预剪枝：边构建，边剪枝</p>
<ol>
<li>指定深度d</li>
<li>节点的min_sample</li>
<li>节点熵值或者gini值小于阙值<br>熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。</li>
<li>当所以特征都用完了</li>
<li>指定节点个数<br>当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。</li>
</ol>
<p>II. 后剪枝： 构建好后，然后才开始裁剪</p>
<script type="math/tex; mode=display">C_\alpha(T) = C(T)+\alpha|T_{leaf}|</script><p>在构造含一棵树后，选一些节点做计算，看是否需要剪枝</p>
<h2><span id="jue-ce-shu-dan-ge-jie-dian-xuan-ze-de-dai-ma-shi-xian">决策树单个节点选择的代码实现</span><a href="#jue-ce-shu-dan-ge-jie-dian-xuan-ze-de-dai-ma-shi-xian" class="header-anchor">#</a></h2><h3><span id="jian-dan-shi-xian-liao-dan-ge-jie-dian-jue-ce-gou-zao-guo-cheng">简单实现了单个节点决策构造过程</span><a href="#jian-dan-shi-xian-liao-dan-ge-jie-dian-jue-ce-gou-zao-guo-cheng" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span>(<span class="params">X,y,d,value</span>):</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在d纬度上，按照value进行划分</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    index_a =(X[:,d]&lt;=value)</span><br><span class="line">    index_b =(X[:,d]&gt;value)</span><br><span class="line">    <span class="keyword">return</span> X[index_a],X[index_b],y[index_a],y[index_b]</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log </span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">y</span>):</span></span><br><span class="line">    counter = Counter(y) <span class="comment"># 字典</span></span><br><span class="line">    res = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num/len(y)</span><br><span class="line">        res+=-p*log(p)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gain</span>(<span class="params">X,y,d,v</span>):</span></span><br><span class="line">    X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">    e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)</span><br><span class="line">    <span class="keyword">return</span> (entropy(y)-e)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gainratio</span>(<span class="params">X,y,d,v</span>):</span></span><br><span class="line">    X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">    gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)</span><br><span class="line">    <span class="keyword">return</span> gain/(entropy(y_l)+entropy(y_r))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span>(<span class="params">y</span>):</span></span><br><span class="line">    counter = Counter(y)</span><br><span class="line">    res = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num / len(y)</span><br><span class="line">        res += -p**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line">    <span class="comment">#X_l,X_r,y_l,y_r = split(X,y,d,v)</span></span><br><span class="line">    <span class="comment">#return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span>(<span class="params">X,y</span>):</span></span><br><span class="line">    best_entropy = float(<span class="string">'inf'</span>)</span><br><span class="line">    best_d,best_v=<span class="number">-1</span>,<span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        sorted_index = np.argsort(X[:,d])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(X)):</span><br><span class="line">            <span class="keyword">if</span> (X[sorted_index[i],d] != X[sorted_index[i<span class="number">-1</span>],d]):</span><br><span class="line">                v = (X[sorted_index[i<span class="number">-1</span>],d]+X[sorted_index[i],d])/<span class="number">2</span></span><br><span class="line">                X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">                <span class="comment"># 信息熵</span></span><br><span class="line">                e = entropy(y_l)+entropy(y_r)</span><br><span class="line">                <span class="comment">#gini</span></span><br><span class="line">                e = gini(y_l) + gini(y_r)</span><br><span class="line">                <span class="comment"># 信息增益</span></span><br><span class="line">                e = -gain(X,y,d,v)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> e &lt; best_entropy:</span><br><span class="line">                    best_entropy, best_d,best_v = e,d,v</span><br><span class="line">    <span class="keyword">return</span> best_entropy, best_d, best_v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动来划分</span></span><br><span class="line"></span><br><span class="line">data =np.array([[	<span class="number">0.3</span>	,	<span class="number">5</span>	,	<span class="number">2</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.4</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.5</span>	,	<span class="number">6.5</span>	,	<span class="number">1</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.7</span>	,	<span class="number">9</span>	,	<span class="number">2</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.5</span>	,	<span class="number">7</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.4</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">8.5</span>	,	<span class="number">0</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.3</span>	,	<span class="number">5.5</span>	,	<span class="number">2</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.9</span>	,	<span class="number">10</span>	,	<span class="number">0</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">1</span>	,	<span class="number">12</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">9</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">])</span><br><span class="line">X =data[:,<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">y = data[:,<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 手动来划分</span></span><br><span class="line">best_entropy, best_d, best_v = try_split(X, y)</span><br><span class="line">print(best_entropy, best_d, best_v)</span><br><span class="line">X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)</span><br><span class="line">print(X1_l, X1_r, y1_l, y1_r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)</span><br><span class="line">X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)</span><br><span class="line">entropy(y2_l)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="python-sklean-li-mian-tree-mo-kuai-li-mian-de-decisiontreeclassifier">Python sklean里面tree模块里面的DecisionTreeClassifier</span><a href="#python-sklean-li-mian-tree-mo-kuai-li-mian-de-decisiontreeclassifier" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">clf =tree.DecisionTreeClassifier(max_depth=<span class="number">1</span>,criterion =<span class="string">'gini'</span>) <span class="comment"># criterion='entropy|gini'</span></span><br><span class="line"></span><br><span class="line">clf = clf.fit(X,y)</span><br></pre></td></tr></tbody></table></figure>
<p>训练好一颗决策树之后，我们可以使用export_graphviz导出器以Graphviz格式导出树。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz </span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,) </span><br><span class="line">graph = graphviz.Source(dot_data) </span><br><span class="line">graph.render(<span class="string">"data"</span>) </span><br></pre></td></tr></tbody></table></figure><br>在运行时可以出错：<br>ExecutableNotFound: failed to execute [‘dot’, ‘-Tpdf’, ‘-O’, ‘data’], make sure the Graphviz executables are on your systems’ PATH<br>原因：graphviz本身是一个软件，需要额外下载，并将其bin加入环境变量之中。<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html">下载</a><p></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达</title>
    <url>/2019/03/03/%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    <content><![CDATA[<h1><span id> </span><a href="#" class="header-anchor">#</a></h1><p>Neural Networks and Deep Learning </p>
<p>4 周</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>希腊字母</title>
    <url>/2019/03/03/%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D/</url>
    <content><![CDATA[<p>$\alpha$    $\beta$    $\gamma$    $\Gamma$    $\delta$    $\Delta$    $\epsilon$    $\varepsilon$    $\zeta$    $\eta$    $\theta$    $\Theta$    $\vartheta$    $\iota$    $\kappa$    $\lambda$    $\Lambda$    $\mu$    $\nu$<br>    $\xi$    $\Xi$    $\pi$    $\Pi$    $\varpi$    $\rho$    $\varrho$    $\sigma$    $\Sigma$    $\varsigma$    $\tau$    $\upsilon$    $\Upsilon$    $\phi$    $\Phi$    $\varphi$    $\chi$    $\psi$    $\Psi$    $\Omega$    $\omega$</p>
<p>alpha</p>
<p>beta</p>
<p>gamma</p>
<p>delta</p>
<p>epsilon</p>
<p>theta</p>
]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>希腊字母</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2019/03/03/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>主要是分享决策的基本知识点，重点在分类决策树上，对于回归的决策树后面在给出。希望大家和我一起做知识的传播者啦！<span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😁</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>😮</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f62e.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p>
<a id="more"></a>
<p>[TOC]</p>
<h1><span id="jue-ce-shu">决策树</span><a href="#jue-ce-shu" class="header-anchor">#</a></h1><p>英文名字：Descision Tree</p>
<h2><span id="shi-me-shi-jue-ce-shu">什么是决策树</span><a href="#shi-me-shi-jue-ce-shu" class="header-anchor">#</a></h2><p>举个校园相亲的例子，今天校园的小猫(女)和小狗(男)准备配对，小猫如何才能在众多的优质🐶的心仪的狗呢？于是呢？有一只特乖巧的小猫找到了你，你正在学习机器学习，刚好学习了决策树，准备给这只猫猫挑选优质狗，当然，你不仅仅是直接告诉猫哪些狗是合适你的？你更应该详细的给猫讲解决策树是如何根据它提出的标准选出的符合要求的狗呢？<br>猫给出如下信息：<br>年龄&lt;0.5 不心仪；年龄大于&gt;=0.5  6.5&lt;=体重&lt;=8.5;心仪; 年龄&gt;=0.5 体重&gt;8.5 长相好 心仪;其余情况不心仪; 根据上述条件可以构造一颗树：<br><img src="/2019/03/03/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/%E5%86%B3%E7%AD%96%E6%A0%91/1.bmp" alt="tuyi"><!--0.5--></p>
<p>上面的图就是决策树，最终的结果是心仪或者不心仪。决策树算法以树形结构表示数据分类的结果</p>
<h2><span id="ji-ben-gai-nian">基本概念</span><a href="#ji-ben-gai-nian" class="header-anchor">#</a></h2><p>决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。<br>根节点：决策树具有数据结构里面的二叉树、树的全部属性<br>非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试<br>叶子节点 ：分类后获得分类标记<br>分支： 测试的结果</p>
<h2><span id="shu-xue-wen-ti-shang-gini-xi-shu">数学问题-熵-Gini系数</span><a href="#shu-xue-wen-ti-shang-gini-xi-shu" class="header-anchor">#</a></h2><p>什么是熵：熵的概念源于物理学，用于度量一个热力学系统的无序程度。<br>信息熵：不得不提香农这个大写的人啦！信息论里面的知识。在信息论里面，<font color="red">信息熵衡量信息量的大小，也就是对随机变量不确定度的一个衡量。熵越大，不确定性越大；</font>样本纯度越大越好。<br>对于某个单符号无记忆信源，发出符号($x_i$)的概率是$p_i$,概率越大，符号的信息量就越小，香农公式 $I(x_i)=-log_{p_i}$。信源所含的信息熵就是信息量的期望]<br>$H(x)=-\sum p_i*log_{p_i}$<br>Gini系数： $Gimi(p) = 1-\sum_{k=1}^{K}p_k^2$</p>
<h2><span id="jue-ce-shu-ru-he-gou-jian-de-wen-ti">决策树如何构建的问题</span><a href="#jue-ce-shu-ru-he-gou-jian-de-wen-ti" class="header-anchor">#</a></h2><p>自我提问阶段：</p>
<p><font color="green" size="3">每个节点的位置如何确定？</font><br>特征的选择：每次选入的特征作为分裂的标准，都是使得决策树在这个节点的根据你自己选择的标准（信息熵最小、信息增益最大、gini系数最小）.</p>
<p>选取的标准：尽快能的划分出结果，使得分的结果最好。</p>
<p><font color="green" size="3">每个节点在哪个值上做划分，确定分支结构呢？</font><br>遍历划分的节点的分界值操作来解决这个问题</p>
<p><font color="green" size="3">可以想象，我们构造的决策树足够庞大，决策树可以把每一个样本都分对，那么决策树的泛化能力就可以很差了</font><br>为了解决这个问题，就需要剪枝操作了</p>
<h3><span id="xun-lian-suan-fa">训练算法</span><a href="#xun-lian-suan-fa" class="header-anchor">#</a></h3><h4><span id="ji-yu-xin-xi-shang-de-gou-zao">基于信息熵的构造</span><a href="#ji-yu-xin-xi-shang-de-gou-zao" class="header-anchor">#</a></h4><p>当选择某个特征作为节点时，我们就希望这个特征的使得分类结果信息熵越小越好，那么不确定性越小。<br>计算特征的信息熵公式如下：</p>
<script type="math/tex; mode=display">H(x) = -p_i(x)log^{p_i(x)}
= -\frac{n_j}{S}log^{\frac{n_j}{S}}</script><p>$n_j$: 第j个类别，在样本中出现的频数<br>$S$: 样本个数<br>对于离散属性，直接计算信息熵，连续属性，就需要划分区间，按区间计算信息熵。</p>
<ol>
<li>基于某一层的数据集<br> a. 遍历计算所有属性，遍历相应属性以不同值为分截点的信息熵<br> b. 选择信息熵最小的作为节点<ol>
<li>如果到达终止条件，返回相应信息，否则，按照分支重复步骤1<h4><span id="id3-suan-fa-xin-xi-zeng-yi-zui-da-hua">ID3 算法： 信息增益最大化</span><a href="#id3-suan-fa-xin-xi-zeng-yi-zui-da-hua" class="header-anchor">#</a></h4></li>
</ol>
</li>
</ol>
<p>建立在奥卡姆剃刀的基础上。</p>
<ol>
<li>思想</li>
</ol>
<p>集合C的信息熵</p>
<script type="math/tex; mode=display">H(C)=-\sum_{i=1}^{m}p_i log _2^{p_i}</script><p>按照D组划分C，数据集C的条件熵，</p>
<script type="math/tex; mode=display">H(C/D)=\sum_{i=1}^{v}\frac{|C_i|}{|C|}H(C_i) = \sum_{i=1}^{v}\frac{|C_i|}{|C|}\sum_{j = 1}^{m}\frac{|C_{ik}|}{|C_i|}log_2\frac{|C_{ik}|}{|C_2|}</script><p>信息增益 = 信息熵-条件熵</p>
<script type="math/tex; mode=display">gain(C,D) = gain(C)-H(C/D)</script><p>这里我就以网上给出的数据为例，给出根据信息熵构成决策树的计算过程。</p>
<ol>
<li>确定特征，统计属性值和分解结果，总共四个特征，四种特征的统计结果如下图：<br><img src="/2019/03/03/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/%E5%86%B3%E7%AD%96%E6%A0%91/2.jpg" alt="图er"></li>
<li>根据历史数据，在不知到任何情况下，计算数据本身的熵为<script type="math/tex; mode=display">- \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940</script></li>
<li><p>计算每个特征做为节点的信息熵<br>以天气为例，天气三种属性，当Outlook = sunny时，H(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; 当Outlook= overcast,$H(x)=0$,当Outlook = rainy ,$H(x) = 0.971$<br>所以，当选天气作为节点时，此时$H(x)=\frac{5}{14}<em>0.971+\frac{4}{14}</em>0+\frac{5}{14}*0.971 = 0.693$,gain(天气) = 0.247<br>同理，可得gain(温度) =0.029  gain(湿度)=0.152，gain(风)=0.048<br>因此选择天气节点，在递归实现其他节点的选择。<br>信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的；</p>
</li>
<li><p>缺点</p>
<p>没有剪纸策略，容易过拟合</p>
<p>信息增益准则表现出对取值较多的特征，列如编号，生日这种</p>
<p>没有考虑缺失值</p>
</li>
</ol>
<h4><span id="c4-5-xin-xi-zeng-yi-lu">C4.5: 信息增益率</span><a href="#c4-5-xin-xi-zeng-yi-lu" class="header-anchor">#</a></h4><p>C4.5 相对于ID3的缺点改进如下：</p>
<ol>
<li><p>引入了剪纸策略</p>
<p>  对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；   </p>
</li>
<li><p>引入信息增益率作为划分标准</p>
</li>
<li><p>连续特征离散化</p>
</li>
<li><p>缺失值处理。</p>
<p>  以不同概率划分到不同节点中   </p>
</li>
</ol>
<p>如果这里考虑了一列ID,每个ID出现一次，所以算出的信息增益大。<br>$ H(x) = 0$,信息增益最大化了，可以引入信息增益率</p>
<script type="math/tex; mode=display">C(T) = \frac{信息增益}{H(T)}
=\frac{H(C)-H(C/T)}{H(T)}</script><h4><span id="cart-ji-ni-gini-xi-shu">CART:基尼(Gini)系数</span><a href="#cart-ji-ni-gini-xi-shu" class="header-anchor">#</a></h4><script type="math/tex; mode=display">G = 1-\sum_{i=l_k}^{k}p_i^2$$,也是对随机变量不确定性的一个衡量，gini越大，不确定性越大
### 连续属性的处理方法
选取分解点的问题： 分成不同的区间（二分、三分....)，分别计算增益值，然后比较选择。
将需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序
假设该属性对应不同的属性值共N个，那么总共有N-1个可能的候选分割值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点。

阙值：threshold

## 评价
评价函数：
$$C(T) = \sum_{releaf} N_t*H(T)</script><p>$ N_t$：每个叶子节点里面含有的样本个数<br>$H(T)$:叶子节点含有的信息熵</p>
<h3><span id="guo-ni-he">过拟合</span><a href="#guo-ni-he" class="header-anchor">#</a></h3><p>如果决策树过于庞大，分支太多，可能造成过拟合。对应训练样本都尽可能的分对，也许样本本身就存在异常点呢？<br>I. 预剪枝：边构建，边剪枝</p>
<ol>
<li>指定深度d</li>
<li>节点的min_sample</li>
<li>节点熵值或者gini值小于阙值<br>熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。</li>
<li>当所有7特征都用完了</li>
<li>指定节点个数<br>当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。</li>
</ol>
<p>II. 后剪枝： 构建好后，然后才开始裁剪</p>
<script type="math/tex; mode=display">C_\alpha(T) = C(T)+\alpha|T_{leaf}|</script><p>在构造含一棵树后，选一些节点做计算，看是否需要剪枝。  后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多   </p>
<h3><span id="shang-bias">熵 bias</span><a href="#shang-bias" class="header-anchor">#</a></h3><p>生日这种属性，把属性分的太多了，分的越细，往往熵越大。</p>
<h2><span id="jue-ce-shu-dan-ge-jie-dian-xuan-ze-de-dai-ma-shi-xian">决策树单个节点选择的代码实现</span><a href="#jue-ce-shu-dan-ge-jie-dian-xuan-ze-de-dai-ma-shi-xian" class="header-anchor">#</a></h2><h3><span id="jian-dan-shi-xian-liao-dan-ge-jie-dian-jue-ce-gou-zao-guo-cheng">简单实现了单个节点决策构造过程</span><a href="#jian-dan-shi-xian-liao-dan-ge-jie-dian-jue-ce-gou-zao-guo-cheng" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span>(<span class="params">X,y,d,value</span>):</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">在d纬度上，按照value进行划分</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    index_a =(X[:,d]&lt;=value)</span><br><span class="line">    index_b =(X[:,d]&gt;value)</span><br><span class="line">    <span class="keyword">return</span> X[index_a],X[index_b],y[index_a],y[index_b]</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log </span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">y</span>):</span></span><br><span class="line">    counter = Counter(y) <span class="comment"># 字典</span></span><br><span class="line">    res = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num/len(y)</span><br><span class="line">        res+=-p*log(p)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gain</span>(<span class="params">X,y,d,v</span>):</span></span><br><span class="line">    X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">    e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)</span><br><span class="line">    <span class="keyword">return</span> (entropy(y)-e)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gainratio</span>(<span class="params">X,y,d,v</span>):</span></span><br><span class="line">    X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">    gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)</span><br><span class="line">    <span class="keyword">return</span> gain/(entropy(y_l)+entropy(y_r))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span>(<span class="params">y</span>):</span></span><br><span class="line">    counter = Counter(y)</span><br><span class="line">    res = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num / len(y)</span><br><span class="line">        res += -p**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line">    <span class="comment">#X_l,X_r,y_l,y_r = split(X,y,d,v)</span></span><br><span class="line">    <span class="comment">#return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span>(<span class="params">X,y</span>):</span></span><br><span class="line">    best_entropy = float(<span class="string">'inf'</span>)</span><br><span class="line">    best_d,best_v=<span class="number">-1</span>,<span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        sorted_index = np.argsort(X[:,d])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(X)):</span><br><span class="line">            <span class="keyword">if</span> (X[sorted_index[i],d] != X[sorted_index[i<span class="number">-1</span>],d]):</span><br><span class="line">                v = (X[sorted_index[i<span class="number">-1</span>],d]+X[sorted_index[i],d])/<span class="number">2</span></span><br><span class="line">                X_l,X_r,y_l,y_r = split(X,y,d,v)</span><br><span class="line">                <span class="comment"># 信息熵</span></span><br><span class="line">                e = entropy(y_l)+entropy(y_r)</span><br><span class="line">                <span class="comment">#gini</span></span><br><span class="line">                e = gini(y_l) + gini(y_r)</span><br><span class="line">                <span class="comment"># 信息增益</span></span><br><span class="line">                e = -gain(X,y,d,v)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> e &lt; best_entropy:</span><br><span class="line">                    best_entropy, best_d,best_v = e,d,v</span><br><span class="line">    <span class="keyword">return</span> best_entropy, best_d, best_v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动来划分</span></span><br><span class="line"></span><br><span class="line">data =np.array([[	<span class="number">0.3</span>	,	<span class="number">5</span>	,	<span class="number">2</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.4</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.5</span>	,	<span class="number">6.5</span>	,	<span class="number">1</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.7</span>	,	<span class="number">9</span>	,	<span class="number">2</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.5</span>	,	<span class="number">7</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.4</span>	,	<span class="number">6</span>	,	<span class="number">0</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">8.5</span>	,	<span class="number">0</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">0.3</span>	,	<span class="number">5.5</span>	,	<span class="number">2</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.9</span>	,	<span class="number">10</span>	,	<span class="number">0</span>	,	<span class="number">1</span>	],</span><br><span class="line">[	<span class="number">1</span>	,	<span class="number">12</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">[	<span class="number">0.6</span>	,	<span class="number">9</span>	,	<span class="number">1</span>	,	<span class="number">0</span>	],</span><br><span class="line">])</span><br><span class="line">X =data[:,<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">y = data[:,<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 手动来划分</span></span><br><span class="line">best_entropy, best_d, best_v = try_split(X, y)</span><br><span class="line">print(best_entropy, best_d, best_v)</span><br><span class="line">X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)</span><br><span class="line">print(X1_l, X1_r, y1_l, y1_r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)</span><br><span class="line">X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)</span><br><span class="line">entropy(y2_l)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="python-sklean-li-mian-tree-mo-kuai-li-mian-de-decisiontreeclassifier">Python sklean里面tree模块里面的DecisionTreeClassifier</span><a href="#python-sklean-li-mian-tree-mo-kuai-li-mian-de-decisiontreeclassifier" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">clf =tree.DecisionTreeClassifier(max_depth=<span class="number">1</span>,criterion =<span class="string">'gini'</span>) <span class="comment"># criterion='entropy|gini'</span></span><br><span class="line"></span><br><span class="line">clf = clf.fit(X,y)</span><br></pre></td></tr></tbody></table></figure>
<p>训练好一颗决策树之后，我们可以使用export_graphviz导出器以Graphviz格式导出树。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz </span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,) </span><br><span class="line">graph = graphviz.Source(dot_data) </span><br><span class="line">graph.render(<span class="string">"data"</span>) </span><br></pre></td></tr></tbody></table></figure><br>在运行时可以出错：<br>ExecutableNotFound: failed to execute [‘dot’, ‘-Tpdf’, ‘-O’, ‘data’], make sure the Graphviz executables are on your systems’ PATH<br>原因：graphviz本身是一个软件，需要额外下载，并将其bin加入环境变量之中。<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html">下载</a><p></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>我的读书笔记</title>
    <url>/2019/02/28/SVD/</url>
    <content><![CDATA[<h1><span id="mu-lu">目录 <span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></span><a href="#mu-lu" class="header-anchor">#</a></h1><p><span class="github-emoji"><span>1⃣</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/0031-20e3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <a href="#1">简单说一下特征值、特征向量与特征分解</a><br>&nbsp;&nbsp; I. <a href="#1.1">特征值、特征向量与特征分解</a><br>&nbsp;&nbsp; II. <a href="#1.2">几何意义</a><br>&nbsp;&nbsp; III.  <a href="#1.3">如何实现通过Matlab、Python实现</a><br><span class="github-emoji"><span>2⃣</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/0032-20e3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><a href="#2">详细解说SVD</a><br>&nbsp;&nbsp; I. <a href="#2.1">几何意义</a><br>&nbsp;&nbsp; I. <a href="#2.2">奇异值分解的推导过程</a><br>&nbsp;&nbsp; I. <a href="#2.3">SVD算例</a><br>&nbsp;&nbsp; I. <a href="#2.4">如何通过Matlab和Python</a><br><span class="github-emoji"><span>3⃣</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/0033-20e3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><a href="#3">应用举例</a><br>&nbsp;&nbsp; I. <a href="#3.1">特征值、特征向量与特征分解</a><br><span class="github-emoji"><span>4⃣</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/0034-20e3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><a href="#4">特征分解、奇异值分解的区别</a><br>&nbsp;&nbsp; I. <a href="#4">特征分解、奇异值分解的区别</a></p>
<h1><span id="jian-dan-shuo-yi-xia-te-zheng-zhi-te-zheng-xiang-liang-yu-te-zheng-fen-jie"><font id="1" color="blue"> 简单说一下特征值、特征向量与特征分解</font></span><a href="#jian-dan-shuo-yi-xia-te-zheng-zhi-te-zheng-xiang-liang-yu-te-zheng-fen-jie" class="header-anchor">#</a></h1><a id="more"></a>
<h2><span id="te-zheng-zhi-te-zheng-xiang-liang-yu-te-zheng-fen-jie"><font id="1.1">特征值、特征向量与特征分解</font></span><a href="#te-zheng-zhi-te-zheng-xiang-liang-yu-te-zheng-fen-jie" class="header-anchor">#</a></h2><p><font color="red">Theory:</font><br>对于一个正阵$M$，满足如下：</p>
<script type="math/tex; mode=display">Mx=\lambda x</script><p>其中$\lambda$被成为特征值，满足$||M-\lambda E||=0$再有$(M-\lambda E)x=0$，可计算其特征向量。<br>如果有了特征值和特征向量后呢，则可以将矩阵$M$用特征分解：</p>
<script type="math/tex; mode=display">M=W\sum W^{-1}</script><p>$W={w_1,w_2,…,w_n}$分别是特征值$\lambda_1,\lambda_2,…,\lambda_n$对应的特征向量构成的方阵</p>
<h2><span id="ji-he-yi-yi"><font id="1.2"> 几何意义 </font></span><a href="#ji-he-yi-yi" class="header-anchor">#</a></h2><p>对应矩阵M,其对应的线性变化</p>
<script type="math/tex; mode=display">Mx = x'</script><p>上面这个式子，$Mx，x’$是一个向量，$x,x’$可能是不共线的(如图(b))，如果向量$Mx,x’$满足$Mx=x’=\lambda x$,则如图(b)，这说明了这个变换就是对向量x做一个拉伸或者压缩。<br><img src="/2019/02/28/SVD/SVD1.png" alt="图一"></p>
<h2><span id="ru-he-shi-xian-tong-guo-matlab-python-shi-xian"><font id="1.2">如何实现通过Matlab、Python实现</font></span><a href="#ru-he-shi-xian-tong-guo-matlab-python-shi-xian" class="header-anchor">#</a></h2><p>数学推导：</p>
<script type="math/tex; mode=display">Mx = \lambda x</script><script type="math/tex; mode=display">Mx-\lambda x=(M-\lambda E)x=0</script><p>齐次线性方程组有非零解，则$||M-\lambda E||=0$可求得特征向量<br>再带回，可得特征向量。<br>Matlab:<br></p><figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line">d = eig(M) <span class="comment">% 求取矩阵M的特征值，向量形式存储</span></span><br><span class="line">[V,D] = eig(M) <span class="comment">% 计算M的特征值对角阵D和特征向量V，使得MV = VD成立</span></span><br><span class="line">[V,D] = eig(M,<span class="string">'nobalance'</span>)   <span class="comment">%当矩阵M中有与截断误差数量级相差不远的值时，该指令可能更精确。'nobalance'起误差调节作用</span></span><br></pre></td></tr></tbody></table></figure><br>Python<br>numpy科学计算库提供相应的方法<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.diag((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)) <span class="comment"># 这是你想要求取特征值的数组</span></span><br><span class="line">a,b = numpy.linalg.elg(x) <span class="comment"># 特征值赋值给a,对应的特征向量赋值给b</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<h1><span id="xiang-xi-jie-shuo-svd"><font id="2" color="blue">详细解说SVD</font></span><a href="#xiang-xi-jie-shuo-svd" class="header-anchor">#</a></h1><p>SVD的英文全称： Singular Value Decomposition，中文名字：奇异值分解</p>
<h2><span id="ji-he-yi-yi"><font id="2.1">几何意义</font></span><a href="#ji-he-yi-yi" class="header-anchor">#</a></h2><p><a href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd">图来源</a><br>以二维空间为例<br><img src="/2019/02/28/SVD/SVD3.png" alt="图二"><img src="/2019/02/28/SVD/SVD4.png" alt="图二"><br>几何意义就是把一个单位正交的网格，转换为另外一个单位正交的网格</p>
<p>假如选取了一组单位正交基{$\vec{v}_1$,$\vec{v}_2$},刚好矩阵$M$的线性变化$M\vec{v}_1 $,$M\vec{v}_2 $ 也正交，用$\vec{u}_1,\vec{u}_2 $分别表示$M\vec{v}_1 $,$M\vec{v}_2 $ 的单位向量，用$\lambda_1,\lambda_2 $表示$M\vec{v}_1 $,$M\vec{v}_2$的长度，描述网格在这些特定方向上的拉伸量，也被称作矩阵M的奇异值。<br>$M\vec{v}_1 =\lambda_1\vec{u}_1 $<br>$M\vec{v}_2 =\lambda_2\vec{u}_2 $<br>对任意给定的向量 $\vec{x}$ ,则有</p>
<script type="math/tex; mode=display">
\mathbf{x}=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \mathbf{v}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \mathbf{v}_{2}</script><p> 再将M的线性变换</p>
<script type="math/tex; mode=display">
\begin{aligned} M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) M \mathbf{N}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) M \mathbf{v}_{2} \\ M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \sigma_{1} \mathbf{u}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \sigma_{2} \mathbf{u}_{2} \end{aligned}</script><script type="math/tex; mode=display">
\begin{array}{c}{M \mathbf{x}=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top} \mathbf{x}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top} \mathbf{x}} \\ {M=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top}}\end{array}</script><p> so</p>
<script type="math/tex; mode=display">
M=U \Sigma V^{T}</script><h2><span id="qi-yi-zhi-fen-jie-de-tui-dao-guo-cheng"><font id="2.2">奇异值分解的推导过程</font></span><a href="#qi-yi-zhi-fen-jie-de-tui-dao-guo-cheng" class="header-anchor">#</a></h2><p>$u=(u_1,u_2,…,u_m)$<br>$v=(v_1,v_2,…,v_n)$<br>$u,v$都是空间的基,是正交矩阵 $u^Tu=E,v^Tv = E$<br><img src="/2019/02/28/SVD/SVD2.png" alt="图二"><br>任何一个矩阵$M_{m*n}$，$rank(M)=k$，一定存在ＳＶＤ,换句话说，M可以将一组单位正交基映射到另一组单位正交基。答案是肯定的<br>证明如下：<br>在n为空间中，有一组单位正交基{$\vec{v}_1,\vec{v}_2,…,\vec{v}_n$},线性变化作用以后</p>
<script type="math/tex; mode=display">
{M\vec{v}_1,M\vec{v}_2,...,M\vec{v}_n}</script><p>也是正交的，则有</p>
<script type="math/tex; mode=display">
(M\vec{v}_i,M\vec{v}_j) = (M\vec{x}_i)^TM\vec{v}_j=\vec{v}_i^TM^TM\vec{v}_j=0</script><p>注意喔，$M^TM$是矩阵喔，则会有$M^TM\vec{v}_j=\lambda \vec{v}_j$<br>接下去，</p>
<script type="math/tex; mode=display">
\begin{aligned} v_{i}^{T} M^{T} \mathrm{M} v_{j}=& v_{i}^{T} \lambda_{j} v_{j} \\ &=\lambda_{j} v_{i}^{T} v_{j} \\ &=\lambda_{j} v_{i}\dot v_{j}=0 \end{aligned}</script><p> 上述就证明了是有的：任何一个矩阵，都可以将一组单位正交基转换成另外一组正交基。<br> 当$i=j$,$<m\vec{v}_i,m \vec{v}_i>=\lambda_i \vec{v}_i \vec{v}_i=\lambda_i$<br> 进行一些单位化，记$u_i=\frac{A\vec{v}_i}{|M\vec{v}_i|}=\frac{1}{\sqrt{\lambda_i}}M\vec{v}_i$<br>则</m\vec{v}_i,m></p>
<script type="math/tex; mode=display">
A v_{i}=\sigma_{i} u_{i}, \sigma_{i}(\operatorname{奇异值})=\sqrt{\lambda_{i}}, 0 \leq i \leq \mathrm{k}, \mathrm{k}=\operatorname{Rank}(\mathrm{A})</script><p> 当$k &lt; i &lt;= m$时，对$u1，u2，…，uk$进行扩展$u(k+1),…,um$，使得$u1，u2，…，um$为$m$维空间中的一组正交基.也可对$\vec{v}_1,\vec{v}_2,…,\vec{v}_k$进行扩展，扩展的$\vec{v}_{k+1},…,\vec{v}_{n}$存在零子空间里面。</p>
<script type="math/tex; mode=display">
M\left[ \begin{array}{lll}{\vec{v}_{1}} & {\cdots} & {\vec{v}_{k}}\end{array}\right| \vec{v}_{k+1} \quad \cdots \quad \vec{v}_{m} ]=
\left[ \begin{array}{c}{\vec{u}_{1}^{T}} \\ {\vdots} \\ {\frac{\vec{u}_{k}^{T}}{\vec{u}_{k+1}}} \\ {\vdots} \\ {\vec{u}_{n}^{T}}\end{array}\right] \left[ \begin{array}{ccc|c}\sigma_{1} &   & 0 & 0\\  & {\ddots} & \sigma_{k} & 0 \\ \hline 0 &  & 0 &0\end{array}\right]</script><script type="math/tex; mode=display">
M=\left[ \begin{array}{lll}{\vec{u}_{1}} & {\cdots} & {\vec{u}_{k}}\end{array}\right] \left
[ \begin{array}{ccc}\sigma_{1} &  & \\  & {\ddots} & \\  &  & {\sigma_{k}}\end{array}\right] 
\left[ \begin{array}{c}{\vec{v}_{1}^{T}} \\ {\vdots} \\ {\vec{v}_{k}^{T}}\end{array}\right]+
\left[ \begin{array}{ccc}{\vec{u}_{k+1}} & {\cdots} & {\vec{u}_{m}}\end{array}\right] 
\left[\begin{array}{c} 0 \end{array} \right] 
\left[ \begin{array}{c}{\vec{v}_{k+1}^{T}} \\ {\vdots} \\ {\vec{v}_{n}^{T}}\end{array}\right]</script><h2><span id="svd-suan-li"><font id="2.3">SVD算例</font></span><a href="#svd-suan-li" class="header-anchor">#</a></h2><p>U：$AA^T$的特征值和特征向量，用单位化的特征向量构成 U<br>V: $A^TA$ 的特征值和特征向量，用单位化的特征向量构成 V<br>$\sum_{mn} $ :将$ AA^{T} $或者 A^{T}A 的特征值求平方根，然后构成 Σ<br>以矩阵$A = \left[\begin{matrix} 1 &amp; 1\\1 &amp;1\\ 0 &amp;0\\\end{matrix} \right]$<br>第一步 U ，下面是一种计算方法<br>对矩阵</p>
<script type="math/tex; mode=display">
A A^{T}=\left[ \begin{array}{lll}{2} & {2} & {0} \\ {2} & {2} & {0} \\ {0} & {0} & {0}\end{array}\right]</script><p> 特征分解，<br> 特征是4，0，0<br> 特征向量是<br> $\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},\left[-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},[0,0,1]^{T}$,可得到</p>
<script type="math/tex; mode=display">
U=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right]</script><p> 第二步<br> 计算矩阵$A^TA$的特征分解，可得<br> 特征值4，0，</p>
<script type="math/tex; mode=display">
V=\left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]</script><p>第三步<br>计算$\sum_{mn}$</p>
<script type="math/tex; mode=display">
\Sigma=\left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right]</script><p> 最后，</p>
<script type="math/tex; mode=display">
A=U \Sigma V^{T}=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]^{T}=\left[ \begin{array}{cc}{1} & {1} \\ {1} & {1} \\ {0} & {0}\end{array}\right]</script><h2><span id="ru-he-tong-guo-matlab-he-python"><font id="2.4">如何通过Matlab和Python</font></span><a href="#ru-he-tong-guo-matlab-he-python" class="header-anchor">#</a></h2><p>Matlab：<br></p><figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line">s = svd(A)</span><br><span class="line">[U,S,V] = svd(A)</span><br><span class="line">[U,S,V] = svd(A,<span class="string">'econ'</span>)</span><br><span class="line">[U,S,V] = svd(A,<span class="number">0</span>)</span><br><span class="line">input: A 矩阵</span><br><span class="line">output:</span><br><span class="line">        s:奇异值，以列向量形式返回。奇异值是以降序顺序列出的非负实数</span><br><span class="line">        S：</span><br><span class="line">        U:左奇异向量，以矩阵的列形式返回。</span><br><span class="line">        V:奇异值，以对角矩阵形式返回。S 的对角元素是以降序排列的非负奇异值。</span><br><span class="line">        右奇异向量，以矩阵的列形式返回。</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>Python<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">M = np.array([ [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">U,S,V  = np.linalg.svd(M)</span><br></pre></td></tr></tbody></table></figure><p></p>
<h1><span id="ying-yong-ju-li"><font id="3" color="blue">应用举例</font></span><a href="#ying-yong-ju-li" class="header-anchor">#</a></h1><h2><span id="ying-yong"><font id="3.1">应用</font></span><a href="#ying-yong" class="header-anchor">#</a></h2><p> 2.1 信息检索<br> 2.2 推荐系统<br> 2.3 基于协同过滤的推荐系统<br> 2.4 图像压缩</p>
<h1><span id="te-zheng-zhi-fen-jie-he-qi-yi-zhi-fen-jie-de-qu-bie"><font id="4" color="blue">特征值分解和奇异值分解的区别</font></span><a href="#te-zheng-zhi-fen-jie-he-qi-yi-zhi-fen-jie-de-qu-bie" class="header-anchor">#</a></h1><ol>
<li>特征值分解只能是方阵，而奇异值分解是矩阵就可以</li>
<li>特征值分解只考虑了对矩阵缩放效果，奇异值分解对矩阵有选择、收缩、投影的效果<br><img src="/2019/02/28/SVD/SVD5.png" alt="图二"><img src="/2019/02/28/SVD/SVD6.png" alt="图二"><img src="/2019/02/28/SVD/SVD7.png" alt="图二"></li>
</ol>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>python库</title>
    <url>/2019/02/24/python%E5%BA%93/</url>
    <content><![CDATA[<p>开始接触Python是大二结束的时候，到现在都快两年了，其实一直并不是很细节的学习，只是希望能够跑个结果。不过呢？，以后肯定是会经常用Python，所以呢？我接下来会认真学习Python</p>
<a id="more"></a>
<h1><span id="python-gao-ji-yong-fa-zong-jie">Python 高级用法总结</span><a href="#python-gao-ji-yong-fa-zong-jie" class="header-anchor">#</a></h1><p>基本数据类型：整型、浮点型、布尔类型</p>
<h3><span id="rong-qi-containers">容器： Containers</span><a href="#rong-qi-containers" class="header-anchor">#</a></h3><p>容器是一种把多个元素组织在一起的数据结构，容器中的元素可以逐个地迭代获取，可以用in, not in关键字判断元素是否包含在容器中。通常这类数据结构把所有的元素存储在内存中（也有一些特例，并不是所有的元素都放在内存，比如迭代器和生成器对象）在Python中，常见的容器对象有：<br>list, deque<br>set, frozensets<br>dict, defaultdict, OrderedDict, Counter<br>tuple, namedtuple<br>str</p>
<h2><span id="list-tui-dao-list-comprehensions">list推导（list comprehensions)</span><a href="#list-tui-dao-list-comprehensions" class="header-anchor">#</a></h2><p>官方解释：列表解析式是Python内置的非常<strong>简单</strong>却<strong>强大</strong>的可以用来创建list的生成式。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">对于一个列表，既要遍历索引又要遍历元素。</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">array = [<span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'Python'</span>]</span><br><span class="line"><span class="keyword">for</span> i, element <span class="keyword">in</span> enumerate(array):</span><br><span class="line">    array[i] = <span class="string">'%d: %s'</span> % (i, seq[i])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getitem</span>(<span class="params">index, element</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'%d: %s'</span> % (index, element)</span><br><span class="line"></span><br><span class="line">array = [<span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'Python'</span>]</span><br><span class="line">arrayIndex = [getitem(index, element) <span class="keyword">for</span> index, element <span class="keyword">in</span> enumerate(array)]</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="die-dai-qi-he-sheng-cheng-qi">迭代器和生成器</span><a href="#die-dai-qi-he-sheng-cheng-qi" class="header-anchor">#</a></h2><h3><span id="ke-die-dai-dui-xiang">可迭代对象：</span><a href="#ke-die-dai-dui-xiang" class="header-anchor">#</a></h3><p>凡是可以返回一个迭代器的对象都可称之为可迭代对象<br>例如：list    dic    str     set     tuple     range()     enumerate(枚举)     f=open()（文件句柄）<br></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">### 迭代器(iterator)</span></span><br><span class="line">是一个带状态的对象，他能在你调用next()方法的时候返回容器中的下一个值，任何实现了__iter__和__next__()（python2中实现next()）方法的对象都是迭代器，__iter__返回迭代器自身，__next__返回容器中的下一个值，如果容器中没有更多元素了，则抛出StopIteration异常</span><br><span class="line"><span class="comment">### 生成器(generator)</span></span><br><span class="line">生成器其实是一种特殊的迭代器，不过这种迭代器更加优雅。它不需要再像上面的类一样写__iter__()和__next__()方法了，只需要一个yiled关键字。 生成器一定是迭代器（反之不成立）</span><br><span class="line"><span class="comment">#列表生成式</span></span><br><span class="line">lis = [x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line"><span class="comment"># 受到内存限制，列表容量肯定是有限的</span></span><br><span class="line"><span class="comment">#生成器表达式</span></span><br><span class="line">generator_ex = (x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>))</span><br></pre></td></tr></tbody></table></figure><br>生成器： 不用创建完整的list，为节省大量的空间，在Python中，这种一边循环一边计算的机制，称为生成器：generator<br>Tuples:()<br> 字典：{：，}<br> Sets: {,}<br>函数<br>类<p></p>
<h1><span id="python-ku-numpy">Python库——numpy</span><a href="#python-ku-numpy" class="header-anchor">#</a></h1><h3><span id="what">What</span><a href="#what" class="header-anchor">#</a></h3><p>NumPy=Numerical+Python<br>主要是提供了高性能多维数组这个对象，以及处理相关的方法</p>
<h3><span id="how">How</span><a href="#how" class="header-anchor">#</a></h3><ol>
<li>自定义一个（1D or MD)数组或者特殊的数组,一维，二维</li>
<li>数组切片（也就是提取数组元素），注意 a[:,0]和a[:,0:1]是不同的喔</li>
<li>关于数组属性的方法</li>
<li>数组运算</li>
<li>索引<br> where 函数<br> 索引的布尔数组</li>
<li>广播（Broadcasting）<br>用于处理不同性状的 数组。 Broadcasting提供了一种矢量化数组操作的方法，使得循环发生在C而不是Python。标量乘以一个矢量的时候，用Boradcasting更快，因为 broadcasting在乘法期间移动较少的内存</li>
<li>array 和 matrix 选择哪个?<br> <a href="https://www.numpy.org.cn/user_guide/numpy_for_matlab_users.html">戳我</a></li>
<li>矢量化和广播、索引<br>在Python中循环数组或任何数据结构时，会涉及很多开销。 NumPy中的向量化操作将内部循环委托给高度优化的C和Fortran函数，从而实现更清晰，更快速的Python代码。<h2><span id="stack-vstack-hstack">stack|vstack|hstack</span><a href="#stack-vstack-hstack" class="header-anchor">#</a></h2></li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">np.stack((a, b))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">% hstack</span><br><span class="line">a = np.array((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">b = np.array((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">np.hstack((a,b))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">a = np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">b = np.array([[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>]])</span><br><span class="line">np.hstack((a,b))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">% vstack</span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">np.vstack((a,b))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">a = np.array([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">b = np.array([[<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]])</span><br><span class="line">np.vstack((a,b))</span><br><span class="line">array([[<span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>]])</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="mean">mean</span><a href="#mean" class="header-anchor">#</a></h2><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">a = np.array([[1, 2], [3, 4]])</span><br><span class="line">np.mean(a)</span><br><span class="line"></span><br><span class="line">np.mean(a, axis=0)</span><br><span class="line"></span><br><span class="line">np.mean(a, axis=1)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="reshape">reshape</span><a href="#reshape" class="header-anchor">#</a></h2><p>reshape(x, y)，其中x表示转换后数组的行数，y表示转换后数组的列数。当x或者y为-1时，表示该元素随机分配，如reshape(2, -1)表示列数随机，行数为两行。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">格式：np.reshape((x, y, z))</span><br><span class="line"></span><br><span class="line">参数的含义：</span><br><span class="line"></span><br><span class="line">x：表示生成的三维数组中二维数组的个数</span><br><span class="line"></span><br><span class="line">y：表示单个二维数组中一维数组的个数</span><br><span class="line"></span><br><span class="line">z：表示三维数组的列数</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="numpy-shu-zu-qu-diao-rong-yu-de-wei-du-squeeze-han-shu">numpy数组去掉冗余的维度——-squeeze()函数</span><a href="#numpy-shu-zu-qu-diao-rong-yu-de-wei-du-squeeze-han-shu" class="header-anchor">#</a></h2><p>import numpy as np a = [[[10, 2, 3]]] a = np.array(a) a_sque = np.squeeze(a) print(a) print(a_sque)</p>
<h1><span id="python-ku-pandas">Python库——pandas</span><a href="#python-ku-pandas" class="header-anchor">#</a></h1><p>记得学习pandas是在大三时候的美赛，花了一天多时间学习pandas，然后预处理数据，当时三个队友都是各自的家，是非常愉快的！！！</p>
<h3><span id="what">what</span><a href="#what" class="header-anchor">#</a></h3><p>Python Data Analysis Library</p>
<ol>
<li>三种数据结构<br>序列： Series 1D<br>数据帧： DataFrame 2D<br>面板： Panel &gt;2D</li>
<li>自定义创建<ol>
<li>可以通过字段、数据、series、列表</li>
<li>列表传入的时候，主要行列，如果单个列表：列；如果是[[],[]]是按行[]</li>
<li>如果位置不对可转置</li>
<li>创建空 pd.DataFrame()</li>
</ol>
</li>
<li>选择区块<br> a) Series<br> []<br> b) DataFrame<br> 列选择 [‘colums的名字’]<br> 行列选择：.loc[列名,行名]名称 .iloc[列索引,行索引]整数</li>
<li>array<br>.value</li>
<li>统计描述<br> .descibe(include = ‘all’) .head() .tail()<br> .select_dtype(include=[])<br> .columns<br> .dtype</li>
<li><p>缺少数据</p>
<ol>
<li>查看缺失值<br>isnull() notnull() 也可以 做一些统计，sum, any,all </li>
<li>清理缺失值<br> dropna(axis=0)：axis = 0:index axis=1,columns</li>
<li>填充缺少指<br> fillna() 标量替换</li>
<li>替换</li>
</ol>
</li>
<li><p>统计函数</p>
</li>
</ol>
<ol>
<li><p>Pandas 函数应用<br>表合理函数应用：pipe()<br>行或列函数应用：apply()<br>元素函数应用：applymap()<br>eg： pd.pipe(lambda x: x*100)</p>
</li>
<li><p>类别变量向量化<br>非数值类型的处理方法</p>
</li>
<li><p>时间序列生成 data_range</p>
<ol>
<li>pandas.date_range(“11:00”, “21:30”, freq=”30min”)</li>
<li>参数<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Return a fixed frequency DatetimeIndex.</span><br></pre></td></tr></tbody></table></figure>
</li>
</ol>
</li>
</ol>
<p>Parameters<br>startstr or datetime-like, optional<br>Left bound for generating dates.</p>
<p>endstr or datetime-like, optional<br>Right bound for generating dates.</p>
<p>periodsint, optional<br>Number of periods to generate.</p>
<p>freqstr or DateOffset, default ‘D’<br>Frequency strings can have multiples, e.g. ‘5H’. See here for a list of frequency aliases.</p>
<p>tzstr or tzinfo, optional<br>Time zone name for returning localized DatetimeIndex, for example ‘Asia/Hong_Kong’. By default, the resulting DatetimeIndex is timezone-naive.</p>
<p>normalizebool, default False<br>Normalize start/end dates to midnight before generating date range.</p>
<p>namestr, default None<br>Name of the resulting DatetimeIndex.</p>
<p>closed{None, ‘left’, ‘right’}, optional<br>Make the interval closed with respect to the given frequency to the ‘left’, ‘right’, or both sides (None, the default).</p>
<p>**kwargs<br>For compatibility. Has no effect on the result.</p>
<p>Returns<br>rngDatetimeIndex<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">11. DataFrame.stack</span><br><span class="line">Parameters</span><br><span class="line">levelint, str, list, default -1</span><br><span class="line">Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels.</span><br><span class="line"></span><br><span class="line">dropnabool, default True</span><br><span class="line">Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.</span><br><span class="line"></span><br><span class="line">Returns</span><br><span class="line">DataFrame or Series</span><br><span class="line">Stacked dataframe or series.</span><br><span class="line">​```python</span><br><span class="line">df_single_level_cols</span><br><span class="line">     weight height</span><br><span class="line">cat       0      1</span><br><span class="line">dog       2      3</span><br><span class="line">df_single_level_cols.stack()</span><br><span class="line">cat  weight    0</span><br><span class="line">     height    1</span><br><span class="line">dog  weight    2</span><br><span class="line">     height    </span><br></pre></td></tr></tbody></table></figure><br>DataFrame.value_connts()返回序列，index=统计值，值：统计个数<p></p>
<h2><span id="matplotlib">Matplotlib</span><a href="#matplotlib" class="header-anchor">#</a></h2><p>matplotlib.pyplot as plt</p>
<ol>
<li>窗口：figure: 一个窗口，plt.figure(num=,figsize=(h,w))下面数据都属于当前的figure,有一定的顺序喔</li>
<li>画图：plt.plot(x,y,color=,linewidth=,linestyle,label=)</li>
<li>标注信息： plt.xlim((,)), plt.yxlim((,)),plt.xlabel(),plt.ylabel(),ticks:图像的小标，plt.xticks(),plt.yticks([值1，值2],[r’$值1\ 对应的文字$’,r’值2的文字 \alpha])</li>
<li>坐标轴：axis gac=’get current axis’<br>ax = plt.gca() # 轴<br># 获取四个轴<br>ax.spines[‘right|left|top|’].set_color(‘none’)<br>ax.xaxis.set_ticks_position(‘bottom’)<br>ax.spines[‘bottom’].set_position((‘data’,-1))</li>
<li>图例：legend:<br> a. plt.plot(,label=), plt.legend()<br> b. l1, = plt.plot() plt.legend(handles=[l1,],labels=[,],loc=’best|upper right|’)</li>
<li>注解 annotation<br>a. 点的位置(x0，y0) plt.scatter(). plt.plot([x0,y0],[y0,0],’k—‘,lw=)<br>b . method 1:<br>plt.annotate(r’name’,xy=(,)起始点，xycoords=’data’//基于xy,xytext=(+30,30),textcoords=’offseet points’//文本基于xy,arrowprops=dict(arrowstyle=’-&gt;’箭头,connectionstyle=’arc3,rad=.2’)弧度)</li>
<li>Bar 柱状图<br>plt.bar(x,+|-y,facecolor=””,edgecolor,)<br>|# ha horizontal alignment 对齐方式<br>for x,y in zip(x,y):<br> plt.text(x+0,4,y+0.05,’%.2f’%y,ha=’center’,va=’bottom’)</li>
<li>很多自动 subplot(总行，当前行的列，总的按最小分的第几个)<br>subplot(,,)<h2><span id="index">index</span><a href="#index" class="header-anchor">#</a></h2></li>
</ol>
<p>reset_index:限于DataFrame</p>
<p>set_index</p>
<p>index</p>
<h1><span id="scikit-learn">scikit-learn</span><a href="#scikit-learn" class="header-anchor">#</a></h1><p>官方教程绝对是最好最棒的选择，有简单数学推导、直观立马就能上手的案例，还能提阅读英文的能力喔，实在是一举多得啊！！！！ <a href="https://scikit-learn.org/">scikit-learn.org</a></p>
<h2><span id="regression">regression</span><a href="#regression" class="header-anchor">#</a></h2><h2><span id="feature-selection">Feature selection</span><a href="#feature-selection" class="header-anchor">#</a></h2><h3><span id="method-from-sklearn-feature-selection-import-variancethreshold">Method from sklearn.feature_selection import VarianceThreshold</span><a href="#method-from-sklearn-feature-selection-import-variancethreshold" class="header-anchor">#</a></h3><h3><span id="sklearn-feature-selection-selectfrommodel">sklearn.feature_selection.SelectFromModel</span><a href="#sklearn-feature-selection-selectfrommodel" class="header-anchor">#</a></h3><p><em>class</em> sklearn.feature_selection.SelectFromModel(<em>estimator</em>, , <em>threshold=None</em>, <em>prefit=False</em>, <em>norm_order=1</em>, <em>max_features=None</em>)</p>
<h1><span id> </span><a href="#" class="header-anchor">#</a></h1><h1><span id="seaborn">seaborn</span><a href="#seaborn" class="header-anchor">#</a></h1><p>seaborn.jointplot(x, y, data=None, kind=’scatter’, stat_func=None, color=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None, **kwargs)</p>
<ul>
<li><p>Parameters</p>
<p><strong>x, y</strong>strings or vectorsData or names of variables in <code>data</code>.<strong>data</strong>DataFrame, optionalDataFrame when <code>x</code> and <code>y</code> are variable names.<strong>kind</strong>{ “scatter” | “reg” | “resid” | “kde” | “hex” }, optionalKind of plot to draw.<strong>stat_func</strong>callable or None, optional<em>Deprecated**</em>color<strong>matplotlib color, optionalColor used for the plot elements.</strong>height<strong>numeric, optionalSize of the figure (it will be square).</strong>ratio<strong>numeric, optionalRatio of joint axes height to marginal axes height.</strong>space<strong>numeric, optionalSpace between the joint and marginal axes</strong>dropna<strong>bool, optionalIf True, remove observations that are missing from <code>x</code> and <code>y</code>.</strong>{x, y}lim<strong>two-tuples, optionalAxis limits to set before plotting.</strong>{joint, marginal, annot}_kws<strong>dicts, optionalAdditional keyword arguments for the plot components.</strong>kwargs**key, value pairingsAdditional keyword arguments are passed to the function used to draw the plot on the joint Axes, superseding items in the <code>joint_kws</code> dictionary.</p>
</li>
<li><p>Returns</p>
<p><strong>grid</strong><a href="https://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid"><code>JointGrid</code></a><a href="https://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid"><code>JointGrid</code></a> object with the plot on it.</p>
</li>
</ul>
<p><a href="http://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid">http://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid</a></p>
<p>g = sns.jointplot(x=”x”, y=”y”, kind = ‘reg’ , space=0,color = ‘g’, data=df11,stat_func=sci.pearsonr)</p>
<p>sns.set()</p>
<p>sns.axes_style(“darkgrid”)</p>
<p>sns.set_context(“paper”)</p>
<p><a href="https://blog.mazhangjing.com/2018/03/29/learn_seaborn/">https://blog.mazhangjing.com/2018/03/29/learn_seaborn/</a></p>
<p><a href="https://blog.csdn.net/weiyudang11/article/details/51549672">https://blog.csdn.net/weiyudang11/article/details/51549672</a></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#初始化类</span><br><span class="line">g=sns.JointGrid(x='v_ma5',y='price_change',data=stock,space=0.5,ratio=5)</span><br><span class="line"></span><br><span class="line">g=sns.JointGrid(x='v_ma5',y='price_change',data=stock,space=0.5,ratio=5)</span><br><span class="line">g=g.plot_joint(plt.scatter,color='.3',edgecolor='r')</span><br><span class="line">g=g.plot_marginals(sns.distplot,kde=False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from scipy import stats</span><br><span class="line">g=sns.JointGrid(x='v_ma5',y='price_change',data=stock,space=0.5,ratio=5)</span><br><span class="line">g=g.plot_joint(plt.scatter,color='.3',edgecolor='r')</span><br><span class="line">_=g.ax_marg_x.hist(stock.v_ma10,color='r',alpha=.6,bins=50)</span><br><span class="line">_=g.ax_marg_y.hist(stock.low,color='y',orientation="horizontal",bins=20)</span><br><span class="line">rquare=lambda a,b:stats.pearsonr(a,b)[0]**2</span><br><span class="line">g=g.annotate(rquare,template='{stat}:{val:.2f}',stat='$R^2$',loc='upper left',fontsize=12)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="yan-se-he-feng-ge-she-zhi">颜色和风格设置</span><a href="#yan-se-he-feng-ge-she-zhi" class="header-anchor">#</a></h2><h2><span id="diao-se-ban">调色板</span><a href="#diao-se-ban" class="header-anchor">#</a></h2><p>主要使用以下几个函数设置颜色：<br>color_palette() 能传入任何Matplotlib所有支持的颜色<br>color_palette() 不写参数则默认颜色</p>
<p>current_palette = sns.color_palette() sns.palplot(current_palette) plt.show()</p>
<p>set_palette() 设置所有图的颜色</p>
<p>sns.palplot(sns.color_palette(“hls”,8)) plt.show()</p>
<h2><span id="yan-se-de-liang-du-ji-bao-he-du">颜色的亮度及饱和度</span><a href="#yan-se-de-liang-du-ji-bao-he-du" class="header-anchor">#</a></h2><p>l-光度 lightness<br>s-饱和 saturation</p>
<p>sns.palplot(sns.hls_palette(8,l=.7,s=.9)) plt.show()</p>
<h2><span id="xkcd-xuan-qu-yan-se">xkcd选取颜色</span><a href="#xkcd-xuan-qu-yan-se" class="header-anchor">#</a></h2><p>xkcd包含了一套众包努力的针对随机RGB色的命名。产生了954个可以随时通过xkcd_rgb字典中调用的命名颜色</p>
<p>plt.plot([0,1],[0,1],sns.xkcd_rgb[‘pale red’],lw = 3) #lw = 线宽度<br>plt.plot([0,1],[0,2],sns.xkcd_rgb[‘medium green’],lw = 3)<br>plt.plot([0,1],[0,3],sns.xkcd_rgb[‘denim blue’],lw = 3)<br>plt.show()</p>
<p><img src="https://img-blog.csdnimg.cn/20190216223007343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Ryb2tlX1pob3U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2><span id="hui-zong">汇总</span><a href="#hui-zong" class="header-anchor">#</a></h2><p><a href="http://seaborn.pydata.org/api.html#">http://seaborn.pydata.org/api.html#</a></p>
<p><a href="https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py">https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py</a></p>
<p><a href="https://xkcd.com/color/rgb/">https://xkcd.com/color/rgb/</a></p>
<h1><span id="di-tu-geopy">地图：geopy</span><a href="#di-tu-geopy" class="header-anchor">#</a></h1><h2><span id="geocoding">Geocoding</span><a href="#geocoding" class="header-anchor">#</a></h2><p><a href="https://geopy.readthedocs.io/en/latest/#module-geopy.distance">https://geopy.readthedocs.io/en/latest/#module-geopy.distance</a></p>
<h3><span id="from-address-to-coordinates">from address to coordinates</span><a href="#from-address-to-coordinates" class="header-anchor">#</a></h3><p>使用 geopy 查询地址前需要首先选择一个地图服务，绝大多数的服务需要一个密钥（api_key）。你需要到其开发者页面注册和申请，例如<a href="http://lbsyun.baidu.com/">百度</a>。免费的密钥通常会对使用频率和次数做限制。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from geopy.geocoders import Nominatim</span><br><span class="line">&gt;&gt;&gt; geolocator = Nominatim(user_agent="specify_your_app_name_here")</span><br><span class="line">&gt;&gt;&gt; location = geolocator.geocode("175 5th Avenue NYC")</span><br><span class="line">&gt;&gt;&gt; print(location.address)</span><br><span class="line">Flatiron Building, 175, 5th Avenue, Flatiron, New York, NYC, New York, ...</span><br><span class="line">&gt;&gt;&gt; print((location.latitude, location.longitude))</span><br><span class="line">(40.7410861, -73.9896297241625)</span><br><span class="line">&gt;&gt;&gt; print(location.raw)</span><br><span class="line">{'place_id': '9167009604', 'type': 'attraction', ...}</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="from-coordinates-to-address">from coordinates to address</span><a href="#from-coordinates-to-address" class="header-anchor">#</a></h3><p>使用百度</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from geopy.geocoders import Baidu</span><br><span class="line">geocoder = Baidu(</span><br><span class="line">            api_key='ak',#自己修改</span><br><span class="line">            security_key='sk',#自己修改</span><br><span class="line">            timeout=200</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from geopy.geocoders import Nominatim</span><br><span class="line">&gt;&gt;&gt; geolocator = Nominatim(user_agent="specify_your_app_name_here")</span><br><span class="line">&gt;&gt;&gt; location = geolocator.reverse("52.509669, 13.376294")</span><br><span class="line">&gt;&gt;&gt; print(location.address)</span><br><span class="line">Potsdamer Platz, Mitte, Berlin, 10117, Deutschland, European Union</span><br><span class="line">&gt;&gt;&gt; print((location.latitude, location.longitude))</span><br><span class="line">(52.5094982, 13.3765983)</span><br><span class="line">&gt;&gt;&gt; print(location.raw)</span><br><span class="line">{'place_id': '654513', 'osm_type': 'node', ...}</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="measuring-distance">Measuring Distance</span><a href="#measuring-distance" class="header-anchor">#</a></h2><p>  <a href="https://en.wikipedia.org/wiki/Geodesics_on_an_ellipsoid">geodesic distance</a> uses ellipsoidal model of the earth.     <a href="https://en.wikipedia.org/wiki/Great-circle_distance">great-circle distance</a> uses a spherical model of the earth.</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from geopy import distance</span><br><span class="line">&gt;&gt;&gt; newport_ri = (41.49008, -71.312796)</span><br><span class="line">&gt;&gt;&gt; cleveland_oh = (41.499498, -81.695391)</span><br><span class="line">&gt;&gt;&gt; print(distance.distance(newport_ri, cleveland_oh).miles)</span><br><span class="line">538.39044536</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; wellington = (-41.32, 174.81)</span><br><span class="line">&gt;&gt;&gt; salamanca = (40.96, -5.50)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(distance.distance(wellington, salamanca).km)</span><br><span class="line">19959.6792674</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(distance.great_circle(newport_ri, cleveland_oh).miles)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>我的读书笔记</title>
    <url>/2019/02/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<a id="more"></a>
<h2><span id="2019-di-shi-wu-zhou"><font color="red">2019 第十五周<font></font></font></span><a href="#2019-di-shi-wu-zhou" class="header-anchor">#</a></h2><p>三月份至2019.4.9这段时间，才发现我是如此没有自律的人，充分体现了我是人的特性，那就是我是群体动物，苦笑.jpg,苦笑.jpg,</p>
<p>en, 最近突然想给自己打上厨娘的身份，如果可以每天花两个小时做饭就好了</p>
<hr>
<h2><span id="yuan-ni-bei-shi-jie-wen-rou-de-xiang-dai">愿你被世界温柔的相待</span><a href="#yuan-ni-bei-shi-jie-wen-rou-de-xiang-dai" class="header-anchor">#</a></h2><ol>
<li>接触的东西越多，越深入，就会发现我是如此的菜，开始有些知识焦虑了，知识那么多<del>~，可是我只有一个头脑啊</del>~</li>
<li>开始不想写一些特别低俗的博客了，一是觉得浪费时间，二是输出效果太差，引不起特别大的关注，虽然我写博客，完全是站在自己的角度，没有考虑读者的意愿，（滑稽.jpg)。</li>
<li>现在的自己，不是停留在基本的问题上，更应该去探索未知 的知识世界，虽然离这个flag可能还有几年的时间，能够给世界的知识创造一点点价值，哪怕只是一小点点。离这个目标还需要努力啊！！！！！</li>
<li>我想我应该去记录学习知识的过程，突破更大的更困难的问题。 </li>
</ol>
<hr>
<h2><span id="2019-di-si-zhou-du-shu-bi-ji">2019-第四周读书笔记</span><a href="#2019-di-si-zhou-du-shu-bi-ji" class="header-anchor">#</a></h2><ol>
<li><p>这周读了一本小说，是张爱玲的《倾城之恋》，原来和电视剧的何晟铭主演《倾城之恋》不是同一个事情啊！</p>
</li>
<li><p>看了《阿甘正传》，“生活就像一盒巧克力，你永远不知道下一颗是什么味道。“这是阿甘对生活最好的诠释。小时候，有人骑着自行车羞辱他，他只会跑，拼命的跑，只会再公路上跑。长大后，别人骑着车想打他，阿甘还是跑，但是这次阿甘学会了网草坪上跑！就被大学看上，进入运动大学，还通过参加比赛赢得了冠军，然后，阿甘当兵了，再后来，打乒乓球很出色。阿甘似乎做什么都能成功，也许心无旁骛，最笨的方法+时间=收获。</p>
</li>
</ol>
<p>我觉得很心酸的是，当珍妮告诉他有儿子时候，阿甘问，”他聪明吗“？</p>
<hr>
<h2><span id="2019-di-si-zhou-an-pai">2019第四周安排</span><a href="#2019-di-si-zhou-an-pai" class="header-anchor">#</a></h2><ol>
<li><p>改论文，改变自己的办事效率喔，拒绝重复工作</p>
</li>
<li><p>编程能力</p>
</li>
<li><p>慢慢的做事情，先慢后快，</p>
</li>
<li><p>生活、学习、交友、文采<br>2019-第三周读书笔记<br>这次读了《极简思维：颠覆传统思维模式的极简法则》作者：S.J斯科特 巴里.达文波特</p>
</li>
</ol>
<p>我们生活充满了各种诱惑、杂乱信息、导致了生活的混乱，产生知识焦虑、年龄危机、人际关系的淡化。作者给我们介绍了许多问题、许多的解决方法，让我们这个信息爆炸的时代可以过的充实些。</p>
<p>每天睡8个小时、还剩下16个小时，在减去2个小时解决个人卫生和饮食，那么还有14个小时，一个星期98个小时。那么98个小时，你投入在哪里呢？</p>
<p>总的来说，这本书传达的东西，我还是很喜欢的，极简主义者，少不得也多不得！！！！！！！！！！！！！！！</p>
<hr>
<h2><span id="du-chai-diao-si-wei-li-de-qiang">读《拆掉思维里的墙》</span><a href="#du-chai-diao-si-wei-li-de-qiang" class="header-anchor">#</a></h2><p>摘录 ：我们的生活也由三个支架组成：自我、家庭与团体和职业。这样的支架支撑着我们的灵魂，它在记录我们的生命。我们一直都在调整着三个位置的平稳，使之成为最稳固的联动三脚架。</p>
<p>这句大概是结合我的经历，最具有感悟的。因为一旦走出大学，这三者才开始真正的组成我们的生活。</p>
<p>古典老师，从职业、成功学、爱情、家庭等等不同的案例，给我分析了大多数人会面临的无形的”墙“，给了我们如何拆掉这些墙的方法。但是呢，对于古典老师的爱情观点，我并不是很赞同，因为呢，那些愿意陪你度过余生的人付出的感情，是如此的廉价吗？有的人既可以是白玫瑰，也可以红玫瑰啊！</p>
<hr>
<h2><span id="2019-nian-di-er-zhou-an-t-pai">2019年第二周安T排</span><a href="#2019-nian-di-er-zhou-an-t-pai" class="header-anchor">#</a></h2><p>每天两个小时阅读论文或者专业书籍的阅读<br>开题报告修改和PPT制作（3h)<br>《拆掉思维里面的墙》（3h)<br>看哈利波特（一集）</p>
<h2><span id>—-</span><a href="#" class="header-anchor">#</a></h2><h1><span id="2018-nian-de-zong-jie">2018年的总结</span><a href="#2018-nian-de-zong-jie" class="header-anchor">#</a></h1><p> 小小的悔恨与遗憾</p>
<ol>
<li>大三下，在课堂上，打了半学期的游戏</li>
<li>生活还是不规律，超喜欢深夜逛知乎、刷B站</li>
<li>额头上，不停的冒着痘痘啊</li>
<li>英语单词量在下降ing</li>
<li>运动量在降低喔</li>
<li>很讨厌洗衣服</li>
</ol>
<p>相比于上一年进步的方面</p>
<ol>
<li>愿意去承担更多的责任</li>
<li>更乐意去交流</li>
<li>越来越重视健康style</li>
<li>不会随意发泄自己的情绪了<ol>
<li>更加认识到自身的优势与劣势了<br>感到愉快的事情</li>
</ol>
</li>
<li>知道自己想要什么，知道自己在做什么</li>
<li>整理完了大学期间所有的东西，往事不堪回首， 但也只能是柳暗花明又一村。</li>
<li>能读研究生了</li>
</ol>
<p>聊聊2019年的点点期许<br>学习上</p>
<ol>
<li>多看19场知乎live</li>
<li>阅读10本书籍，书单也有了</li>
<li>在专业学习上，希望有所提升咯<br>生活中</li>
<li>早睡早起身体好</li>
<li>看十部美剧，尽管我最大的兴趣是睡觉</li>
<li>时常更新歌单，不想在一年里面都是相同的旋律</li>
<li>静静静静静静静</li>
<li>合理安排</li>
<li>折星星</li>
<li>番茄闹钟</li>
<li>偶尔听听 TED<br>技术</li>
<li>清理下了github 仓库</li>
<li>重新更新了  github page</li>
<li>多读、多写、多想</li>
</ol>
]]></content>
      <categories>
        <category>读书日常</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title>MayMay</title>
    <url>/2019/02/22/MayMay/</url>
    <content><![CDATA[<p><a href="https://www.kaggle.com/dgawlik/house-prices-eda/data">https://www.kaggle.com/dgawlik/house-prices-eda/data</a><br><a href="https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python">https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python</a></p>
]]></content>
      <tags>
        <tag>wan</tag>
      </tags>
  </entry>
</search>
